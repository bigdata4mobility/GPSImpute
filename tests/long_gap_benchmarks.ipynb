{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import skmob\n",
    "import pandas as pd\n",
    "import skmob.measures.individual as ind_measure\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.kernels import RQKernel as RQ, RBFKernel as SE, \\\n",
    "PeriodicKernel as PER, ScaleKernel, LinearKernel as LIN, MaternKernel as MAT, \\\n",
    "SpectralMixtureKernel as SMK, PiecewisePolynomialKernel as PPK, CylindricalKernel as CYL\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gpytorch.constraints import Interval\n",
    "import time\n",
    "\n",
    "# Import intra-package scripts\n",
    "import utils.helper_func as helper_func\n",
    "import utils.GP as GP\n",
    "from utils.helper_func import dec_floor\n",
    "import mobileDataToolkit.analysis as analysis\n",
    "import mobileDataToolkit.preprocessing_v2 as preprocessing\n",
    "import mobileDataToolkit.methods as methods\n",
    "import mobileDataToolkit.metrics as metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Data\\\\seattle_2000_all_obs_sampled.csv\"\n",
    "df = pd.read_csv(file_path, header=0)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month column\n",
    "df['month'] = pd.DatetimeIndex(df['datetime']).month\n",
    "\n",
    "# Group by user ID, find month with third most observations (average)\n",
    "df_m = df.groupby('UID').apply(lambda x: x[x['month'] == x['month'].value_counts().index[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only that month's data for each user\n",
    "df_m = df_m.reset_index(drop=True)\n",
    "df_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m.UID.unique().shape # confirm that we have 50 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_speed_kmh = 400 # for filtering out unrealistic speeds\n",
    "spatial_radius_km = 0.3 # for compressing similar points using Douglas-Peucker algorithm\n",
    "\n",
    "df_curr = df_m[df_m.UID == df_m.UID.unique()[1]]\n",
    "\n",
    "tdf = skmob.TrajDataFrame(df_curr, latitude='orig_lat', longitude='orig_long', datetime='datetime')\n",
    "f_tdf = skmob.preprocessing.filtering.filter(tdf, max_speed_kmh=max_speed_kmh, include_loops=False)\n",
    "# Print the difference in number of rows\n",
    "print(\"Number of rows before filtering: {}\".format(tdf.shape[0]))\n",
    "print(\"Number of rows after filtering: {}\".format(f_tdf.shape[0]))\n",
    "fc_tdf = skmob.preprocessing.compression.compress(f_tdf, spatial_radius_km=spatial_radius_km)\n",
    "# Print the difference in number of rows\n",
    "print(\"Number of rows after compression: {}\".format(fc_tdf.shape[0]))\n",
    "# Remove data points with uncertainty > 100m\n",
    "fcu_tdf = fc_tdf[fc_tdf['orig_unc'] <= 100]\n",
    "# Print the difference in number of rows\n",
    "print(\"Number of rows after uncertainty filtering: {}\".format(fcu_tdf.shape[0]))\n",
    "\n",
    "df_curr = fcu_tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sci-kit mobility metrics\n",
    "no_loc_gt = skmob.measures.individual._number_of_locations_individual(df_curr)\n",
    "rg_gt = skmob.measures.individual._radius_of_gyration_individual(df_curr).squeeze()\n",
    "k_rg_gt = skmob.measures.individual._k_radius_of_gyration_individual(df_curr).squeeze()\n",
    "jumps_gt = skmob.measures.individual._jump_lengths_individual(df_curr).squeeze()\n",
    "spat_burst_gt = helper_func.burstiness(jumps_gt)\n",
    "loc_freq_gt = skmob.measures.individual._location_frequency_individual(df_curr, normalize=True) # matrix\n",
    "rand_entr_gt = skmob.measures.individual._random_entropy_individual(df_curr).squeeze()\n",
    "real_entr_gt = skmob.measures.individual._real_entropy_individual(df_curr).squeeze()\n",
    "recency_gt = skmob.measures.individual._recency_rank_individual(df_curr).squeeze()  # matrix\n",
    "freq_rank_gt = skmob.measures.individual._frequency_rank_individual(df_curr).squeeze() # matrix\n",
    "uncorr_entr_gt = skmob.measures.individual._uncorrelated_entropy_individual(df_curr).squeeze()\n",
    "max_dist_gt = skmob.measures.individual._maximum_distance_individual(df_curr).squeeze()\n",
    "dist_straight_gt = skmob.measures.individual._distance_straight_line_individual(df_curr).squeeze()\n",
    "waiting_time_gt = skmob.measures.individual._waiting_times_individual(df_curr).squeeze() # array\n",
    "home_loc_gt = skmob.measures.individual._home_location_individual(df_curr) # tuple\n",
    "max_dist_home_gt = skmob.measures.individual._max_distance_from_home_individual(df_curr).squeeze()\n",
    "mob_network_gt = skmob.measures.individual._individual_mobility_network_individual(df_curr) # big matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(df_curr, f\"no_loc_gp_pred\", no_loc_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curr.no_loc_gp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_len_ls = [15, 20, 30, 60, 360, 1440, 10080] # Bin lengths to test: 15 min, 20 min, 30 min, 1 hr, 6 hr, 1 day, 1 week\n",
    "\n",
    "upper_bound = dec_floor(analysis.tempOcp(df_curr, 'unix_min', bin_len=360))\n",
    "curr_ocp = analysis.tempOcp(df_curr, 'unix_min', bin_len=360)\n",
    "# See current temporal occupancy\n",
    "print(\"Current temporal occupancy: {}\".format(curr_ocp))\n",
    "while True:\n",
    "    try:\n",
    "        # Choose random decimal between 0 and upper bound\n",
    "        target_ocp = dec_floor(np.random.uniform(0.3, upper_bound))\n",
    "        print(\"Target temporal occupancy: {}\".format(target_ocp))\n",
    "        # Simulate gaps in the user's data to match the target level\n",
    "        gapped_user_data, train_index, new_ocp = analysis.simulate_gaps(df_curr, target_ocp, unix_col='unix_min', bin_len=360)\n",
    "    except:\n",
    "        continue\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change name of 'lat' and 'lon' columns to 'orig_lat' and 'orig_long'\n",
    "df_curr = df_curr.rename(columns={'lat': 'orig_lat', 'lng': 'orig_long'})\n",
    "\n",
    "curr_mt = preprocessing.dp_MultiTrip(data=df_curr)\n",
    "curr_mt.Multi_Trip_Preprocess(lat='orig_lat', long='orig_long', datetime='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move 'unix_start_t' to before 'SaM'\n",
    "cols = list(curr_mt.data.columns)\n",
    "cols.insert(16, cols.pop(cols.index('unix_min')))\n",
    "curr_mt.data = curr_mt.data.loc[:, cols]   \n",
    "curr_mt.data.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_mt.Multi_Trip_TrainTestSplit(test_start_date=None, test_end_date=None, \n",
    "                                training_index = set(gapped_user_data['unix_min']), lat='orig_lat', \n",
    "                                long='orig_long', datetime='datetime', unix='unix_min', inputstart='unix_min', inputend='day_6')\n",
    "\n",
    "# See number of points in training and test sets\n",
    "print(\"Number of points in training set: {}\".format(len(curr_mt.X_train[:,0])))\n",
    "print(\"Number of points in test set: {}\".format(len(curr_mt.X_test[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training and test data in two subplots, one lat vs time and one long vs time\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n",
    "ax[0].scatter(curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_lat'],\n",
    "                color='blue', label='Training data', s=1)\n",
    "ax[0].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_lat'],\n",
    "                color='red', label='Test data', s=1)\n",
    "ax[0].set_ylabel('Latitude')\n",
    "ax[1].scatter(curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_long'],\n",
    "                color='blue', label='Training data', s=1)\n",
    "ax[1].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_long'],\n",
    "                color='red', label='Test data', s=1)\n",
    "ax[1].set_xlabel('Time (Unix)')\n",
    "ax[1].set_ylabel('Longitude')\n",
    "ax[1].legend()\n",
    "\n",
    "# Save figure to file\n",
    "fig.savefig('training_test_data.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler1 = MinMaxScaler(feature_range=(0, 100))\n",
    "#scaler2 = MinMaxScaler(feature_range=(0, 10))\n",
    "scaler3 = StandardScaler()\n",
    "\n",
    "mean_lat = curr_mt.y_train[:,0].mean()\n",
    "mean_long = curr_mt.y_train[:,1].mean()\n",
    "std_lat = curr_mt.y_train[:,0].std()\n",
    "std_long = curr_mt.y_train[:,1].std()\n",
    "\n",
    "# Normalize the unix time such that it starts at 0\n",
    "#tr_df.X_train[:,0] = tr_df.X_train[:,0] - tr_df.X_train[:,0].min()\n",
    "#tr_df.X_test[:,0] = tr_df.X_test[:,0] - tr_df.X_train[:,0].min()\n",
    "\n",
    "#unix_train = torch.tensor(np.float64(scaler1.fit_transform(curr_mt.X_train[:,0].reshape(-1,1))))\n",
    "#secs_train = torch.tensor(scaler2.fit_transform(tr_df.X_train[:,1].reshape(-1,1))).float()\n",
    "#unix_test = torch.tensor(np.float64(scaler1.transform(curr_mt.X_test[:,0].reshape(-1,1))))\n",
    "#secs_test = torch.tensor(scaler2.transform(tr_df.X_test[:,1].reshape(-1,1))).float()\n",
    "\n",
    "#X_train = torch.cat([unix_train, curr_mt.X_train[:, 1::]], -1)\n",
    "#X_test = torch.cat([unix_test, curr_mt.X_test[:, 1::]], -1)\n",
    "\n",
    "#X_train = tr_df.X_train.float()\n",
    "#X_test = tr_df.X_test.float()\n",
    "\n",
    "curr_mt.y_train = torch.tensor(np.float64(scaler3.fit_transform(curr_mt.y_train)))\n",
    "curr_mt.y_test = torch.tensor(np.float64(scaler3.transform(curr_mt.y_test)))\n",
    "\n",
    "n_dims = curr_mt.X_train.shape[1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = curr_mt.X_train.shape[1]\n",
    "print(\"Number of dimensions: {}\".format(n_dims))\n",
    "\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)\n",
    "\n",
    "model = GP.MTGPRegressor(curr_mt.X_train, curr_mt.y_train, \n",
    "                         ScaleKernel(RQ(ard_num_dims=n_dims) * PER(active_dims=[0])) + ScaleKernel(RQ(ard_num_dims=n_dims) * PER(active_dims=[0])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial lengthscale guess as half the average length of gap in training set\n",
    "init_lengthscale = curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'].diff().mean() / 2\n",
    "\n",
    "#scaled_unix_lengthscale = scaler1.transform(torch.tensor(init_lengthscale).reshape(-1,1)).item()\n",
    "\n",
    "initializations = np.ones(n_dims - 1)\n",
    "initializations = np.insert(initializations, 0, init_lengthscale)\n",
    "\n",
    "model.covar_module.data_covar_module.kernels[0].base_kernel.kernels[0].lengthscale = initializations\n",
    "model.covar_module.data_covar_module.kernels[1].base_kernel.kernels[0].lengthscale = initializations\n",
    "\n",
    "# Set initial period lengths\n",
    "model.covar_module.data_covar_module.kernels[0].base_kernel.kernels[1].period_length = 60*8\n",
    "model.covar_module.data_covar_module.kernels[1].base_kernel.kernels[1].period_length = 60*24\n",
    "#model.covar_module.data_covar_module.kernels[3].base_kernel.period_length = 60*12\n",
    "#model.covar_module.data_covar_module.kernels[4].base_kernel.period_length = 60*6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls, mll = GP.training(model, curr_mt.X_train, curr_mt.y_train, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model parameters (converting back to original scale)\n",
    "print(model.covar_module.data_covar_module.kernels[0].base_kernel.lengthscale)\n",
    "print(model.covar_module.data_covar_module.kernels[1].base_kernel.lengthscale)\n",
    "print(model.covar_module.data_covar_module.kernels[0].outputscale)\n",
    "print(model.covar_module.data_covar_module.kernels[1].outputscale)\n",
    "print(model.likelihood.noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    log_ll = mll(model(curr_mt.X_train), curr_mt.y_train) * curr_mt.X_train.shape[0]\n",
    "            \n",
    "N = curr_mt.X_train.shape[0]\n",
    "m = sum(p.numel() for p in model.hyperparameters())\n",
    "bic = -2 * log_ll + m * np.log(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, mean = model.predict(curr_mt.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all model parameters to file\n",
    "with open('model_params.txt', 'w') as f:\n",
    "    f.write('Lengthscale: {}\\n'.format(model.covar_module.data_covar_module.kernels[0].base_kernel.kernels[0].lengthscale))\n",
    "    f.write('Period: {}\\n'.format(model.covar_module.data_covar_module.kernels[0].base_kernel.period_length))\n",
    "    f.write('Outputscale: {}\\n'.format(model.covar_module.data_covar_module.kernels[0].outputscale))\n",
    "    f.write('Noise: {}\\n'.format(model.likelihood.noise))\n",
    "    f.write('BIC: {}\\n'.format(bic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predictions as we did earlier with training and testing points\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6), sharex=True)\n",
    "ax[0].scatter(curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['norm_lat'],\n",
    "                color='blue', label='Training data', s=1)\n",
    "ax[0].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],  \n",
    "                curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['norm_lat'],\n",
    "                color='green', label='Testing data', s=1)\n",
    "ax[0].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                predictions.mean[:,0],\n",
    "                color='red', label='Predictions', s=1)\n",
    "ax[0].set_title('Latitude')\n",
    "ax[0].set_ylabel('Latitude')\n",
    "\n",
    "ax[1].scatter(curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['norm_long'],\n",
    "                color='blue', label='Training data', s=1)\n",
    "ax[1].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['norm_long'],\n",
    "                color='green', label='Testing data', s=1)\n",
    "ax[1].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                predictions.mean[:,1],\n",
    "                color='red', label='Predictions', s=1)\n",
    "ax[1].set_title('Longitude')\n",
    "ax[1].set_ylabel('Longitude')\n",
    "ax[1].set_xlabel('Time (Unix)')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use smaller font\n",
    "plt.rcParams.update({'font.size': 9})\n",
    "# Make the font nicer\n",
    "plt.rcParams.update({'font.family': 'serif'})\n",
    "f, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.set_title('Predictions')\n",
    "pd.DataFrame(mean.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='red', alpha=0.5, s=0.4, label='Predictions')\n",
    "pd.DataFrame(curr_mt.y_test.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='blue', alpha=0.5, s=0.4, label='Actual')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_res = metrics.average_eval(pd.Series(curr_mt.y_test[:,0]), pd.Series(curr_mt.y_test[:,1]), pd.Series(mean[:,0]), pd.Series(mean[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import Holt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unix_min_tr = np.array(curr_mt.X_train[:,0]).astype(int)\n",
    "unix_min_te = np.array(curr_mt.X_test[:,0]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = pd.Series(curr_mt.y_train[:,0].tolist(), unix_min_tr)\n",
    "lat_t = pd.Series(curr_mt.y_test[:,0].tolist(), unix_min_te)\n",
    "# Replace duplicates (in time) with the mean of the two values\n",
    "lat = lat.groupby(lat.index).mean().reset_index()\n",
    "lat = pd.Series(lat[0].tolist(), lat['index'].tolist())\n",
    "lat_tc = lat_t.groupby(lat_t.index).mean().reset_index()\n",
    "lat_tc = pd.Series(lat_tc[0].tolist(), lat_tc['index'].tolist())\n",
    "# Replace zeroes with positives close to zero\n",
    "lat.replace(0, 0.000000001, inplace=True)\n",
    "\n",
    "\n",
    "lon = pd.Series(curr_mt.y_train[:,1].tolist(), unix_min_tr)\n",
    "lon_t = pd.Series(curr_mt.y_test[:,1].tolist(),unix_min_te)\n",
    "# Replace duplicates (in time) with the mean of the two values\n",
    "lon = lon.groupby(lon.index).mean().reset_index()\n",
    "lon = pd.Series(lon[0].tolist(), lon['index'].tolist())\n",
    "lon_tc = lon_t.groupby(lon_t.index).mean().reset_index()\n",
    "lon_tc = pd.Series(lon_tc[0].tolist(), lon_tc['index'].tolist())\n",
    "# Replace zeroes with positives close to zero\n",
    "lon.replace(0, 0.000000001, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_level = 0.1\n",
    "ses_lat = SimpleExpSmoothing(lat, initialization_method=\"heuristic\").fit(smoothing_level=smoothing_level, optimized=True)\n",
    "pred_lat = ses_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "pred_lat_comp = pred_lat[pred_lat.index.isin(unix_min_te)]\n",
    "\n",
    "ses_lon = SimpleExpSmoothing(lon, initialization_method=\"heuristic\").fit(smoothing_level=smoothing_level, optimized=True)\n",
    "pred_lon = ses_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "pred_lon_comp = pred_lon[pred_lon.index.isin(unix_min_te)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SES_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp, pred_lon_comp)\n",
    "SES_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LI_res = LI(curr_mt.X_train[:,0], curr_mt.X_test[:,0], curr_mt.y_train, curr_mt.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LI_res and GP_res dictionaries\n",
    "for key in LI_res.keys():\n",
    "    print(key)\n",
    "    print('GP: ', GP_res[key])\n",
    "    print('LI: ', LI_res[key])\n",
    "    print('SES: ', SES_res[key])\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skmob_alt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
