{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import math\n",
    "#import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "#from sklearn.metrics import silhouette_score\n",
    "from gpytorch.kernels import RQKernel as RQ, RBFKernel as SE, \\\n",
    "PeriodicKernel as PER, ScaleKernel, LinearKernel as LIN, MaternKernel as MAT, \\\n",
    "SpectralMixtureKernel as SMK, PiecewisePolynomialKernel as PPK, CylindricalKernel as CYL\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import mobileDataToolkit.preprocessing_v2 as preprocessing\n",
    "import mobileDataToolkit.analysis as analysis\n",
    "import mobileDataToolkit.methods as methods\n",
    "import mobileDataToolkit.metrics as metrics\n",
    "import utils.GP as GP\n",
    "import utils.helper_func as helper_func\n",
    "from utils.helper_func import dec_floor\n",
    "import geopandas as gpd\n",
    "import skmob\n",
    "import skmob.preprocessing.detection\n",
    "import skmob.preprocessing.clustering\n",
    "import movingpandas as mpd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = \"C:/Users/ekino/OneDrive - UW/GPR/Data/newAllTrips_withmetrics.csv\"\n",
    "c_path = \"C:/Users/ekino/OneDrive - UW/GPR/Data/newCompressedTrips.csv\"\n",
    "all_path = \"C:/Users/ekino/OneDrive - UW/GPR/Data/10_users_all_obs_raw.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mobility metrics dataset preprocessing\n",
    "m_df = pd.read_csv(c_path, header=0)\n",
    "m_df = m_df.dropna()\n",
    "\n",
    "# Filter out trips with unrealistic speeds, durations, and number of points\n",
    "m_df = m_df[(m_df['vel_avg'] < 80) & #no faster than 80 m/s (as the crow flies)\n",
    "            (m_df['time_total'] < 7200*4) & # no longer than 6 hours\n",
    "            (m_df['time_total'] >= 3600) & # no shorter than 1 hour\n",
    "            (m_df['npoints'] > 4) & # at least 5 points for modeling\n",
    "            (m_df['StartDay'] == m_df['EndDay']) # start day and end day must be the same\n",
    "            ]\n",
    "\n",
    "m_df = m_df[m_df['Id_perc'] != 2141084034]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = m_df[['vel_avg', 'distanceTotal', 'time_total', 'hcr', 'vcr', 'npoints', 'sr']]\n",
    "\n",
    "\n",
    "def mob_clust(feats = feats):\n",
    "    kmeans_kwargs = {\n",
    "        \"init\": \"random\",\n",
    "        \"n_init\": 10,\n",
    "        \"max_iter\": 300,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    # A list holds the SSE values for each k\n",
    "    sse = []\n",
    "    for k in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(feats)\n",
    "        sse.append(kmeans.inertia_)\n",
    "      \n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    plt.plot(range(1, 11), sse)\n",
    "    plt.xticks(range(1, 11))\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"SSE\")\n",
    "    #plt.show()\n",
    "    \n",
    "    kl = KneeLocator(\n",
    "        range(1, 11), sse, curve=\"convex\", direction=\"decreasing\"\n",
    "    )\n",
    "    \n",
    "    kmeans = KMeans(\n",
    "         init=\"random\",\n",
    "         n_clusters=kl.elbow,\n",
    "         n_init=10,\n",
    "         max_iter=300,\n",
    "         random_state=42\n",
    "     )\n",
    "    \n",
    "    kmeans.fit(feats)\n",
    "    \n",
    "    centers = kmeans.cluster_centers_\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    return centers, labels, kmeans\n",
    "\n",
    "random.seed(10)\n",
    "centers, labels, kmeans = mob_clust(feats)\n",
    "m_df['labels'] = labels\n",
    "m_df = m_df.reset_index()\n",
    "print(m_df.labels.value_counts())\n",
    "\n",
    "# Cluster 0 is the fastest average speed, and has the lowest sample size\n",
    "# Cluster 1 is the slowest average speed, and has the highest sample size\n",
    "# Cluster 2 is the middle speed, and has the middle amount of samples\n",
    "\n",
    "# Average speeds are inflated due to distances being \"as the crow flies\"--real transportation networks are more convoluted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(in_path, header=0)\n",
    "df['date'] = pd.to_datetime(df['Date_Time']).dt.date\n",
    "\n",
    "trains1 = pd.DataFrame( columns = ['index', 'time', 'day', 'week', 'train_lat', 'train_long', 'trip_ID'])\n",
    "trains2 = pd.DataFrame( columns = ['index', 'time', 'day', 'week', 'train_lat', 'train_long', 'trip_ID'])\n",
    "trains3 = pd.DataFrame( columns = ['index', 'time', 'day', 'week', 'train_lat', 'train_long', 'trip_ID'])\n",
    "\n",
    "tot1 = pd.DataFrame( columns =['trip_ID', 'time', 'day', 'test_lat', 'pred_lat', 'test_long', \\\n",
    "                               'pred_long', 'dist','temp_ocp','prec_train', 'prec_test', 'lengthscale', \\\n",
    "                                   'var_lat', 'var_long', 'noise', 'loss', 'rmse_lat', 'rmse_long']) \n",
    "tot2 = pd.DataFrame( columns =['trip_ID', 'time', 'day', 'test_lat', 'pred_lat', 'test_long', \\\n",
    "                               'pred_long', 'dist','temp_ocp','prec_train', 'prec_test', 'lengthscale', \\\n",
    "                                   'var_lat', 'var_long', 'noise', 'loss', 'rmse_lat', 'rmse_long']) \n",
    "tot3 = pd.DataFrame( columns =['trip_ID', 'time', 'day', 'test_lat', 'pred_lat', 'test_long', \\\n",
    "                               'pred_long', 'dist','temp_ocp','prec_train', 'prec_test', 'lengthscale', \\\n",
    "                                   'var_lat', 'var_long', 'noise', 'loss', 'rmse_lat', 'rmse_long'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_len_ls = [600, 900, 1200, 1800, 3600] # Bin lengths to test\n",
    "m_threshold = 200  # Meter threshold for determining similar trips (i.e., if origin and destination are <= 200 m apart)\n",
    "min_n = 2 # minimum number of points for a similar trip to be considered\n",
    "max_speed_kmh = 400 # for filtering out unrealistic speeds\n",
    "spatial_radius_km = 0.1 # for compressing similar points using Douglas-Peucker algorithm\n",
    "\n",
    "# Main for loop for testing each trip\n",
    "for i in range(0, len(m_df)):\n",
    "    #try:\n",
    "        if m_df['labels'][i] == 0:\n",
    "            trip1 = df[df['trip_ID'] == m_df['Id_perc'][i]].drop_duplicates(subset=['unix_start_t'], keep='first')\n",
    "\n",
    "            # Main for loop for testing each bin length\n",
    "            for j in bin_len_ls:\n",
    "                try:\n",
    "                    upper_bound = dec_floor(analysis.tempOcp(trip1, 'unix_start_t', bin_len=j))\n",
    "                    # Choose random decimal between 0 and upper bound\n",
    "                    target_ocp = dec_floor(np.random.uniform(0.1, upper_bound))\n",
    "                    # Simulate gaps in the user's data to match the target level\n",
    "                    gapped_user_data, train_index = analysis.simulate_gaps(trip1, target_ocp, unix_col='unix_start_t', bin_len=j)\n",
    "\n",
    "                    # Find all trips associated with trip 1's user\n",
    "                    trips = df[df['user_ID'] == trip1['user_ID'].iloc[0]]\n",
    "\n",
    "                    similar_trips = helper_func.loc_based_filter(trips, trip1, m_threshold=m_threshold)\n",
    "\n",
    "                    # Check if any similar trips have less than or equal to two points; if so, remove them\n",
    "                    similar_trips = similar_trips.groupby('trip_ID').filter(lambda x: len(x) >= min_n)\n",
    "\n",
    "                    # Also include trips that are one trip ID away from the trip of interest\n",
    "                    similar_trips = similar_trips.append(trips[trips['trip_ID'].isin(trip1['trip_ID'] + 1) | trips['trip_ID'].isin(trip1['trip_ID'] - 1)])\n",
    "\n",
    "                    #tdf = skmob.TrajDataFrame(similar_trips, latitude='orig_lat', longitude='orig_long', datetime='Date_Time')\n",
    "                    #f_tdf = skmob.preprocessing.filtering.filter(tdf, max_speed_kmh=max_speed_kmh, include_loops=False)\n",
    "                    #c_tdf = skmob.preprocessing.compression.compress(f_tdf, spatial_radius_km=spatial_radius_km)\n",
    "\n",
    "                    tr_df = preprocessing.dp_MultiTrip(data=similar_trips)\n",
    "                    tr_df.Multi_Trip_Preprocess(lat='orig_lat', long='orig_long', datetime='Date_Time')\n",
    "\n",
    "                    # Move 'unix_start_t' to before 'SaM'\n",
    "                    cols = list(tr_df.data.columns)\n",
    "                    cols.insert(21, cols.pop(cols.index('unix_start_t')))\n",
    "                    tr_df.data = tr_df.data.loc[:, cols]   \n",
    "\n",
    "                    scaler1 = MinMaxScaler(feature_range=(0, 100))\n",
    "                    #scaler2 = MinMaxScaler(feature_range=(0, 10))\n",
    "                    scaler3 = MinMaxScaler(feature_range=(0, 100))\n",
    "\n",
    "                    # Normalize the unix time such that it starts at 0\n",
    "                    #tr_df.X_train[:,0] = tr_df.X_train[:,0] - tr_df.X_train[:,0].min()\n",
    "                    #tr_df.X_test[:,0] = tr_df.X_test[:,0] - tr_df.X_train[:,0].min()\n",
    "\n",
    "                    unix_train = torch.tensor(np.float64(scaler1.fit_transform(tr_df.X_train[:,0].reshape(-1,1))))\n",
    "                    #secs_train = torch.tensor(scaler2.fit_transform(tr_df.X_train[:,1].reshape(-1,1))).float()\n",
    "                    unix_test = torch.tensor(np.float64(scaler1.transform(tr_df.X_test[:,0].reshape(-1,1))))\n",
    "                    #secs_test = torch.tensor(scaler2.transform(tr_df.X_test[:,1].reshape(-1,1))).float()\n",
    "\n",
    "                    X_train = torch.cat([unix_train, tr_df.X_train[:, 1::]], -1)\n",
    "                    X_test = torch.cat([unix_test, tr_df.X_test[:, 1::]], -1)\n",
    "\n",
    "                    #X_train = tr_df.X_train.float()\n",
    "                    #X_test = tr_df.X_test.float()\n",
    "\n",
    "                    y_train = torch.tensor(np.float64(scaler3.fit_transform(tr_df.y_train)))\n",
    "                    y_test = torch.tensor(np.float64(scaler3.transform(tr_df.y_test)))\n",
    "\n",
    "                    n_dims = tr_df.X_train.shape[1]\n",
    "\n",
    "                    tr_df.Multi_Trip_TrainTestSplit(trip1.iloc[0].Date_Time, trip1.iloc[-1].Date_Time, \n",
    "                                                training_index = set(gapped_user_data['unix_start_t']), lat='orig_lat', \n",
    "                                                long='orig_long', datetime='Date_Time', unix='unix_start_t', inputstart='unix_start_t', inputend=tr_df.data.columns[-1])\n",
    "\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_len_ls = [600, 900, 1200, 1800, 3600] # Bin lengths to test\n",
    "m_threshold = 200  # Meter threshold for determining similar trips (i.e., if origin and destination are <= 200 m apart)\n",
    "min_n = 2 # minimum number of points for a similar trip to be considered\n",
    "max_speed_kmh = 400 # for filtering out unrealistic speeds\n",
    "spatial_radius_km = 0.1 # for compressing similar points using Douglas-Peucker algorithm\n",
    "\n",
    "# Main for loop for testing each trip\n",
    "for i in range(0, len(m_df)):\n",
    "    try:\n",
    "        if m_df['labels'][i] == 0:\n",
    "            trip1 = df[df['trip_ID'] == m_df['trip_ID'][i]].drop_duplicates(subset=['unix_start_t'], keep='first')\n",
    "\n",
    "            # Main for loop for testing each bin length\n",
    "            for j in bin_len_ls:\n",
    "                #try:\n",
    "                    upper_bound = dec_floor(analysis.tempOcp(trip1, 'unix_start_t', bin_len=j))\n",
    "                    # Choose random decimal between 0 and upper bound\n",
    "                    target_ocp = dec_floor(np.random.uniform(0.1, upper_bound))\n",
    "                    # Simulate gaps in the user's data to match the target level\n",
    "                    gapped_user_data, train_index = analysis.simulate_gaps(trip1, target_ocp, unix_col='unix_start_t', bin_len=j)\n",
    "\n",
    "                    # Find all trips associated with trip 1's user\n",
    "                    trips = df[df['user_ID'] == trip1['user_ID'].iloc[0]]\n",
    "\n",
    "                    similar_trips = helper_func.loc_based_filter(trips, trip1, m_threshold=m_threshold)\n",
    "\n",
    "                    # Check if any similar trips have less than or equal to two points; if so, remove them\n",
    "                    similar_trips = similar_trips.groupby('trip_ID').filter(lambda x: len(x) >= min_n)\n",
    "\n",
    "                    # Also include trips that are one trip ID away from the trip of interest\n",
    "                    similar_trips = similar_trips.append(trips[trips['trip_ID'].isin(trip1['trip_ID'] + 1) | trips['trip_ID'].isin(trip1['trip_ID'] - 1)])\n",
    "\n",
    "                    tdf = skmob.TrajDataFrame(similar_trips, latitude='orig_lat', longitude='orig_long', datetime='Date_Time')\n",
    "                    f_tdf = skmob.preprocessing.filtering.filter(tdf, max_speed_kmh=max_speed_kmh, include_loops=False)\n",
    "                    fc_tdf = skmob.preprocessing.compression.compress(f_tdf, spatial_radius_km=spatial_radius_km)\n",
    "\n",
    "                    tr_df = preprocessing.dp_MultiTrip(data=similar_trips)\n",
    "                    tr_df.Multi_Trip_Preprocess(lat='lat', long='lng', datetime='datetime')\n",
    "\n",
    "                    # Move 'unix_start_t' to before 'SaM'\n",
    "                    cols = list(tr_df.data.columns)\n",
    "                    cols.insert(21, cols.pop(cols.index('unix_start_t')))\n",
    "                    tr_df.data = tr_df.data.loc[:, cols]   \n",
    "                    tr_df.data.columns\n",
    "                #except:\n",
    "                #    pass\n",
    "\n",
    "        elif m_df['labels'][i] == 1:\n",
    "            trip2 = df[df['trip_ID'] == m_df['trip_ID'][i]].drop_duplicates(subset=['unix_start_t'], keep='first')\n",
    "            \n",
    "\n",
    "            # Main for loop for testing each bin length\n",
    "            for j in bin_len_ls:\n",
    "                try:\n",
    "                    upper_bound = dec_floor(analysis.tempOcp(trip2, 'unix_start_t', bin_len=j))\n",
    "                    target_ocp = dec_floor(np.random.uniform(0.1, upper_bound))\n",
    "                    gapped_user_data, train_index = analysis.simulate_gaps(trip2, target_ocp, unix_col='unix_start_t', bin_len=j)\n",
    "                    pass\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        elif m_df['labels'][i] == 2:\n",
    "            trip3 = df[df['trip_ID'] == m_df['trip_ID'][i]].drop_duplicates(subset=['unix_start_t'], keep='first')\n",
    "        \n",
    "            # Main for loop for testing each bin length\n",
    "            for j in bin_len_ls:\n",
    "                temp_ocp3 = analysis.tempOcp(trip1, 'unix_start_t', bin_len=j)\n",
    "                if j <= temp_ocp3:\n",
    "                    try:\n",
    "                        upper_bound = dec_floor(analysis.tempOcp(trip3, 'unix_start_t', bin_len=j))\n",
    "                        target_ocp = dec_floor(np.random.uniform(0.1, upper_bound))\n",
    "                        gapped_user_data, train_index = analysis.simulate_gaps(trip3, target_ocp, unix_col='unix_start_t', bin_len=j)\n",
    "                        pass\n",
    "                    except:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip1 = df[df['trip_ID'] == m_df['Id_perc'][12]].drop_duplicates(subset=['unix_start_t'], keep='first')\n",
    "upper_bound = dec_floor(analysis.tempOcp(trip1, 'unix_start_t', bin_len=60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target temporal occupancy\n",
    "target_temp_ocp = 0.5\n",
    "\n",
    "# Simulate gaps in the user's data to match the target level\n",
    "gapped_user_data, train_index = analysis.simulate_gaps(trip1, target_temp_ocp, unix_col='unix_start_t', bin_len=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = trip1[trip1['unix_start_t'].isin(train_index)]\n",
    "test = trip1[~trip1['unix_start_t'].isin(train_index)]\n",
    "\n",
    "print(len(train))\n",
    "len(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack the local training set with the longitudinal training data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of filtered/compressed data, only retain trips whose start/end locations are within 200m of the start/end location of the testing trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all trips associated with trip 1's user\n",
    "trips = df[df['user_ID'] == trip1['user_ID'].iloc[0]]\n",
    "\n",
    "similar_trips = helper_func.loc_based_filter(trips, trip1, m_threshold=200)\n",
    "\n",
    "# Check if any similar trips have fewer than three points; if so, remove them\n",
    "similar_trips = similar_trips.groupby('trip_ID').filter(lambda x: len(x) > 3)\n",
    "\n",
    "# Also include trips that are one trip ID away from the trip of interest\n",
    "similar_trips = similar_trips.append(trips[trips['trip_ID'].isin(trip1['trip_ID'] + 1) | trips['trip_ID'].isin(trip1['trip_ID'] - 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use smaller font\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "f, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].set_title('Original Data, n = {}'.format(len(trips)))\n",
    "axs[1].set_title('OD filtered data, n = {}'.format(len(similar_trips)))\n",
    "trips.plot(x='orig_long', y='orig_lat', ax=axs[0], color='red', alpha=0.5, s=0.5, kind='scatter')\n",
    "similar_trips.plot(x='orig_long', y='orig_lat', ax=axs[1], color='blue', alpha=0.5, s=0.5, kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tdf = skmob.TrajDataFrame(similar_trips, latitude='orig_lat', longitude='orig_long', datetime='Date_Time')\n",
    "f_tdf = skmob.preprocessing.filtering.filter(tdf, max_speed_kmh=400, include_loops=True)\n",
    "fc_tdf = skmob.preprocessing.compression.compress(f_tdf, spatial_radius_km=0.1)\n",
    "#fcs_tdf = skmob.preprocessing.detection.stay_locations(fc_tdf)\n",
    "\n",
    "n_deleted_points = len(tdf) - len(f_tdf) # number of deleted points during filtering\n",
    "print(\"The number of deleted points during filtering is: {}\".format(n_deleted_points))\n",
    "\n",
    "n_deleted_points = len(f_tdf) - len(fc_tdf) # number of deleted points during compression\n",
    "print(\"The ratio of deleted points during compression to the number of original points is: {}\".format(n_deleted_points / len(similar_trips)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use smaller font\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "f, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].set_title('Original Data, n = {}'.format(len(similar_trips)))\n",
    "axs[1].set_title('Filtered and Compressed Data, n = {}'.format(len(fc_tdf)))\n",
    "tdf.plot(x='lng', y='lat', ax=axs[0], color='red', alpha=0.5, s=1, kind='scatter')\n",
    "fc_tdf.plot(x='lng', y='lat', ax=axs[1], color='blue', alpha=0.5, s=1, kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr_df = preprocessing.dp_MultiTrip(data=fc_tdf)\n",
    "tr_df = preprocessing.dp_MultiTrip(data=similar_trips)\n",
    "tr_df.Multi_Trip_Preprocess(lat='orig_lat', long='orig_long', datetime='Date_Time')\n",
    "tr_df.data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the first trip ID in the below array\n",
    "trip1['trip_ID'].unique()\n",
    "np.setdiff1d(tr_df.data['trip_ID'].unique(), trip1['trip_ID'].unique()) # Exclude the first trip ID in the below array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for each trip\n",
    "colors = ['green', 'cyan']\n",
    "# Plot the similar trips\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "f, axs = plt.subplots(1, 1, figsize=(10, 5))\n",
    "# Plot trip1\n",
    "axs.set_title('Trip 1')\n",
    "axs.plot(tr_df.data[tr_df.data['trip_ID'] == trip1['trip_ID'].iloc[0]]['lng'], tr_df.data[tr_df.data['trip_ID'] == trip1['trip_ID'].iloc[0]]['lat'], color='red', alpha=0.5, label='Trip 1')\n",
    "# in the same plot, plot the similar trips using a different color for each trip, but not the original trip\n",
    "axs.set_title('Similar Trips')\n",
    "for i, j in enumerate(np.setdiff1d(tr_df.data['trip_ID'].unique(), trip1['trip_ID'].unique())):\n",
    "    axs.plot(tr_df.data[tr_df.data['trip_ID'] == j]['lng'], tr_df.data[tr_df.data['trip_ID'] == j]['lat'], alpha=0.5, color=colors[i], label='Trip {}'.format(j))\n",
    "\n",
    "axs.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move 'unix_start_t' to before 'SaM'\n",
    "cols = list(tr_df.data.columns)\n",
    "cols.insert(22, cols.pop(cols.index('unix_start_t')))\n",
    "tr_df.data = tr_df.data.loc[:, cols]   \n",
    "tr_df.data.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df.Multi_Trip_TrainTestSplit(trip1.iloc[0].Date_Time, trip1.iloc[-1].Date_Time, \n",
    "                                training_index = set(gapped_user_data['unix_start_t']), lat='orig_lat', \n",
    "                                long='orig_long', datetime='Date_Time', unix='unix_start_t', inputstart='SaM', inputend=tr_df.data.columns[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's confirm that there is no test data in the training set, and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The length of the testing set is \" + str(len(test)))\n",
    "print(\"The number of testing points that are NOT in the training set is \" + str(test['unix_start_t'].isin(tr_df.train['unix_min']).value_counts().item()))\n",
    "print(\"The length of the training set is \" + str(len(train)))\n",
    "\n",
    "train['unix_start_t'].isin(tr_df.train['unix_min']).value_counts()\n",
    "# There was indeed one point that was in the original training set that is not in the new training set (likely due to the filtering)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "scaler1 = MinMaxScaler(feature_range=(0, 100))\n",
    "#scaler2 = MinMaxScaler(feature_range=(0, 10))\n",
    "scaler3 = MinMaxScaler(feature_range=(0, 100))\n",
    "\n",
    "# Normalize the unix time such that it starts at 0\n",
    "#tr_df.X_train[:,0] = tr_df.X_train[:,0] - tr_df.X_train[:,0].min()\n",
    "#tr_df.X_test[:,0] = tr_df.X_test[:,0] - tr_df.X_train[:,0].min()\n",
    "\n",
    "unix_train = torch.tensor(np.float64(scaler1.fit_transform(tr_df.X_train[:,0].reshape(-1,1))))\n",
    "#secs_train = torch.tensor(scaler2.fit_transform(tr_df.X_train[:,1].reshape(-1,1))).float()\n",
    "unix_test = torch.tensor(np.float64(scaler1.transform(tr_df.X_test[:,0].reshape(-1,1))))\n",
    "#secs_test = torch.tensor(scaler2.transform(tr_df.X_test[:,1].reshape(-1,1))).float()\n",
    "\n",
    "X_train = torch.cat([unix_train, tr_df.X_train[:, 1::]], -1)\n",
    "X_test = torch.cat([unix_test, tr_df.X_test[:, 1::]], -1)\n",
    "\n",
    "#X_train = tr_df.X_train.float()\n",
    "#X_test = tr_df.X_test.float()\n",
    "\n",
    "y_train = torch.tensor(np.float64(scaler3.fit_transform(tr_df.y_train)))\n",
    "y_test = torch.tensor(np.float64(scaler3.transform(tr_df.y_test)))\n",
    "\n",
    "n_dims = tr_df.X_train.shape[1]\n",
    "print(n_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.likelihoods.multitask_gaussian_likelihood import _MultitaskGaussianLikelihoodBase\n",
    "from gpytorch.likelihoods.noise_models import FixedGaussianNoise\n",
    "from gpytorch.lazy import ConstantDiagLazyTensor, KroneckerProductLazyTensor\n",
    "\n",
    "class FixedTaskNoiseMultitaskLikelihood(_MultitaskGaussianLikelihoodBase):\n",
    "    def __init__(self, noise, *args, **kwargs):\n",
    "        noise_covar = FixedGaussianNoise(noise=noise)\n",
    "        super().__init__(noise_covar=noise_covar, *args, **kwargs)\n",
    "        self.has_global_noise = False\n",
    "        self.has_task_noise = False\n",
    "        \n",
    "    def _shaped_noise_covar(self, shape, add_noise=True, *params, **kwargs):\n",
    "        if not self.has_task_noise:\n",
    "            data_noise = self.noise_covar(*params, shape=torch.Size((shape[:-2],)), **kwargs)\n",
    "            eye = torch.ones(1, device=data_noise.device, dtype=data_noise.dtype)\n",
    "            # TODO: add in a shape for batched models\n",
    "            task_noise = ConstantDiagLazyTensor(\n",
    "                eye, diag_shape=torch.Size((self.num_tasks,))\n",
    "            )\n",
    "            return KroneckerProductLazyTensor(data_noise, task_noise)\n",
    "        else:\n",
    "            # TODO: copy over pieces from MultitaskGaussianLikelihood\n",
    "            raise NotImplementedError(\"Task noises not supported yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_var = torch.rand(y_train.shape[0]).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)\n",
    "\n",
    "model = GP.MTGPRegressor(X_train, y_train, \n",
    "                         ScaleKernel(RQ(active_dims = [0])) + ScaleKernel( RQ(ard_num_dims = n_dims - 1, active_dims=list(range(1, n_dims)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the length of gaps in the training set, set x_lim to 200 to see the distribution better\n",
    "tr_df.data.groupby('trip_ID')['unix_start_t'].apply(lambda x: x.diff()).plot.hist(bins=200, figsize=(10,5), xlim=(0,200))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial lengthscale guess as half the average length of gap in training set\n",
    "init_lengthscale = similar_trips.groupby('trip_ID')['unix_start_t'].apply(lambda x: x.diff().mean()).mean() / 2\n",
    "\n",
    "# Initialize model parameters\n",
    "#scaled_unix_lengthscale = scaler1.transform(torch.tensor(init_lengthscale).reshape(-1,1)).item()\n",
    "#scaled_SaM_lengthscale = scaler2.transform(torch.tensor(init_lengthscale).reshape(-1,1)).item()\n",
    "\n",
    "categorical_inits = np.ones(n_dims - 1)\n",
    "#init_params = np.insert(categorical_inits, 0, scaled_unix_lengthscale)\n",
    "init_params = np.insert(categorical_inits, 0, init_lengthscale)\n",
    "\n",
    "model.covar_module.data_covar_module.kernels[0].base_kernel.lengthscale = init_lengthscale\n",
    "model.covar_module.data_covar_module.kernels[1].base_kernel.lengthscale = torch.tensor(categorical_inits).float()\n",
    "\n",
    "model.covar_module.data_covar_module.kernels[1].outputscale = torch.tensor(0.2)\n",
    "model.covar_module.data_covar_module.kernels[0].outputscale = torch.tensor(0.8)\n",
    " #= torch.tensor(init_params).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls, mll = GP.training(model, X_train, y_train, lr=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model parameters (converting back to original scale)\n",
    "print(model.covar_module.data_covar_module.kernels[0].base_kernel.lengthscale)\n",
    "print(model.covar_module.data_covar_module.kernels[1].base_kernel.lengthscale)\n",
    "print(model.covar_module.data_covar_module.kernels[0].outputscale)\n",
    "print(model.covar_module.data_covar_module.kernels[1].outputscale)\n",
    "print(model.likelihood.noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    log_ll = mll(model(X_train), y_train) * X_train.shape[0]\n",
    "            \n",
    "N = X_train.shape[0]\n",
    "m = sum(p.numel() for p in model.hyperparameters())\n",
    "bic = -2 * log_ll + m * np.log(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, mean = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_preds(mean, tr_df.date_train, \n",
    "                 tr_df.date_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use smaller font\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "f, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.set_title('Predictions')\n",
    "pd.DataFrame(mean.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='red', alpha=0.5, s=1)\n",
    "pd.DataFrame(y_test.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='blue', alpha=0.5, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.average_eval(pd.Series(y_test[:,0]), pd.Series(y_test[:,1]), pd.Series(mean[:,0]), pd.Series(mean[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27d8c82720d960df87743be14161e6f7351af57d8b5f04fea83f01e4b383fdff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
