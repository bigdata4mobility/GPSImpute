{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skmob\n",
    "import pandas as pd\n",
    "import skmob.measures.individual as ind_measure\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.kernels import RQKernel as RQ, RBFKernel as SE, \\\n",
    "PeriodicKernel as PER, ScaleKernel, LinearKernel as LIN, MaternKernel as MAT, \\\n",
    "SpectralMixtureKernel as SMK, PiecewisePolynomialKernel as PPK, CylindricalKernel as CYL\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gpytorch.constraints import Interval\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import statistics as stats\n",
    "\n",
    "# Import intra-package scripts\n",
    "import utils.helper_func as helper_func\n",
    "import utils.GP as GP\n",
    "from utils.helper_func import dec_floor\n",
    "import mobileDataToolkit.analysis as analysis\n",
    "import mobileDataToolkit.preprocessing_v2 as preprocessing\n",
    "import mobileDataToolkit.methods as methods\n",
    "import mobileDataToolkit.metrics as metrics\n",
    "\n",
    "# Import benchmarks\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Data\\\\seattle_2000_all_obs_sampled.csv\"\n",
    "df = pd.read_csv(file_path, header=0)\n",
    "\n",
    "# Add month column\n",
    "df['month'] = pd.DatetimeIndex(df['datetime']).month\n",
    "\n",
    "# Group by user ID, find month with third most observations (average)\n",
    "df_m = df.groupby('UID').apply(lambda x: x[x['month'] == x['month'].value_counts().index[2]])\n",
    "\n",
    "df_m = df_m.reset_index(drop=True)\n",
    "\n",
    "max_speed_kmh = 400 # for filtering out unrealistic speeds\n",
    "spatial_radius_km = 0.3 # for compressing similar points using Douglas-Peucker algorithm\n",
    "bin_len_ls = [10080, 1440, 360, 60, 30, 15] # Bin lengths to test: 1 week, 1 day, 6 hours, 1 hour, 30 min, 15 min\n",
    "init_period_len_1 = 60*8 # 8 hours\n",
    "init_period_len_2 = 60*24 # 24 hours\n",
    "\n",
    "runtimes = []\n",
    "\n",
    "\n",
    "for j in bin_len_ls:\n",
    "    bin_len = j\n",
    "    # Create a directory for each bin length\n",
    "    if not os.path.exists('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Results\\\\' + str(bin_len)):\n",
    "        os.makedirs('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Results\\\\' + str(bin_len))\n",
    "    print(\"Starting tests on bin length = {}\".format(bin_len))\n",
    "    # Main loop that will go through each user ID, create a directory for each user, etc.\n",
    "    for i in df_m.UID.unique():\n",
    "        try:\n",
    "            \n",
    "            df_curr = df_m[df_m.UID == i]\n",
    "\n",
    "            tdf = skmob.TrajDataFrame(df_curr, latitude='orig_lat', longitude='orig_long', datetime='datetime')\n",
    "            f_tdf = skmob.preprocessing.filtering.filter(tdf, max_speed_kmh=max_speed_kmh, include_loops=False)\n",
    "            # Print the difference in number of rows\n",
    "            print(\"Number of rows before filtering: {}\".format(tdf.shape[0]))\n",
    "            print(\"Number of rows after filtering: {}\".format(f_tdf.shape[0]))\n",
    "            fc_tdf = skmob.preprocessing.compression.compress(f_tdf, spatial_radius_km=spatial_radius_km)\n",
    "            # Print the difference in number of rows\n",
    "            print(\"Number of rows after compression: {}\".format(fc_tdf.shape[0]))\n",
    "            # Remove data points with uncertainty > 100m\n",
    "            fcu_tdf = fc_tdf[fc_tdf['orig_unc'] <= 100]\n",
    "            # Print the difference in number of rows\n",
    "            print(\"Number of rows after uncertainty filtering: {}\".format(fcu_tdf.shape[0]))\n",
    "            df_curr = fcu_tdf\n",
    "\n",
    "            # Calculate sci-kit mobility metrics\n",
    "            df_curr_metrics = helper_func.skmob_metric_calcs(df_curr, method='GT', lat='lat', long='lng', datetime='datetime')\n",
    "\n",
    "            # Remove duplicates in the unix column\n",
    "            df_curr = df_curr.drop_duplicates(subset=['unix_min'])\n",
    "\n",
    "            curr_ocp = analysis.tempOcp(df_curr, 'unix_min', bin_len=bin_len)\n",
    "\n",
    "            upper_bound = dec_floor(curr_ocp)\n",
    "            \n",
    "            # See current temporal occupancy\n",
    "            print(\"Current temporal occupancy: {}\".format(curr_ocp))\n",
    "            while True:\n",
    "                try:\n",
    "                    if curr_ocp <= 0.1:\n",
    "                        target_ocp = np.random.uniform(0, curr_ocp)\n",
    "                    else:\n",
    "                        # Choose random decimal between 0 and upper bound\n",
    "                        target_ocp = dec_floor(np.random.uniform(0.1, upper_bound))\n",
    "                    print(\"Target temporal occupancy: {}\".format(target_ocp))\n",
    "                    # Simulate gaps in the user's data to match the target level\n",
    "                    gapped_user_data, train_index, new_ocp = analysis.simulate_gaps(df_curr, target_ocp, unix_col='unix_min', bin_len= bin_len)\n",
    "                except:\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "            # Change name of 'lat' and 'lon' columns to 'orig_lat' and 'orig_long'\n",
    "            df_curr = df_curr.rename(columns={'lat': 'orig_lat', 'lng': 'orig_long'})\n",
    "\n",
    "            # Create MultiTrip object\n",
    "            curr_mt = preprocessing.dp_MultiTrip(data=df_curr)\n",
    "            curr_mt.Multi_Trip_Preprocess(lat='orig_lat', long='orig_long', datetime='datetime')\n",
    "\n",
    "            # Move 'unix_start_t' to before 'SaM'\n",
    "            cols = list(curr_mt.data.columns)\n",
    "            cols.insert(16, cols.pop(cols.index('unix_min')))\n",
    "            curr_mt.data = curr_mt.data.loc[:, cols] \n",
    "            # Print data columns\n",
    "            print(curr_mt.data.columns)\n",
    "\n",
    "            curr_mt.Multi_Trip_TrainTestSplit(test_start_date=None, test_end_date=None, \n",
    "                                        training_index = set(gapped_user_data['unix_min']), lat='orig_lat', \n",
    "                                        long='orig_long', datetime='datetime', unix='unix_min', inputstart='unix_min', \n",
    "                                        inputend=curr_mt.data.columns[-1])\n",
    "\n",
    "            n_train = len(curr_mt.X_train[:,0])\n",
    "            n_test = len(curr_mt.X_test[:,0])\n",
    "            n_dims = curr_mt.X_train.shape[1]\n",
    "\n",
    "            # See number of points in training and test sets\n",
    "            print(\"Number of points in training set: {}\".format(n_train))\n",
    "            print(\"Number of points in test set: {}\".format(n_test))\n",
    "            print(\"Number of input dimensions: {}\".format(n_dims))\n",
    "\n",
    "            # If there are no points in the test set, skip to the next user\n",
    "            if n_test == 0:\n",
    "                print(\"No points in test set. Skipping to next user.\")\n",
    "                continue\n",
    "\n",
    "            # Visualize the training and test data in two subplots, one lat vs time and one long vs time\n",
    "            plt.rcParams.update({'font.size': 9})\n",
    "            plt.rcParams.update({'font.family': 'serif'})\n",
    "            fig1, ax = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n",
    "            ax[0].scatter(curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_lat'],\n",
    "                            color='blue', label='Training data', s=1)\n",
    "            ax[0].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_lat'],\n",
    "                            color='red', label='Test data', s=1)\n",
    "            ax[0].set_ylabel('Latitude')\n",
    "            ax[1].scatter(curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_long'],\n",
    "                            color='blue', label='Training data', s=1)\n",
    "            ax[1].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_long'],\n",
    "                            color='red', label='Test data', s=1)\n",
    "            ax[1].set_xlabel('Time')\n",
    "            ax[1].set_ylabel('Longitude')\n",
    "            ax[1].legend()\n",
    "\n",
    "\n",
    "            mean_lat = curr_mt.y_train[:,0].mean()\n",
    "            mean_long = curr_mt.y_train[:,1].mean()\n",
    "            std_lat = curr_mt.y_train[:,0].std()\n",
    "            std_long = curr_mt.y_train[:,1].std()\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            y_train_scaled = torch.tensor(np.float64(scaler.fit_transform(curr_mt.y_train)))\n",
    "            y_test_scaled = torch.tensor(np.float64(scaler.transform(curr_mt.y_test)))\n",
    "\n",
    "            likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)\n",
    "\n",
    "            model = GP.MTGPRegressor(curr_mt.X_train, y_train_scaled, \n",
    "                                    ScaleKernel(RQ(ard_num_dims=n_dims) * PER(active_dims=[0])) + \n",
    "                                    ScaleKernel(RQ(ard_num_dims=n_dims) * PER(active_dims=[0])))\n",
    "\n",
    "            # Set initial lengthscale guess as half the average length of gap in training set\n",
    "            init_lengthscale = curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'].diff().mean() / 2 \n",
    "            initializations = np.ones(n_dims - 1)\n",
    "            initializations = np.insert(initializations, 0, init_lengthscale)\n",
    "            model.covar_module.data_covar_module.kernels[0].base_kernel.kernels[0].lengthscale = initializations\n",
    "            model.covar_module.data_covar_module.kernels[1].base_kernel.kernels[0].lengthscale = initializations\n",
    "\n",
    "            # Set initial period lengths\n",
    "            model.covar_module.data_covar_module.kernels[0].base_kernel.kernels[1].period_length = init_period_len_1\n",
    "            model.covar_module.data_covar_module.kernels[1].base_kernel.kernels[1].period_length = init_period_len_2\n",
    "\n",
    "            # Train model\n",
    "            start = time.time()\n",
    "            ls, mll = GP.training(model, curr_mt.X_train, y_train_scaled, lr=0.3, n_epochs=150)\n",
    "            end = time.time()\n",
    "            runtime = end - start\n",
    "            runtimes.append(runtime)\n",
    "\n",
    "            iters = range(0, len(ls))\n",
    "            fig2, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "            ax.plot(iters, ls, 'g')\n",
    "            ax.set_title('Training Loss')\n",
    "            ax.set_xlabel('Iteration')\n",
    "            ax.set_ylabel('Loss')\n",
    "            ax.legend()\n",
    "            \n",
    "            mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                log_ll = mll(model(curr_mt.X_train), y_train_scaled) * curr_mt.X_train.shape[0]\n",
    "                        \n",
    "            N = curr_mt.X_train.shape[0]\n",
    "            m = sum(p.numel() for p in model.hyperparameters())\n",
    "            bic = -2 * log_ll + m * np.log(N)\n",
    "\n",
    "            predictions, mean = model.predict(curr_mt.X_test)\n",
    "\n",
    "            # Use smaller font\n",
    "            plt.rcParams.update({'font.size': 9})\n",
    "            # Make the font nicer\n",
    "            plt.rcParams.update({'font.family': 'serif'})\n",
    "            fig3, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "            ax.set_title('Predictions')\n",
    "            pd.DataFrame(mean.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='red', alpha=0.5, s=0.4, label='Predictions')\n",
    "            pd.DataFrame(y_test_scaled.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='blue', alpha=0.5, s=0.4, label='Actual')\n",
    "            ax.set_xlabel('Longitude')\n",
    "            ax.set_ylabel('Latitude')\n",
    "\n",
    "            # Unix time for benchmarks\n",
    "            unix_min_tr = np.array(curr_mt.X_train[:,0]).astype(int)\n",
    "            unix_min_te = np.array(curr_mt.X_test[:,0]).astype(int)\n",
    "\n",
    "            # Model results\n",
    "            mtgp_res = metrics.average_eval(pd.Series(y_test_scaled[:,0]), pd.Series(y_test_scaled[:,1]), pd.Series(mean[:,0]), pd.Series(mean[:,1]))\n",
    "\n",
    "            # Convert mean predictions back to original scale in lat/long\n",
    "            orig_preds = scaler.inverse_transform(mean.reshape(-1,2))\n",
    "\n",
    "            GP_full_preds_df = helper_func.preds_to_full_df(preds_lat=orig_preds[:,0], preds_long=orig_preds[:,1], \n",
    "                                                        test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "\n",
    "            mtgp_tdf = helper_func.skmob_metric_calcs(GP_full_preds_df, method='GP', lat='lat', long='long', datetime='datetime')\n",
    "\n",
    "            mtgp_rec_acc = helper_func.matrix_acc(mtgp_tdf.recency_gp_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "            mtgp_freq_rank_acc = helper_func.matrix_acc(mtgp_tdf.freq_rank_gp_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "            try:\n",
    "                # Linear Interpolation\n",
    "                print(\"Running Linear Interpolation...\")\n",
    "                LI_preds_lat, LI_preds_long = methods.LI(curr_mt.X_train[:,0], curr_mt.X_test[:,0], y_train_scaled, y_test_scaled)\n",
    "\n",
    "                LI_preds_df = pd.DataFrame(LI_preds_lat, columns=['lat'])\n",
    "                LI_preds_df['long'] = LI_preds_long\n",
    "\n",
    "                LI_preds_origs = scaler.inverse_transform(LI_preds_df)\n",
    "                \n",
    "                LI_full_preds_df = helper_func.preds_to_full_df(preds_lat=LI_preds_origs[:,0], preds_long=LI_preds_origs[:,1], \n",
    "                                                            test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "                \n",
    "                LI_tdf = helper_func.skmob_metric_calcs(LI_full_preds_df, method='LI', lat='lat', long='long', datetime='datetime')\n",
    "                LI_res = metrics.average_eval(np.array(y_test_scaled[:,0]), np.array(y_test_scaled[:,1]), LI_preds_lat, LI_preds_long)\n",
    "\n",
    "                LI_rec_acc = helper_func.matrix_acc(LI_tdf.recency_li_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "                LI_freq_rank_acc = helper_func.matrix_acc(LI_tdf.freq_rank_li_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "            except:\n",
    "                print(\"Error in LI\")\n",
    "                LI_res = None\n",
    "                LI_rec_acc = None\n",
    "                LI_freq_rank_acc = None\n",
    "            \n",
    "            lat = pd.Series(y_train_scaled[:,0].tolist(), unix_min_tr)\n",
    "            lat_t = pd.Series(y_test_scaled[:,0].tolist(), unix_min_te)\n",
    "            # Replace duplicates (in time) with the mean of the two values\n",
    "            lat = lat.groupby(lat.index).mean().reset_index()\n",
    "            lat = pd.Series(lat[0].tolist(), lat['index'].tolist())\n",
    "            lat_tc = lat_t.groupby(lat_t.index).mean().reset_index()\n",
    "            lat_tc = pd.Series(lat_tc[0].tolist(), lat_tc['index'].tolist())\n",
    "            # Replace zeroes with positives close to zero\n",
    "            lat.replace(0, 0.000000001, inplace=True)\n",
    "\n",
    "            lon = pd.Series(y_train_scaled[:,1].tolist(), unix_min_tr)\n",
    "            lon_t = pd.Series(y_test_scaled[:,1].tolist(),unix_min_te)\n",
    "            # Replace duplicates (in time) with the mean of the two values\n",
    "            lon = lon.groupby(lon.index).mean().reset_index()\n",
    "            lon = pd.Series(lon[0].tolist(), lon['index'].tolist())\n",
    "            lon_tc = lon_t.groupby(lon_t.index).mean().reset_index()\n",
    "            lon_tc = pd.Series(lon_tc[0].tolist(), lon_tc['index'].tolist())\n",
    "            # Replace zeroes with positives close to zero\n",
    "            lon.replace(0, 0.000000001, inplace=True)\n",
    "\n",
    "            # SES model\n",
    "            ses_smoothing_level = 0.1\n",
    "            print(\"Running Simple Exponential Smoothing...\")\n",
    "            ses_lat = SimpleExpSmoothing(lat, initialization_method=\"heuristic\").fit(smoothing_level=ses_smoothing_level, optimized=True)\n",
    "            pred_lat_ses = ses_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "            pred_lat_comp_ses = pred_lat_ses[pred_lat_ses.index.isin(unix_min_te)]\n",
    "\n",
    "            ses_lon = SimpleExpSmoothing(lon, initialization_method=\"heuristic\").fit(smoothing_level=ses_smoothing_level, optimized=True)\n",
    "            pred_lon_ses = ses_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "            pred_lon_comp_ses = pred_lon_ses[pred_lon_ses.index.isin(unix_min_te)]\n",
    "\n",
    "            ses_preds_df = pd.DataFrame(pred_lat_comp_ses, columns=['lat'])\n",
    "            ses_preds_df['long'] = pred_lon_comp_ses\n",
    "\n",
    "            ses_preds_origs = scaler.inverse_transform(ses_preds_df)\n",
    "\n",
    "            ses_full_preds_df = helper_func.preds_to_full_df(preds_lat=ses_preds_origs[:,0], preds_long=ses_preds_origs[:,1],\n",
    "                                                        test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "            ses_tdf = helper_func.skmob_metric_calcs(ses_full_preds_df, method='ses', lat='lat', long='long', datetime='datetime')\n",
    "            ses_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_ses, pred_lon_comp_ses)\n",
    "\n",
    "            ses_rec_acc = helper_func.matrix_acc(ses_tdf.recency_ses_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "            ses_freq_rank_acc = helper_func.matrix_acc(ses_tdf.freq_rank_ses_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "            # Holt model\n",
    "            holt_smoothing_level_lat=0.2\n",
    "            holt_smoothing_slope_lat=0.045\n",
    "\n",
    "            print(\"Running Holt-Winters model...\")\n",
    "            holt_lat = Holt(lat, damped_trend=True, initialization_method=\"estimated\").fit(smoothing_level=holt_smoothing_level_lat, smoothing_slope=holt_smoothing_slope_lat)\n",
    "            pred_lat_holt = holt_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "            pred_lat_comp_holt = pred_lat_holt[pred_lat_holt.index.isin(unix_min_te)]\n",
    "\n",
    "            holt_smoothing_level_lon=0.1\n",
    "            holt_smoothing_slope_lon=0.0307\n",
    "\n",
    "            holt_lon = Holt(lon, damped_trend=True, initialization_method=\"estimated\").fit(smoothing_level=holt_smoothing_level_lon, smoothing_slope=holt_smoothing_slope_lon)\n",
    "            pred_lon_holt = holt_lon.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "            pred_lon_comp_holt = pred_lon_holt[pred_lon_holt.index.isin(unix_min_te)]\n",
    "\n",
    "            holt_preds_df = pd.DataFrame(pred_lat_comp_holt, columns=['lat'])\n",
    "            holt_preds_df['long'] = pred_lon_comp_holt\n",
    "\n",
    "            holt_preds_origs = scaler.inverse_transform(holt_preds_df)\n",
    "\n",
    "            holt_full_preds_df = helper_func.preds_to_full_df(preds_lat=holt_preds_origs[:,0], preds_long=holt_preds_origs[:,1],\n",
    "                                                        test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "            holt_tdf = helper_func.skmob_metric_calcs(holt_full_preds_df, method='holt', lat='lat', long='long', datetime='datetime')\n",
    "\n",
    "            holt_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_holt, pred_lon_comp_holt)\n",
    "\n",
    "            holt_rec_acc = helper_func.matrix_acc(holt_tdf.recency_holt_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "            holt_freq_rank_acc = helper_func.matrix_acc(holt_tdf.freq_rank_holt_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "            try:\n",
    "                # Exponential Smoothing\n",
    "                es_seasonal_periods=24\n",
    "\n",
    "                print(\"Running Exponential Smoothing...\")\n",
    "                es = ExponentialSmoothing(lat, seasonal_periods=es_seasonal_periods, trend='add', seasonal='add', damped_trend=True, use_boxcox=False, initialization_method='estimated').fit()\n",
    "                pred_lat_es = es.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "                pred_lat_comp_es = pred_lat_es[pred_lat_es.index.isin(unix_min_te)]\n",
    "\n",
    "                es = ExponentialSmoothing(lon, seasonal_periods=es_seasonal_periods, trend='add', seasonal='add', damped_trend=True, use_boxcox=False, initialization_method='estimated').fit()\n",
    "                pred_lon_es = es.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "                pred_lon_comp_es = pred_lon_es[pred_lon_es.index.isin(unix_min_te)]\n",
    "\n",
    "                es_preds_df = pd.DataFrame(pred_lat_comp_es, columns=['lat'])\n",
    "                es_preds_df['long'] = pred_lon_comp_es\n",
    "\n",
    "                es_preds_origs = scaler.inverse_transform(es_preds_df)\n",
    "\n",
    "                es_full_preds_df = helper_func.preds_to_full_df(preds_lat=es_preds_origs[:,0], preds_long=es_preds_origs[:,1],\n",
    "                                                            test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "                es_tdf = helper_func.skmob_metric_calcs(es_full_preds_df, method='es', lat='lat', long='long', datetime='datetime')\n",
    "                es_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_es, pred_lon_comp_es)\n",
    "\n",
    "                es_rec_acc = helper_func.matrix_acc(es_tdf.recency_es_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "                es_freq_rank_acc = helper_func.matrix_acc(es_tdf.freq_rank_es_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "            except:\n",
    "                print(\"Exponential Smoothing failed\")\n",
    "                es_res = None\n",
    "                es_rec_acc = None\n",
    "                es_freq_rank_acc = None\n",
    "            \n",
    "            try:\n",
    "                # ARIMA\n",
    "                arima_order = (1,1,0)\n",
    "                print(\"Running ARIMA...\")\n",
    "                arima = ARIMA(lat, order=arima_order).fit()\n",
    "                pred_lat_arima = arima.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "                pred_lat_comp_arima = pred_lat_arima[pred_lat_arima.index.isin(unix_min_te)]\n",
    "\n",
    "                arima = ARIMA(lon, order=arima_order).fit()\n",
    "                pred_lon_arima = arima.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "                pred_lon_comp_arima = pred_lon_arima[pred_lon_arima.index.isin(unix_min_te)]\n",
    "\n",
    "                arima_preds_df = pd.DataFrame(pred_lat_comp_arima, columns=['lat'])\n",
    "                arima_preds_df['long'] = pred_lon_comp_arima\n",
    "\n",
    "                arima_preds_origs = scaler.inverse_transform(arima_preds_df)\n",
    "                \n",
    "                arima_full_preds_df = helper_func.preds_to_full_df(preds_lat=arima_preds_origs[:,0], preds_long=arima_preds_origs[:,1],\n",
    "                                                            test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "                arima_tdf = helper_func.skmob_metric_calcs(arima_full_preds_df, method='arima', lat='lat', long='long', datetime='datetime')\n",
    "                arima_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_arima, pred_lon_comp_arima)\n",
    "\n",
    "                arima_rec_acc = helper_func.matrix_acc(arima_tdf.recency_arima_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "                arima_freq_rank_acc = helper_func.matrix_acc(arima_tdf.freq_rank_arima_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "            except:\n",
    "                print(\"ARIMA failed\")\n",
    "                arima_res = None\n",
    "                arima_rec_acc = None\n",
    "                arima_freq_rank_acc = None\n",
    "\n",
    "            try:\n",
    "                # SARIMAX\n",
    "                sarimax_order = (1,0,0)\n",
    "                print(\"Running SARIMAX...\")\n",
    "                sarimax_seasonal_order = (1, 1, 1, 24)\n",
    "                sarimax_lat = SARIMAX(lat, order=sarimax_order, seasonal_order=sarimax_seasonal_order).fit(disp=False)\n",
    "                pred_lat_sar = sarimax_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "                pred_lat_comp_sar = pred_lat_sar[pred_lat_sar.index.isin(unix_min_te)]\n",
    "\n",
    "                sarimax_lon = SARIMAX(lon, order=sarimax_order, seasonal_order=sarimax_seasonal_order).fit(disp=False)\n",
    "                pred_lon_sar = sarimax_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "                pred_lon_comp_sar = pred_lon_sar[pred_lon_sar.index.isin(unix_min_te)]\n",
    "\n",
    "                sarimax_preds_df = pd.DataFrame(pred_lat_comp_sar, columns=['lat'])\n",
    "                sarimax_preds_df['long'] = pred_lon_comp_sar\n",
    "\n",
    "                sarimax_preds_origs = scaler.inverse_transform(sarimax_preds_df)\n",
    "\n",
    "                sarimax_full_preds_df = helper_func.preds_to_full_df(preds_lat=sarimax_preds_origs[:,0], preds_long=sarimax_preds_origs[:,1],\n",
    "                                                            test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "                sarimax_tdf = helper_func.skmob_metric_calcs(sarimax_full_preds_df, method='sarimax', lat='lat', long='long', datetime='datetime')\n",
    "                sarimax_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_sar, pred_lon_comp_sar)\n",
    "\n",
    "                sarimax_rec_acc = helper_func.matrix_acc(sarimax_tdf.recency_sarimax_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "                sarimax_freq_rank_acc = helper_func.matrix_acc(sarimax_tdf.freq_rank_sarimax_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "            except:\n",
    "                print(\"SARIMAX failed\")\n",
    "                sarimax_res = None\n",
    "                sarimax_rec_acc = None\n",
    "                sarimax_freq_rank_acc = None\n",
    "\n",
    "            # Create a directory for each user\n",
    "            if not os.path.exists('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Results\\\\' + str(bin_len) + '\\\\' + str(i)):\n",
    "                os.makedirs('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Results\\\\' + str(bin_len) + '\\\\' + str(i))\n",
    "            else:\n",
    "                # If directory already exists, then prediction has already been done for this user, so skip\n",
    "                print(\"User {} already exists\".format(i))\n",
    "                continue\n",
    "            # Navigate to the directory\n",
    "            os.chdir('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Results\\\\' + str(bin_len) + '\\\\' + str(i))\n",
    "\n",
    "            # Save figure to file\n",
    "            fig1.savefig('train_test_sets_plot.png', dpi=300)\n",
    "            fig2.savefig('training_loss_plot.png', dpi=300)\n",
    "            fig3.savefig('predictions_plot.png', dpi=300)\n",
    "\n",
    "            # Create dictionary to store parameters\n",
    "            params = {\n",
    "                'max_speed_kmh': max_speed_kmh,\n",
    "                'spatial_radius_km': spatial_radius_km,\n",
    "                'bin_len': bin_len,\n",
    "                'tdf.shape[0]': tdf.shape[0],\n",
    "                'f_tdf.shape[0]': f_tdf.shape[0],\n",
    "                'fc_tdf.shape[0]': fc_tdf.shape[0],\n",
    "                'fcu_tdf.shape[0]': fcu_tdf.shape[0],\n",
    "                'curr_ocp': curr_ocp,\n",
    "                'target_ocp': target_ocp,\n",
    "                'new_ocp': new_ocp,\n",
    "                'n_train': n_train,\n",
    "                'n_test': n_test,\n",
    "                'n_dims': n_dims,\n",
    "                'mean_lat': mean_lat,\n",
    "                'mean_long': mean_long,\n",
    "                'std_lat': std_lat,\n",
    "                'std_long': std_long,\n",
    "                'init_lengthscale': init_lengthscale,\n",
    "                'init_period_len_1': init_period_len_1,\n",
    "                'init_period_len_2': init_period_len_2,\n",
    "                'log_ll': log_ll,\n",
    "                'm': m,\n",
    "                'bic': bic,\n",
    "                'gp_runtime': runtime,\n",
    "                'ses_smoothing_level': ses_smoothing_level,\n",
    "                'holt_smoothing_level_lat': holt_smoothing_level_lat,\n",
    "                'holt_smoothing_slope_lat': holt_smoothing_slope_lat,\n",
    "                'holt_smoothing_level_lon': holt_smoothing_level_lon,\n",
    "                'holt_smoothing_slope_lon': holt_smoothing_slope_lon,\n",
    "                'es_seasonal_periods': es_seasonal_periods,\n",
    "                'arima_order': arima_order,\n",
    "                'sarimax_order': sarimax_order,\n",
    "                'sarimax_seasonal_order': sarimax_seasonal_order\n",
    "            }\n",
    "\n",
    "            print(\"Saving parameters...\")\n",
    "\n",
    "            # See the differences in metric results\n",
    "            \n",
    "            # Convert all values to float, except for tuples\n",
    "            for k, v in params.items():\n",
    "                try:\n",
    "                    params[k] = float(v)\n",
    "                except TypeError:\n",
    "                    pass\n",
    "            # Write params to a file\n",
    "            with open('params_' + str(i) + '.json', 'w') as fp:\n",
    "                json.dump(params, fp)\n",
    "\n",
    "            # Create dataframe to store parameters\n",
    "            params_df = pd.DataFrame.from_dict(params, orient='index')\n",
    "            params_df.columns = ['value']\n",
    "            params_df.to_csv('params_' + str(i) + '.csv')\n",
    "\n",
    "            # Create dataframe to store results\n",
    "            results = pd.DataFrame.from_dict([mtgp_res, ses_res, holt_res, es_res, arima_res, sarimax_res])\n",
    "            results['model'] = ['MTGP', 'SES', 'Holt', 'ES', 'ARIMA', 'SARIMAX']\n",
    "            results = results.set_index('model')\n",
    "\n",
    "            # Create dataframe to store scalar scikit-mobility metrics\n",
    "            skmob_metrics_df = pd.DataFrame(columns=['no_loc', 'rg', 'k_rg',    \n",
    "                                                'spat_burst', 'rand_entr', \n",
    "                                                'real_entr', 'uncorr_entr',\n",
    "                                                'max_dist', 'dist_straight', 'max_dist_home', \n",
    "                                                'recency', 'freq_rank'])\n",
    "\n",
    "            skmob_metrics_df['methods'] = ['MTGP', 'SES', 'Holt', 'ES', 'ARIMA', 'SARIMAX','LI', 'Ground Truth']\n",
    "            # Make methods the index\n",
    "            skmob_metrics_df = skmob_metrics_df.set_index('methods')\n",
    "\n",
    "            skmob_metrics_df.iloc[0] = mtgp_tdf.no_loc_gp_pred, mtgp_tdf.rg_gp_pred, mtgp_tdf.k_rg_gp_pred, mtgp_tdf.spat_burst_gp_pred, mtgp_tdf.rand_entr_gp_pred, mtgp_tdf.real_entr_gp_pred, mtgp_tdf.uncorr_entr_gp_pred, mtgp_tdf.max_dist_gp_pred, mtgp_tdf.dist_straight_gp_pred, mtgp_tdf.max_dist_home_gp_pred, mtgp_rec_acc, mtgp_freq_rank_acc\n",
    "            skmob_metrics_df.iloc[1] = ses_tdf.no_loc_ses_pred, ses_tdf.rg_ses_pred, ses_tdf.k_rg_ses_pred, ses_tdf.spat_burst_ses_pred, ses_tdf.rand_entr_ses_pred, ses_tdf.real_entr_ses_pred, ses_tdf.uncorr_entr_ses_pred, ses_tdf.max_dist_ses_pred, ses_tdf.dist_straight_ses_pred, ses_tdf.max_dist_home_ses_pred, ses_rec_acc, ses_freq_rank_acc\n",
    "            skmob_metrics_df.iloc[2] = holt_tdf.no_loc_holt_pred, holt_tdf.rg_holt_pred, holt_tdf.k_rg_holt_pred, holt_tdf.spat_burst_holt_pred, holt_tdf.rand_entr_holt_pred, holt_tdf.real_entr_holt_pred, holt_tdf.uncorr_entr_holt_pred, holt_tdf.max_dist_holt_pred, holt_tdf.dist_straight_holt_pred, holt_tdf.max_dist_home_holt_pred, holt_rec_acc, holt_freq_rank_acc\n",
    "            skmob_metrics_df.iloc[3] = es_tdf.no_loc_es_pred, es_tdf.rg_es_pred, es_tdf.k_rg_es_pred, es_tdf.spat_burst_es_pred, es_tdf.rand_entr_es_pred, es_tdf.real_entr_es_pred, es_tdf.uncorr_entr_es_pred, es_tdf.max_dist_es_pred, es_tdf.dist_straight_es_pred, es_tdf.max_dist_home_es_pred, es_rec_acc, es_freq_rank_acc\n",
    "            skmob_metrics_df.iloc[4] = arima_tdf.no_loc_arima_pred, arima_tdf.rg_arima_pred, arima_tdf.k_rg_arima_pred, arima_tdf.spat_burst_arima_pred, arima_tdf.rand_entr_arima_pred, arima_tdf.real_entr_arima_pred, arima_tdf.uncorr_entr_arima_pred, arima_tdf.max_dist_arima_pred, arima_tdf.dist_straight_arima_pred, arima_tdf.max_dist_home_arima_pred, arima_rec_acc, arima_freq_rank_acc\n",
    "            skmob_metrics_df.iloc[5] = sarimax_tdf.no_loc_sarimax_pred, sarimax_tdf.rg_sarimax_pred, sarimax_tdf.k_rg_sarimax_pred, sarimax_tdf.spat_burst_sarimax_pred, sarimax_tdf.rand_entr_sarimax_pred, sarimax_tdf.real_entr_sarimax_pred, sarimax_tdf.uncorr_entr_sarimax_pred, sarimax_tdf.max_dist_sarimax_pred, sarimax_tdf.dist_straight_sarimax_pred, sarimax_tdf.max_dist_home_sarimax_pred, sarimax_rec_acc, sarimax_freq_rank_acc\n",
    "            skmob_metrics_df.iloc[6] = LI_tdf.no_loc_li_pred, LI_tdf.rg_li_pred, LI_tdf.k_rg_li_pred, LI_tdf.spat_burst_li_pred, LI_tdf.rand_entr_li_pred, LI_tdf.real_entr_li_pred, LI_tdf.uncorr_entr_li_pred, LI_tdf.max_dist_li_pred, LI_tdf.dist_straight_li_pred, LI_tdf.max_dist_home_li_pred, LI_rec_acc, LI_freq_rank_acc\n",
    "            skmob_metrics_df.iloc[7] = df_curr_metrics.no_loc_gt_pred, df_curr_metrics.rg_gt_pred, df_curr_metrics.k_rg_gt_pred, df_curr_metrics.spat_burst_gt_pred, df_curr_metrics.rand_entr_gt_pred, df_curr_metrics.real_entr_gt_pred, df_curr_metrics.uncorr_entr_gt_pred, df_curr_metrics.max_dist_gt_pred, df_curr_metrics.dist_straight_gt_pred, df_curr_metrics.max_dist_home_gt_pred, -1, -1,\n",
    "            \n",
    "            # Find absolute difference between predicted and ground truth\n",
    "            skmob_metrics_df['no_loc_error'] = skmob_metrics_df['no_loc'] - skmob_metrics_df.iloc[7]['no_loc']\n",
    "            skmob_metrics_df['rg_error'] = skmob_metrics_df['rg'] - skmob_metrics_df.iloc[7]['rg']\n",
    "            skmob_metrics_df['k_rg_error'] = skmob_metrics_df['k_rg'] - skmob_metrics_df.iloc[7]['k_rg']\n",
    "            skmob_metrics_df['spat_burst_error'] = skmob_metrics_df['spat_burst'] - skmob_metrics_df.iloc[7]['spat_burst']\n",
    "            skmob_metrics_df['rand_entr_error'] = skmob_metrics_df['rand_entr'] - skmob_metrics_df.iloc[7]['rand_entr']\n",
    "            skmob_metrics_df['real_entr_error'] = skmob_metrics_df['real_entr'] - skmob_metrics_df.iloc[7]['real_entr']\n",
    "            skmob_metrics_df['uncorr_entr_error'] = skmob_metrics_df['uncorr_entr'] - skmob_metrics_df.iloc[7]['uncorr_entr']\n",
    "            skmob_metrics_df['max_dist_error'] = skmob_metrics_df['max_dist'] - skmob_metrics_df.iloc[7]['max_dist']\n",
    "            skmob_metrics_df['dist_straight_error'] = skmob_metrics_df['dist_straight'] - skmob_metrics_df.iloc[7]['dist_straight']\n",
    "            skmob_metrics_df['max_dist_home_error'] = skmob_metrics_df['max_dist_home'] - skmob_metrics_df.iloc[7]['max_dist_home']\n",
    "\n",
    "            # Find mean absolute error (MAE) and median absolute error for each method from the absolute differences in each metric\n",
    "            skmob_metrics_df['mae'] = (1/10) * (abs(skmob_metrics_df['no_loc_error']) + abs(skmob_metrics_df['rg_error']) + abs(skmob_metrics_df['k_rg_error']) + abs(skmob_metrics_df['spat_burst_error']) + abs(skmob_metrics_df['rand_entr_error']) + abs(skmob_metrics_df['real_entr_error']) + abs(skmob_metrics_df['uncorr_entr_error']) + abs(skmob_metrics_df['max_dist_error']) + abs(skmob_metrics_df['dist_straight_error']) + abs(skmob_metrics_df['max_dist_home_error']) )\n",
    "            skmob_metrics_df['mad'] = np.median(0 - np.median(np.array([abs(skmob_metrics_df['no_loc_error']), abs(skmob_metrics_df['rg_error']), abs(skmob_metrics_df['k_rg_error']), abs(skmob_metrics_df['spat_burst_error']), abs(skmob_metrics_df['rand_entr_error']), abs(skmob_metrics_df['real_entr_error']), abs(skmob_metrics_df['uncorr_entr_error']), abs(skmob_metrics_df['max_dist_error']), abs(skmob_metrics_df['dist_straight_error']), abs(skmob_metrics_df['max_dist_home_error']) ])))\n",
    "            # Write skmob metrics to a file\n",
    "            skmob_metrics_df.to_csv('skmob_metrics_' + str(i) + '.csv')\n",
    "\n",
    "            # Write results to a file\n",
    "            results.to_csv('results_' + str(i) + '.csv')\n",
    "\n",
    "            # Create a directory for all results if it doesn't exist\n",
    "            if not os.path.exists('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Results\\\\' + str(bin_len) + '\\\\all_results'):\n",
    "                os.makedirs('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Results\\\\' + str(bin_len) + '\\\\all_results')\n",
    "            # Navigate to the directory\n",
    "            os.chdir('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Results\\\\' + str(bin_len) + '\\\\all_results')\n",
    "\n",
    "            # Write results there as well\n",
    "            results.to_csv('results_' + str(i) + '.csv')\n",
    "\n",
    "            # Create a directory for all skmob metrics if it doesn't exist\n",
    "            if not os.path.exists('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Results\\\\' + str(bin_len) + '\\\\all_skmob_metrics'):\n",
    "                os.makedirs('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Results\\\\' + str(bin_len) + '\\\\all_skmob_metrics')\n",
    "            # Navigate to the directory\n",
    "            os.chdir('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Results\\\\' + str(bin_len) + '\\\\all_skmob_metrics')\n",
    "\n",
    "            # Write skmob metrics there as well\n",
    "            skmob_metrics_df.to_csv('skmob_metrics_' + str(i) + '.csv')\n",
    "\n",
    "            # Create a directory for all parameters if it doesn't exist\n",
    "            if not os.path.exists('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Results\\\\' + str(bin_len) + '\\\\all_parameters'):\n",
    "                os.makedirs('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Results\\\\' + str(bin_len) + '\\\\all_parameters')\n",
    "            # Navigate to the directory\n",
    "            os.chdir('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Results\\\\' + str(bin_len) + '\\\\all_parameters')\n",
    "\n",
    "            # Write parameters there as well\n",
    "            params_df.to_csv('params_' + str(i) + '.csv')\n",
    "\n",
    "        except:\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skmob_alt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3be6a4b976e3fc5992cbda345f874ed08629b5aa1a879cf1954f33c2638a941f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
