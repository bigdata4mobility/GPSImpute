{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skmob\n",
    "import pandas as pd\n",
    "import skmob.measures.individual as ind_measure\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.kernels import RQKernel as RQ, RBFKernel as SE, PeriodicKernel as PER, ScaleKernel\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "#import statistics as stats\n",
    "\n",
    "# Import intra-package scripts\n",
    "import utils.helper_func as helper_func\n",
    "import utils.GP as GP\n",
    "from utils.helper_func import dec_floor\n",
    "import mobileDataToolkit.analysis as analysis\n",
    "import mobileDataToolkit.preprocessing_v2 as preprocessing\n",
    "import mobileDataToolkit.methods as methods\n",
    "import mobileDataToolkit.metrics as metrics\n",
    "\n",
    "# Import benchmarks\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_path = \"/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Data/seattle_2000_all_obs_sampled.csv\"\n",
    "df = pd.read_csv(file_path, header=0)\n",
    "\n",
    "# Add month column\n",
    "df['month'] = pd.DatetimeIndex(df['datetime']).month\n",
    "\n",
    "# Group by user ID, find month with third most observations (average)\n",
    "#df_m = df.groupby('UID').apply(lambda x: x[x['month'] == x['month'].value_counts().index[2]])\n",
    "\n",
    "# Groupby user ID, keep all observations from January and February \n",
    "df_m = df.groupby('UID').apply(lambda x: x[x['month'].isin([1,2])])\n",
    "\n",
    "df_m = df_m.reset_index(drop=True)\n",
    "\n",
    "max_speed_kmh = 400 # for filtering out unrealistic speeds\n",
    "spatial_radius_km = 0.3 # for compressing similar points using Douglas-Peucker algorithm\n",
    "bin_len_ls = [10080, 1440, 360, 60, 30, 15] # Bin lengths to test: 1 week, 1 day, 6 hours, 1 hour, 30 min, 15 min\n",
    "init_period_len_1 = 60*8 # 8 hours\n",
    "init_period_len_2 = 60*24 # 24 hours\n",
    "lr = 0.3 # learning rate\n",
    "n_epochs = 150 # number of epochs\n",
    "\n",
    "# Set search range for ARIMA and SARIMAX\n",
    "p_values = range(0, 3)\n",
    "d_values = range(0, 2)  \n",
    "q_values = range(0, 3) \n",
    "P_values = range(0, 3)  \n",
    "D_values = range(0, 2)  \n",
    "Q_values = range(0, 3)  \n",
    "m_values = range(0, 3)  \n",
    "\n",
    "runtimes_comp = []\n",
    "bics_comp = []\n",
    "runtimes_rbf = []\n",
    "bics_rbf = []\n",
    "\n",
    "def trainingLossPlot(ls):\n",
    "    iters = range(0, len(ls))\n",
    "    fig4, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.plot(iters, ls, 'g')\n",
    "    ax.set_title('Training Loss')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    return fig4\n",
    "\n",
    "def predictionsVsActualPlot(y_test_scaled, mean):\n",
    "    plt.rcParams.update({'font.size': 9})\n",
    "            # Make the font nicer\n",
    "    plt.rcParams.update({'font.family': 'serif'})\n",
    "    fig3, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.set_title('Predictions')\n",
    "    try:\n",
    "        pd.DataFrame(mean.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='red', alpha=0.5, s=0.4, label='Predictions')\n",
    "    except AttributeError: \n",
    "        pd.DataFrame(mean).plot(x=1, y=0, kind='scatter',ax=ax, color='red', alpha=0.5, s=0.4, label='Predictions')\n",
    "    pd.DataFrame(y_test_scaled.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='blue', alpha=0.5, s=0.4, label='Actual')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    return fig3\n",
    "\n",
    "def trainTestPlot(gapped_user_data, curr_mt):\n",
    "    plt.rcParams.update({'font.size': 9})\n",
    "    plt.rcParams.update({'font.family': 'serif'})\n",
    "    fig1, ax = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n",
    "    ax[0].scatter(curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_lat'],\n",
    "                            color='blue', label='Training data', s=1)\n",
    "    ax[0].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_lat'],\n",
    "                            color='red', label='Test data', s=1)\n",
    "    ax[0].set_ylabel('Latitude')\n",
    "    ax[1].scatter(curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_long'],\n",
    "                            color='blue', label='Training data', s=1)\n",
    "    ax[1].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_long'],\n",
    "                            color='red', label='Test data', s=1)\n",
    "    ax[1].set_xlabel('Time')\n",
    "    ax[1].set_ylabel('Longitude')\n",
    "    ax[1].legend()\n",
    "    return fig1\n",
    "\n",
    "def makeSeries(y_train_scaled, y_test_scaled, unix_min_tr, unix_min_te):\n",
    "    lat = pd.Series(y_train_scaled[:,0].tolist(), unix_min_tr)\n",
    "    lat_t = pd.Series(y_test_scaled[:,0].tolist(), unix_min_te)\n",
    "            # Replace duplicates (in time) with the mean of the two values\n",
    "    lat = lat.groupby(lat.index).mean().reset_index()\n",
    "    lat = pd.Series(lat[0].tolist(), lat['index'].tolist())\n",
    "    lat_tc = lat_t.groupby(lat_t.index).mean().reset_index()\n",
    "    lat_tc = pd.Series(lat_tc[0].tolist(), lat_tc['index'].tolist())\n",
    "            # Replace zeroes with positives close to zero\n",
    "    lat.replace(0, 0.000000001, inplace=True)\n",
    "\n",
    "    lon = pd.Series(y_train_scaled[:,1].tolist(), unix_min_tr)\n",
    "    lon_t = pd.Series(y_test_scaled[:,1].tolist(),unix_min_te)\n",
    "            # Replace duplicates (in time) with the mean of the two values\n",
    "    lon = lon.groupby(lon.index).mean().reset_index()\n",
    "    lon = pd.Series(lon[0].tolist(), lon['index'].tolist())\n",
    "    lon_tc = lon_t.groupby(lon_t.index).mean().reset_index()\n",
    "    lon_tc = pd.Series(lon_tc[0].tolist(), lon_tc['index'].tolist())\n",
    "            # Replace zeroes with positives close to zero\n",
    "    lon.replace(0, 0.000000001, inplace=True)\n",
    "    return lat,lat_tc,lon,lon_tc\n",
    "\n",
    "def LI(df_curr_metrics, curr_mt, scaler, y_train_scaled, y_test_scaled, lat_tc):\n",
    "    try:\n",
    "                # Linear Interpolation\n",
    "        print(\"Running Linear Interpolation...\")\n",
    "        LI_preds_lat, LI_preds_long = methods.LI(curr_mt.X_train[:,0], curr_mt.X_test[:,0], y_train_scaled, y_test_scaled)\n",
    "\n",
    "        LI_preds = np.hstack(((LI_preds_lat.reshape(-1, 1), LI_preds_long.reshape(-1, 1))))\n",
    "\n",
    "        LI_preds_origs = scaler.inverse_transform(LI_preds)\n",
    "                \n",
    "        #LI_full_preds_df = helper_func.preds_to_full_df(preds_lat=LI_preds_origs[:,0], preds_long=LI_preds_origs[:,1], \n",
    "        #                                                    test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "        # Changelog: 09/30/2023\n",
    "        # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "        LI_full_preds_df = pd.DataFrame(LI_preds_origs, columns=['lat', 'long'])\n",
    "        LI_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "                \n",
    "        LI_tdf = helper_func.skmob_metric_calcs(LI_full_preds_df, method='LI', lat='lat', long='long', datetime='datetime')\n",
    "        LI_res = metrics.average_eval(np.array(y_test_scaled[:,0]), np.array(y_test_scaled[:,1]), LI_preds_lat, LI_preds_long)\n",
    "        LI_sim = metrics.evaluate_similarity(lat_tc, LI_preds, y_test_scaled)\n",
    "        LI_res.update(LI_sim)\n",
    "\n",
    "        LI_rec_acc = helper_func.matrix_acc(LI_tdf.recency_li_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "        LI_freq_rank_acc = helper_func.matrix_acc(LI_tdf.freq_rank_li_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "        fig6 = predictionsVsActualPlot(y_test_scaled, LI_preds)\n",
    "    except:\n",
    "        print(\"Error in LI\")\n",
    "        LI_res = None\n",
    "        LI_sim = None\n",
    "        LI_rec_acc = None\n",
    "        LI_freq_rank_acc = None\n",
    "    return LI_preds, LI_tdf, LI_res, LI_sim, LI_rec_acc, LI_freq_rank_acc, fig6\n",
    "\n",
    "def SES(df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc):\n",
    "    print(\"Running Simple Exponential Smoothing...\")\n",
    "    ses_lat = SimpleExpSmoothing(lat, initialization_method=\"estimated\").fit()\n",
    "    pred_lat_ses = ses_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "    pred_lat_comp_ses = pred_lat_ses[pred_lat_ses.index.isin(unix_min_te)]\n",
    "\n",
    "    ses_lon = SimpleExpSmoothing(lon, initialization_method=\"estimated\").fit()\n",
    "    pred_lon_ses = ses_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "    pred_lon_comp_ses = pred_lon_ses[pred_lon_ses.index.isin(unix_min_te)]\n",
    "\n",
    "    ses_preds = np.hstack(((pred_lat_comp_ses.values.reshape(-1, 1), pred_lon_comp_ses.values.reshape(-1, 1))))\n",
    "    ses_preds_origs = scaler.inverse_transform(ses_preds)\n",
    "\n",
    "    #ses_full_preds_df = helper_func.preds_to_full_df(preds_lat=ses_preds_origs[:,0], preds_long=ses_preds_origs[:,1],\n",
    "    #                                                    test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "    # Changelog: 09/30/2023\n",
    "    # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "    ses_full_preds_df = pd.DataFrame(ses_preds_origs, columns=['lat', 'long'])\n",
    "    ses_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "\n",
    "    ses_tdf = helper_func.skmob_metric_calcs(ses_full_preds_df, method='ses', lat='lat', long='long', datetime='datetime')\n",
    "    ses_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_ses, pred_lon_comp_ses)\n",
    "    ses_sim = metrics.evaluate_similarity(lat_tc, ses_preds, y_test_scaled)\n",
    "    ses_res.update(ses_sim)\n",
    "\n",
    "    ses_rec_acc = helper_func.matrix_acc(ses_tdf.recency_ses_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "    ses_freq_rank_acc = helper_func.matrix_acc(ses_tdf.freq_rank_ses_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "    fig7 = predictionsVsActualPlot(y_test_scaled, ses_preds)\n",
    "    return ses_lat, ses_lon, ses_preds, ses_tdf, ses_res, ses_sim, ses_rec_acc, ses_freq_rank_acc, fig7\n",
    "\n",
    "def Holts(df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc):\n",
    "    print(\"Running Holt-Winters model...\")\n",
    "    holt_lat = Holt(lat, damped_trend=True, initialization_method=\"estimated\").fit()\n",
    "    pred_lat_holt = holt_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "    pred_lat_comp_holt = pred_lat_holt[pred_lat_holt.index.isin(unix_min_te)]\n",
    "\n",
    "            #holt_smoothing_level_lon=0.1\n",
    "            #holt_smoothing_slope_lon=0.0307\n",
    "\n",
    "    holt_lon = Holt(lon, damped_trend=True, initialization_method=\"estimated\").fit()\n",
    "    pred_lon_holt = holt_lon.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "    pred_lon_comp_holt = pred_lon_holt[pred_lon_holt.index.isin(unix_min_te)]\n",
    "    holt_preds = np.hstack(((pred_lat_comp_holt.values.reshape(-1, 1), pred_lon_comp_holt.values.reshape(-1, 1))))\n",
    "\n",
    "    holt_preds_origs = scaler.inverse_transform(holt_preds)\n",
    "\n",
    "    #holt_full_preds_df = helper_func.preds_to_full_df(preds_lat=holt_preds_origs[:,0], preds_long=holt_preds_origs[:,1],\n",
    "    #                                                    test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "    # Changelog: 09/30/2023\n",
    "    # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "    holt_full_preds_df = pd.DataFrame(holt_preds_origs, columns=['lat', 'long'])\n",
    "    holt_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "\n",
    "    holt_tdf = helper_func.skmob_metric_calcs(holt_full_preds_df, method='holt', lat='lat', long='long', datetime='datetime')\n",
    "    holt_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_holt, pred_lon_comp_holt)\n",
    "    holt_sim = metrics.evaluate_similarity(lat_tc, holt_preds, y_test_scaled)\n",
    "    holt_res.update(holt_sim)\n",
    "\n",
    "    holt_rec_acc = helper_func.matrix_acc(holt_tdf.recency_holt_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "    holt_freq_rank_acc = helper_func.matrix_acc(holt_tdf.freq_rank_holt_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "    fig8 = predictionsVsActualPlot(y_test_scaled, holt_preds)\n",
    "    return holt_lat, holt_lon, holt_preds, holt_tdf, holt_res, holt_sim, holt_rec_acc, holt_freq_rank_acc, fig8\n",
    "\n",
    "def ES(df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc, es_seasonal_periods=24):\n",
    "    try:\n",
    "        # Exponential Smoothing\n",
    "        print(\"Running Exponential Smoothing...\")\n",
    "        es_lat = ExponentialSmoothing(lat, seasonal_periods=es_seasonal_periods, trend='add', seasonal='add', damped_trend=True, use_boxcox=False, initialization_method='estimated').fit()\n",
    "        pred_lat_es = es_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "        pred_lat_comp_es = pred_lat_es[pred_lat_es.index.isin(unix_min_te)]\n",
    "\n",
    "        es_lon = ExponentialSmoothing(lon, seasonal_periods=es_seasonal_periods, trend='add', seasonal='add', damped_trend=True, use_boxcox=False, initialization_method='estimated').fit()\n",
    "        pred_lon_es = es_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "        pred_lon_comp_es = pred_lon_es[pred_lon_es.index.isin(unix_min_te)]\n",
    "\n",
    "        es_preds = np.hstack(((pred_lat_comp_es.values.reshape(-1, 1), pred_lon_comp_es.values.reshape(-1, 1))))\n",
    "\n",
    "        es_preds_origs = scaler.inverse_transform(es_preds)\n",
    "\n",
    "        #es_full_preds_df = helper_func.preds_to_full_df(preds_lat=es_preds_origs[:,0], preds_long=es_preds_origs[:,1],\n",
    "        #                                                    test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "        # Changelog: 09/30/2023\n",
    "        # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "        es_full_preds_df = pd.DataFrame(es_preds_origs, columns=['lat', 'long'])\n",
    "        es_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "\n",
    "        es_tdf = helper_func.skmob_metric_calcs(es_full_preds_df, method='es', lat='lat', long='long', datetime='datetime')\n",
    "        es_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_es, pred_lon_comp_es)\n",
    "        es_sim = metrics.evaluate_similarity(lat_tc, es_preds, y_test_scaled)\n",
    "        es_res.update(es_sim)\n",
    "\n",
    "        es_rec_acc = helper_func.matrix_acc(es_tdf.recency_es_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "        es_freq_rank_acc = helper_func.matrix_acc(es_tdf.freq_rank_es_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "        fig9 = predictionsVsActualPlot(y_test_scaled, es_preds)\n",
    "    except:\n",
    "        print(\"Exponential Smoothing failed\")\n",
    "        es_res = None\n",
    "        es_sim = None\n",
    "        es_rec_acc = None\n",
    "        es_freq_rank_acc = None\n",
    "    return es_seasonal_periods, es_lat, es_lon, es_tdf, es_res, es_sim, es_rec_acc, es_freq_rank_acc, fig9\n",
    "\n",
    "def arima(p_values, d_values, q_values, df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc):\n",
    "    try:\n",
    "                # ARIMA\n",
    "                #arima_order = (1,1,0)\n",
    "        print(\"Running ARIMA...\")\n",
    "        best_arima_order, best_arima_aic = methods.find_best_arima_order(lat, p_values, d_values, q_values)\n",
    "\n",
    "        arima_lat = ARIMA(lat, order=best_arima_order).fit()\n",
    "        pred_lat_arima = arima_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "        pred_lat_comp_arima = pred_lat_arima[pred_lat_arima.index.isin(unix_min_te)]\n",
    "\n",
    "        arima_lon = ARIMA(lon, order=best_arima_order).fit()\n",
    "        pred_lon_arima = arima_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "        pred_lon_comp_arima = pred_lon_arima[pred_lon_arima.index.isin(unix_min_te)]\n",
    "\n",
    "        arima_preds = np.hstack(((pred_lat_comp_arima.values.reshape(-1, 1), pred_lon_comp_arima.values.reshape(-1, 1))))\n",
    "\n",
    "        arima_preds_origs = scaler.inverse_transform(arima_preds)\n",
    "                \n",
    "        #arima_full_preds_df = helper_func.preds_to_full_df(preds_lat=arima_preds_origs[:,0], preds_long=arima_preds_origs[:,1],\n",
    "        #                                                    test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "        # Changelog: 09/30/2023\n",
    "        # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "        arima_full_preds_df = pd.DataFrame(arima_preds_origs, columns=['lat', 'long'])\n",
    "        arima_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "\n",
    "        arima_tdf = helper_func.skmob_metric_calcs(arima_full_preds_df, method='arima', lat='lat', long='long', datetime='datetime')\n",
    "        arima_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_arima, pred_lon_comp_arima)\n",
    "        arima_sim = metrics.evaluate_similarity(lat_tc, arima_preds, y_test_scaled)\n",
    "        arima_res.update(arima_sim)\n",
    "\n",
    "        arima_rec_acc = helper_func.matrix_acc(arima_tdf.recency_arima_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "        arima_freq_rank_acc = helper_func.matrix_acc(arima_tdf.freq_rank_arima_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "        fig10 = predictionsVsActualPlot(y_test_scaled, arima_preds)\n",
    "    except:\n",
    "        print(\"ARIMA failed\")\n",
    "        arima_res = None\n",
    "        arima_sim = None\n",
    "        arima_rec_acc = None\n",
    "        arima_freq_rank_acc = None\n",
    "    return best_arima_order, arima_tdf, arima_res, arima_sim, arima_rec_acc, arima_freq_rank_acc, fig10\n",
    "\n",
    "def sarimax(p_values, d_values, q_values, P_values, D_values, Q_values, m_values, df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc):\n",
    "    try:\n",
    "        # SARIMAX\n",
    "        #sarimax_order = (1,0,0)\n",
    "        print(\"Running SARIMAX...\")\n",
    "        #sarimax_seasonal_order = (1, 1, 1, 24)\n",
    "        best_sarimax_order, best_seasonal_order, best_sarimax_aic = methods.find_best_sarimax_order(lat, p_values, d_values, q_values, P_values, D_values, Q_values, m_values)\n",
    "        sarimax_lat = SARIMAX(lat, order=best_sarimax_order, seasonal_order=best_seasonal_order).fit(disp=False)\n",
    "        pred_lat_sar = sarimax_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "        pred_lat_comp_sar = pred_lat_sar[pred_lat_sar.index.isin(unix_min_te)]\n",
    "\n",
    "        sarimax_lon = SARIMAX(lon, order=best_sarimax_order, seasonal_order=best_seasonal_order).fit(disp=False)\n",
    "        pred_lon_sar = sarimax_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "        pred_lon_comp_sar = pred_lon_sar[pred_lon_sar.index.isin(unix_min_te)]\n",
    "\n",
    "        sarimax_preds = np.hstack(((pred_lat_comp_sar.values.reshape(-1, 1), pred_lon_comp_sar.values.reshape(-1, 1))))\n",
    "\n",
    "        sarimax_preds_origs = scaler.inverse_transform(sarimax_preds)\n",
    "\n",
    "        #sarimax_full_preds_df = helper_func.preds_to_full_df(preds_lat=sarimax_preds_origs[:,0], preds_long=sarimax_preds_origs[:,1],\n",
    "        #                                                    test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "        # Changelog: 09/30/2023\n",
    "        # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "        sarimax_full_preds_df = pd.DataFrame(sarimax_preds_origs, columns=['lat', 'long'])\n",
    "        sarimax_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "\n",
    "        sarimax_tdf = helper_func.skmob_metric_calcs(sarimax_full_preds_df, method='sarimax', lat='lat', long='long', datetime='datetime')\n",
    "        sarimax_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_sar, pred_lon_comp_sar)\n",
    "        sarimax_sim = metrics.evaluate_similarity(lat_tc, sarimax_preds, y_test_scaled)\n",
    "        sarimax_res.update(sarimax_sim)\n",
    "\n",
    "        sarimax_rec_acc = helper_func.matrix_acc(sarimax_tdf.recency_sarimax_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "        sarimax_freq_rank_acc = helper_func.matrix_acc(sarimax_tdf.freq_rank_sarimax_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "        fig11 = predictionsVsActualPlot(y_test_scaled, sarimax_preds)\n",
    "    except:\n",
    "        print(\"SARIMAX failed\")\n",
    "        sarimax_res = None\n",
    "        sarimax_sim = None\n",
    "        sarimax_rec_acc = None\n",
    "        sarimax_freq_rank_acc = None\n",
    "    return best_sarimax_order, best_seasonal_order, sarimax_tdf, sarimax_res, sarimax_sim, sarimax_rec_acc, sarimax_freq_rank_acc, fig11\n",
    "\n",
    "for j in bin_len_ls:\n",
    "    bin_len = j\n",
    "    # Create a directory for each bin length\n",
    "    if not os.path.exists('/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Sept_Results/' + str(bin_len)):\n",
    "        os.makedirs('/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Sept_Results/' + str(bin_len))\n",
    "    print(\"Starting tests on bin length = {}\".format(bin_len))\n",
    "    # Main loop that will go through each user ID, create a directory for each user, etc.\n",
    "    for i in df_m.UID.unique()[3:4]:\n",
    "        try:\n",
    "            \n",
    "            df_curr = df_m[df_m.UID == i]\n",
    "\n",
    "            tdf = skmob.TrajDataFrame(df_curr, latitude='orig_lat', longitude='orig_long', datetime='datetime')\n",
    "            f_tdf = skmob.preprocessing.filtering.filter(tdf, max_speed_kmh=max_speed_kmh, include_loops=False)\n",
    "            # Print the difference in number of rows\n",
    "            print(\"Number of rows before filtering: {}\".format(tdf.shape[0]))\n",
    "            print(\"Number of rows after filtering: {}\".format(f_tdf.shape[0]))\n",
    "            fc_tdf = skmob.preprocessing.compression.compress(f_tdf, spatial_radius_km=spatial_radius_km)\n",
    "            # Print the difference in number of rows\n",
    "            print(\"Number of rows after compression: {}\".format(fc_tdf.shape[0]))\n",
    "            # Remove data points with uncertainty > 100m\n",
    "            fcu_tdf = fc_tdf[fc_tdf['orig_unc'] <= 100]\n",
    "            # Print the difference in number of rows\n",
    "            print(\"Number of rows after uncertainty filtering: {}\".format(fcu_tdf.shape[0]))\n",
    "            df_curr = fcu_tdf\n",
    "\n",
    "            # Remove duplicates in the unix column\n",
    "            df_curr = df_curr.drop_duplicates(subset=['unix_min'])\n",
    "\n",
    "            curr_ocp = analysis.tempOcp(df_curr, 'unix_min', bin_len=bin_len)\n",
    "\n",
    "            upper_bound = dec_floor(curr_ocp)\n",
    "            \n",
    "            # See current temporal occupancy\n",
    "            print(\"Current temporal occupancy: {}\".format(curr_ocp))\n",
    "            while True:\n",
    "                try:\n",
    "                    if curr_ocp <= 0.1:\n",
    "                        target_ocp = np.random.uniform(0, curr_ocp)\n",
    "                    else:\n",
    "                        # Choose random decimal between 0.1 and upper bound\n",
    "                        target_ocp = dec_floor(np.random.uniform(0.1, upper_bound))\n",
    "                    print(\"Target temporal occupancy: {}\".format(target_ocp))\n",
    "                    # Simulate gaps in the user's data to match the target level\n",
    "                    gapped_user_data, train_index, new_ocp = analysis.simulate_gaps(df_curr, target_ocp, unix_col='unix_min', bin_len= bin_len)\n",
    "                except:\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "            # Change name of 'lat' and 'lon' columns to 'orig_lat' and 'orig_long'\n",
    "            df_curr = df_curr.rename(columns={'lat': 'orig_lat', 'lng': 'orig_long'})\n",
    "\n",
    "            # Create MultiTrip object\n",
    "            curr_mt = preprocessing.dp_MultiTrip(data=df_curr)\n",
    "            curr_mt.Multi_Trip_Preprocess(lat='orig_lat', long='orig_long', datetime='datetime')\n",
    "\n",
    "            # Move 'unix_start_t' to before 'SaM'\n",
    "            cols = list(curr_mt.data.columns)\n",
    "            cols.insert(16, cols.pop(cols.index('unix_min')))\n",
    "            curr_mt.data = curr_mt.data.loc[:, cols] \n",
    "            # Print data columns\n",
    "            print(curr_mt.data.columns)\n",
    "\n",
    "            curr_mt.Multi_Trip_TrainTestSplit(test_start_date=None, test_end_date=None, \n",
    "                                        training_index = set(gapped_user_data['unix_min']), lat='orig_lat', \n",
    "                                        long='orig_long', datetime='datetime', unix='unix_min', inputstart='unix_min', \n",
    "                                        inputend=curr_mt.data.columns[-1])\n",
    "\n",
    "            n_train = len(curr_mt.X_train[:,0])\n",
    "            n_test = len(curr_mt.X_test[:,0])\n",
    "            n_dims = curr_mt.X_train.shape[1]\n",
    "\n",
    "            # Calculate sci-kit mobility metrics\n",
    "            # df_curr_metrics = helper_func.skmob_metric_calcs(df_curr, method='GT', lat='lat', long='lng', datetime='datetime')\n",
    "            # CHANGELOG: 09/30/2021\n",
    "            # Calculating skmob metrics only on the test and prediction points, not the entire dataset\n",
    "            df_curr_metrics = helper_func.skmob_metric_calcs(curr_mt.test, method='GT', lat='lat', long='long', datetime='date')\n",
    "\n",
    "            # See number of points in training and test sets\n",
    "            print(\"Number of points in training set: {}\".format(n_train))\n",
    "            print(\"Number of points in test set: {}\".format(n_test))\n",
    "            print(\"Number of input dimensions: {}\".format(n_dims))\n",
    "\n",
    "            # If there are no points in the test set, skip to the next user\n",
    "            if n_test == 0:\n",
    "                print(\"No points in test set. Skipping to next user.\")\n",
    "                continue\n",
    "\n",
    "            # Visualize the training and test data in two subplots, one lat vs time and one long vs time\n",
    "            fig1 = trainTestPlot(gapped_user_data, curr_mt)\n",
    "\n",
    "            mean_lat = curr_mt.y_train[:,0].mean()\n",
    "            mean_long = curr_mt.y_train[:,1].mean()\n",
    "            std_lat = curr_mt.y_train[:,0].std()\n",
    "            std_long = curr_mt.y_train[:,1].std()\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            y_train_scaled = torch.tensor(np.float64(scaler.fit_transform(curr_mt.y_train)))\n",
    "            y_test_scaled = torch.tensor(np.float64(scaler.transform(curr_mt.y_test)))\n",
    "            # Unix time for benchmarks\n",
    "            unix_min_tr = np.array(curr_mt.X_train[:,0]).astype(int)\n",
    "            unix_min_te = np.array(curr_mt.X_test[:,0]).astype(int)\n",
    "\n",
    "            lat, lat_tc, lon, lon_tc = makeSeries(y_train_scaled, y_test_scaled, unix_min_tr, unix_min_te)\n",
    "\n",
    "            likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)\n",
    "\n",
    "            # Composite model with RQ * PER kernels\n",
    "            model = GP.MTGPRegressor(curr_mt.X_train, y_train_scaled, \n",
    "                                    ScaleKernel(ScaleKernel(RQ(ard_num_dims=n_dims)) * ScaleKernel(PER(active_dims=[0]))) + \n",
    "                                    ScaleKernel(ScaleKernel(RQ(ard_num_dims=n_dims)) * ScaleKernel(PER(active_dims=[0])))\n",
    "            )\n",
    "\n",
    "            # Set initial lengthscale guess for unix_min as half the average length of gap in training set\n",
    "            init_lengthscale = curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'].diff().mean() / 2 \n",
    "            # Set the rest of the lengthscale guesses to 1\n",
    "            initializations = np.ones(n_dims - 1)\n",
    "            initializations = np.insert(initializations, 0, init_lengthscale)\n",
    "            model.covar_module.data_covar_module.kernels[0].base_kernel.kernels[0].base_kernel.lengthscale = initializations\n",
    "            model.covar_module.data_covar_module.kernels[1].base_kernel.kernels[0].base_kernel.lengthscale = initializations\n",
    "\n",
    "            # Set initial period lengths\n",
    "            model.covar_module.data_covar_module.kernels[0].base_kernel.kernels[1].base_kernel.period_length = init_period_len_1\n",
    "            model.covar_module.data_covar_module.kernels[1].base_kernel.kernels[1].base_kernel.period_length = init_period_len_2\n",
    "\n",
    "            # Train model\n",
    "            start = time.time()\n",
    "            ls, mll = GP.training(model, curr_mt.X_train, y_train_scaled, lr=lr, n_epochs=n_epochs)\n",
    "            end = time.time()\n",
    "            runtime = end - start\n",
    "            runtimes_comp.append(runtime)\n",
    "\n",
    "            fig2 = trainingLossPlot(ls)\n",
    "            \n",
    "            mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                log_ll = mll(model(curr_mt.X_train), y_train_scaled) * curr_mt.X_train.shape[0]\n",
    "                        \n",
    "            N = curr_mt.X_train.shape[0]\n",
    "            m = sum(p.numel() for p in model.hyperparameters())\n",
    "            bic = -2 * log_ll + m * np.log(N)\n",
    "            bics_comp.append(bic)\n",
    "\n",
    "            predictions, mean = model.predict(curr_mt.X_test)\n",
    "\n",
    "            fig3 = predictionsVsActualPlot(y_test_scaled, mean)\n",
    "\n",
    "            # Model results\n",
    "            mtgp_res = metrics.average_eval(pd.Series(y_test_scaled[:,0]), pd.Series(y_test_scaled[:,1]), pd.Series(mean[:,0]), pd.Series(mean[:,1]))\n",
    "            \n",
    "            mtgp_sim = metrics.evaluate_similarity(lat_tc, mean, y_test_scaled)\n",
    "\n",
    "            mtgp_res.update(mtgp_sim)\n",
    "\n",
    "            # Convert mean predictions back to original scale in lat/long\n",
    "            orig_preds = scaler.inverse_transform(mean.reshape(-1,2))\n",
    "\n",
    "            #GP_full_preds_df = helper_func.preds_to_full_df(preds_lat=orig_preds[:,0], preds_long=orig_preds[:,1], \n",
    "            #                                            test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "            # Changelog: 09/30/2023\n",
    "            # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "            GP_full_preds_df = pd.DataFrame(orig_preds, columns=['lat', 'long'])\n",
    "            GP_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "\n",
    "            mtgp_tdf = helper_func.skmob_metric_calcs(GP_full_preds_df, method='GP', lat='lat', long='long', datetime='datetime')\n",
    "\n",
    "            mtgp_rec_acc = helper_func.matrix_acc(mtgp_tdf.recency_gp_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "            mtgp_freq_rank_acc = helper_func.matrix_acc(mtgp_tdf.freq_rank_gp_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "            # Kernel benchmark with a simpler kernel\n",
    "            # RBF Kernel with ARD\n",
    "            model_rbf = GP.MTGPRegressor(curr_mt.X_train, y_train_scaled,\n",
    "                                    ScaleKernel(SE(ard_num_dims=n_dims))\n",
    "            )\n",
    "\n",
    "            # Set initial lengthscale guess for unix_min as half the average length of gap in training set\n",
    "            init_lengthscale = curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'].diff().mean() / 2\n",
    "            # Set the rest of the lengthscale guesses to 1\n",
    "            initializations = np.ones(n_dims - 1)\n",
    "            initializations = np.insert(initializations, 0, init_lengthscale)\n",
    "            model_rbf.covar_module.data_covar_module.base_kernel.lengthscale = initializations\n",
    "\n",
    "            # Train model\n",
    "            start = time.time()\n",
    "            ls, mll = GP.training(model_rbf, curr_mt.X_train, y_train_scaled, lr=lr, n_epochs=n_epochs)\n",
    "            end = time.time()\n",
    "            runtime = end - start\n",
    "            runtimes_rbf.append(runtime)\n",
    "\n",
    "            fig4 = trainingLossPlot(ls)\n",
    "\n",
    "            mll = gpytorch.mlls.ExactMarginalLogLikelihood(model_rbf.likelihood, model_rbf)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                log_ll = mll(model_rbf(curr_mt.X_train), y_train_scaled) * curr_mt.X_train.shape[0]\n",
    "\n",
    "            N = curr_mt.X_train.shape[0]\n",
    "            m = sum(p.numel() for p in model_rbf.hyperparameters())\n",
    "            bic = -2 * log_ll + m * np.log(N)\n",
    "            bics_rbf.append(bic)\n",
    "\n",
    "            predictions_rbf, mean_rbf = model_rbf.predict(curr_mt.X_test)\n",
    "\n",
    "            fig5 = predictionsVsActualPlot(y_test_scaled, mean_rbf)\n",
    "\n",
    "            # Model results\n",
    "            mtgp_res_rbf = metrics.average_eval(pd.Series(y_test_scaled[:,0]), pd.Series(y_test_scaled[:,1]), pd.Series(mean_rbf[:,0]), pd.Series(mean_rbf[:,1]))\n",
    "\n",
    "            mtgp_sim_rbf = metrics.evaluate_similarity(lat_tc, mean_rbf, y_test_scaled)\n",
    "\n",
    "            mtgp_res_rbf.update(mtgp_sim_rbf)\n",
    "\n",
    "            # Convert mean predictions back to original scale in lat/long\n",
    "            orig_preds_rbf = scaler.inverse_transform(mean_rbf.reshape(-1,2))\n",
    "\n",
    "            #GP_full_preds_df_rbf = helper_func.preds_to_full_df(preds_lat=orig_preds_rbf[:,0], preds_long=orig_preds_rbf[:,1],\n",
    "            #                                                test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "\n",
    "            # Changelog: 09/30/2023\n",
    "            # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "            GP_full_preds_df_rbf = pd.DataFrame(orig_preds, columns=['lat', 'long'])\n",
    "            GP_full_preds_df_rbf['datetime'] = curr_mt.test['date'].values\n",
    "            \n",
    "            mtgp_tdf_rbf = helper_func.skmob_metric_calcs(GP_full_preds_df_rbf, method='GP_RBF', lat='lat', long='long', datetime='datetime')\n",
    "\n",
    "            mtgp_rec_acc_rbf = helper_func.matrix_acc(mtgp_tdf_rbf.recency_gp_rbf_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "            mtgp_freq_rank_acc_rbf = helper_func.matrix_acc(mtgp_tdf_rbf.freq_rank_gp_rbf_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "            # Linear Interpolation\n",
    "            LI_preds, LI_tdf, LI_res, LI_sim, LI_rec_acc, LI_freq_rank_acc, fig6 = LI(df_curr_metrics, curr_mt, scaler, y_train_scaled, y_test_scaled, lat_tc)\n",
    "\n",
    "            # SES model\n",
    "            #ses_smoothing_level = 0.1\n",
    "            ses_lat, ses_lon, ses_preds, ses_tdf, ses_res, ses_sim, ses_rec_acc, ses_freq_rank_acc, fig7 = SES(df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc)\n",
    "\n",
    "            # Holt model\n",
    "            #holt_smoothing_level_lat=0.2\n",
    "            #holt_smoothing_slope_lat=0.045\n",
    "\n",
    "            holt_lat, holt_lon, holt_preds, holt_tdf, holt_res, holt_sim, holt_rec_acc, holt_freq_rank_acc, fig8 = Holts(df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc)\n",
    "\n",
    "            es_seasonal_periods, es_lat, es_lon, es_tdf, es_res, es_sim, es_rec_acc, es_freq_rank_acc, fig9 = ES(df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc)\n",
    "            \n",
    "            best_arima_order, arima_tdf, arima_res, arima_sim, arima_rec_acc, arima_freq_rank_acc, fig10 = arima(p_values, d_values, q_values, df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc)\n",
    "\n",
    "            best_sarimax_order, best_seasonal_order, sarimax_tdf, sarimax_res, sarimax_sim, sarimax_rec_acc, sarimax_freq_rank_acc, fig11 = sarimax(p_values, d_values, q_values, P_values, D_values, Q_values, m_values, df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc)\n",
    "\n",
    "            # Create a directory for each user\n",
    "            if not os.path.exists('/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Sept_Results/' + str(bin_len) + '/' + str(i)):\n",
    "                os.makedirs('/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Sept_Results/' + str(bin_len) + '/' + str(i))\n",
    "            else:\n",
    "                # If directory already exists, then prediction has already been done for this user, so skip\n",
    "                print(\"User {} already exists\".format(i))\n",
    "                continue\n",
    "            # Navigate to the directory\n",
    "            os.chdir('/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Sept_Results/' + str(bin_len) + '/' + str(i))\n",
    "\n",
    "            # Save figure to file\n",
    "            fig1.savefig('train_test_sets_plot.png', dpi=300)\n",
    "            fig2.savefig('training_loss_plot.png', dpi=300)\n",
    "            fig3.savefig('mtgp_predictions_plot.png', dpi=300)\n",
    "            fig4.savefig('training_loss_plot_rbf.png', dpi=300)\n",
    "            fig5.savefig('mtgp_rbf_predictions_plot.png', dpi=300)\n",
    "            fig6.savefig('li_predictions_plot.png', dpi=300)\n",
    "            fig7.savefig('ses_predictions_plot.png', dpi=300)\n",
    "            fig8.savefig('holt_predictions_plot.png', dpi=300)\n",
    "            fig9.savefig('es_predictions_plot.png', dpi=300)\n",
    "            fig10.savefig('arima_predictions_plot.png', dpi=300)\n",
    "            fig11.savefig('sarimax_predictions_plot.png', dpi=300)\n",
    "\n",
    "            # Create dictionary to store parameters\n",
    "            params = {\n",
    "                'max_speed_kmh': max_speed_kmh,\n",
    "                'spatial_radius_km': spatial_radius_km,\n",
    "                'bin_len': bin_len,\n",
    "                'tdf.shape[0]': tdf.shape[0],\n",
    "                'f_tdf.shape[0]': f_tdf.shape[0],\n",
    "                'fc_tdf.shape[0]': fc_tdf.shape[0],\n",
    "                'fcu_tdf.shape[0]': fcu_tdf.shape[0],\n",
    "                'curr_ocp': curr_ocp,\n",
    "                'target_ocp': target_ocp,\n",
    "                'new_ocp': new_ocp,\n",
    "                'n_train': n_train,\n",
    "                'n_test': n_test,\n",
    "                'n_dims': n_dims,\n",
    "                'mean_lat': mean_lat,\n",
    "                'mean_long': mean_long,\n",
    "                'std_lat': std_lat,\n",
    "                'std_long': std_long,\n",
    "                'init_lengthscale': init_lengthscale,\n",
    "                'init_period_len_1': init_period_len_1,\n",
    "                'init_period_len_2': init_period_len_2,\n",
    "                'log_ll': log_ll,\n",
    "                'm': m,\n",
    "                'bic': bic,\n",
    "                'gp_runtime': runtime,\n",
    "                'ses_smoothing_level': ses_lat.params['smoothing_level'],\n",
    "                'holt_smoothing_level_lat': holt_lat.params['smoothing_level'],\n",
    "                'holt_smoothing_slope_lat': holt_lat.params['smoothing_trend'],\n",
    "                'holt_smoothing_level_lon': holt_lon.params['smoothing_level'],\n",
    "                'holt_smoothing_slope_lon': holt_lon.params['smoothing_trend'],\n",
    "                'holt_damping_slope_lat': holt_lat.params['damping_trend'],\n",
    "                'holt_damping_slope_lon': holt_lon.params['damping_trend'],\n",
    "                'es_seasonal_periods': es_seasonal_periods,\n",
    "                'es_smoothing_level_lat': es_lat.params['smoothing_level'],\n",
    "                'es_smoothing_slope_lat': es_lat.params['smoothing_trend'],\n",
    "                'es_smoothing_level_lon': es_lon.params['smoothing_level'],\n",
    "                'es_smoothing_slope_lon': es_lon.params['smoothing_trend'],\n",
    "                'es_damping_slope_lat': es_lat.params['damping_trend'],\n",
    "                'es_damping_slope_lon': es_lon.params['damping_trend'],\n",
    "                'arima_order': best_arima_order,\n",
    "                'sarimax_order': best_sarimax_order,\n",
    "                'sarimax_seasonal_order': best_seasonal_order\n",
    "            }\n",
    "\n",
    "            print(\"Saving parameters...\")\n",
    "\n",
    "            # See the differences in metric results\n",
    "            \n",
    "            # Convert all values to float, except for tuples\n",
    "            for k, v in params.items():\n",
    "                try:\n",
    "                    params[k] = float(v)\n",
    "                except TypeError:\n",
    "                    pass\n",
    "            # Write params to a file\n",
    "            with open('params_' + str(i) + '.json', 'w') as fp:\n",
    "                json.dump(params, fp)\n",
    "\n",
    "            # Create dataframe to store parameters\n",
    "            params_df = pd.DataFrame.from_dict(params, orient='index')\n",
    "            params_df.columns = ['value']\n",
    "            params_df.to_csv('params_' + str(i) + '.csv')\n",
    "\n",
    "            # Create dataframe to store results\n",
    "            results = pd.DataFrame.from_dict([mtgp_res, mtgp_res_rbf, ses_res, holt_res, es_res, arima_res, sarimax_res])\n",
    "            results['model'] = ['MTGP_Comp', 'MTGP_RBF', 'SES', 'Holt', 'ES', 'ARIMA', 'SARIMAX']\n",
    "            results = results.set_index('model')\n",
    "\n",
    "            # Create dataframe to store scalar scikit-mobility metrics\n",
    "            skmob_metrics_df = pd.DataFrame(columns=['no_loc', 'rg', 'k_rg',    \n",
    "                                                'spat_burst', 'rand_entr', \n",
    "                                                'real_entr', 'uncorr_entr',\n",
    "                                                'max_dist', 'dist_straight', 'max_dist_home', \n",
    "                                                'recency', 'freq_rank'])\n",
    "\n",
    "            skmob_metrics_df['methods'] = ['MTGP_Comp', 'MTGP_RBF', 'SES', 'Holt', 'ES', 'ARIMA', 'SARIMAX','LI', 'Ground Truth']\n",
    "            # Make methods the index\n",
    "            skmob_metrics_df = skmob_metrics_df.set_index('methods')\n",
    "\n",
    "            skmob_metrics_df.iloc[0] = mtgp_tdf.no_loc_gp_pred, mtgp_tdf.rg_gp_pred, mtgp_tdf.k_rg_gp_pred, mtgp_tdf.spat_burst_gp_pred, mtgp_tdf.rand_entr_gp_pred, mtgp_tdf.real_entr_gp_pred, mtgp_tdf.uncorr_entr_gp_pred, mtgp_tdf.max_dist_gp_pred, mtgp_tdf.dist_straight_gp_pred, mtgp_tdf.max_dist_home_gp_pred, mtgp_rec_acc, mtgp_freq_rank_acc\n",
    "            skmob_metrics_df.iloc[1] = mtgp_tdf_rbf.no_loc_gp_pred, mtgp_tdf_rbf.rg_gp_pred, mtgp_tdf_rbf.k_rg_gp_pred, mtgp_tdf_rbf.spat_burst_gp_pred, mtgp_tdf_rbf.rand_entr_gp_pred, mtgp_tdf_rbf.real_entr_gp_pred, mtgp_tdf_rbf.uncorr_entr_gp_pred, mtgp_tdf_rbf.max_dist_gp_pred, mtgp_tdf_rbf.dist_straight_gp_pred, mtgp_tdf_rbf.max_dist_home_gp_pred, mtgp_rec_acc_rbf, mtgp_freq_rank_acc_rbf\n",
    "            skmob_metrics_df.iloc[2] = ses_tdf.no_loc_ses_pred, ses_tdf.rg_ses_pred, ses_tdf.k_rg_ses_pred, ses_tdf.spat_burst_ses_pred, ses_tdf.rand_entr_ses_pred, ses_tdf.real_entr_ses_pred, ses_tdf.uncorr_entr_ses_pred, ses_tdf.max_dist_ses_pred, ses_tdf.dist_straight_ses_pred, ses_tdf.max_dist_home_ses_pred, ses_rec_acc, ses_freq_rank_acc\n",
    "            skmob_metrics_df.iloc[3] = holt_tdf.no_loc_holt_pred, holt_tdf.rg_holt_pred, holt_tdf.k_rg_holt_pred, holt_tdf.spat_burst_holt_pred, holt_tdf.rand_entr_holt_pred, holt_tdf.real_entr_holt_pred, holt_tdf.uncorr_entr_holt_pred, holt_tdf.max_dist_holt_pred, holt_tdf.dist_straight_holt_pred, holt_tdf.max_dist_home_holt_pred, holt_rec_acc, holt_freq_rank_acc\n",
    "            skmob_metrics_df.iloc[4] = es_tdf.no_loc_es_pred, es_tdf.rg_es_pred, es_tdf.k_rg_es_pred, es_tdf.spat_burst_es_pred, es_tdf.rand_entr_es_pred, es_tdf.real_entr_es_pred, es_tdf.uncorr_entr_es_pred, es_tdf.max_dist_es_pred, es_tdf.dist_straight_es_pred, es_tdf.max_dist_home_es_pred, es_rec_acc, es_freq_rank_acc\n",
    "            skmob_metrics_df.iloc[5] = arima_tdf.no_loc_arima_pred, arima_tdf.rg_arima_pred, arima_tdf.k_rg_arima_pred, arima_tdf.spat_burst_arima_pred, arima_tdf.rand_entr_arima_pred, arima_tdf.real_entr_arima_pred, arima_tdf.uncorr_entr_arima_pred, arima_tdf.max_dist_arima_pred, arima_tdf.dist_straight_arima_pred, arima_tdf.max_dist_home_arima_pred, arima_rec_acc, arima_freq_rank_acc\n",
    "            skmob_metrics_df.iloc[6] = sarimax_tdf.no_loc_sarimax_pred, sarimax_tdf.rg_sarimax_pred, sarimax_tdf.k_rg_sarimax_pred, sarimax_tdf.spat_burst_sarimax_pred, sarimax_tdf.rand_entr_sarimax_pred, sarimax_tdf.real_entr_sarimax_pred, sarimax_tdf.uncorr_entr_sarimax_pred, sarimax_tdf.max_dist_sarimax_pred, sarimax_tdf.dist_straight_sarimax_pred, sarimax_tdf.max_dist_home_sarimax_pred, sarimax_rec_acc, sarimax_freq_rank_acc\n",
    "            skmob_metrics_df.iloc[7] = LI_tdf.no_loc_li_pred, LI_tdf.rg_li_pred, LI_tdf.k_rg_li_pred, LI_tdf.spat_burst_li_pred, LI_tdf.rand_entr_li_pred, LI_tdf.real_entr_li_pred, LI_tdf.uncorr_entr_li_pred, LI_tdf.max_dist_li_pred, LI_tdf.dist_straight_li_pred, LI_tdf.max_dist_home_li_pred, LI_rec_acc, LI_freq_rank_acc\n",
    "            skmob_metrics_df.iloc[8] = df_curr_metrics.no_loc_gt_pred, df_curr_metrics.rg_gt_pred, df_curr_metrics.k_rg_gt_pred, df_curr_metrics.spat_burst_gt_pred, df_curr_metrics.rand_entr_gt_pred, df_curr_metrics.real_entr_gt_pred, df_curr_metrics.uncorr_entr_gt_pred, df_curr_metrics.max_dist_gt_pred, df_curr_metrics.dist_straight_gt_pred, df_curr_metrics.max_dist_home_gt_pred, -1, -1,\n",
    "            \n",
    "            # Find absolute difference between predicted and ground truth\n",
    "            skmob_metrics_df['no_loc_error'] = skmob_metrics_df['no_loc'] - skmob_metrics_df.iloc[8]['no_loc']\n",
    "            skmob_metrics_df['rg_error'] = skmob_metrics_df['rg'] - skmob_metrics_df.iloc[8]['rg']\n",
    "            skmob_metrics_df['k_rg_error'] = skmob_metrics_df['k_rg'] - skmob_metrics_df.iloc[8]['k_rg']\n",
    "            skmob_metrics_df['spat_burst_error'] = skmob_metrics_df['spat_burst'] - skmob_metrics_df.iloc[8]['spat_burst']\n",
    "            skmob_metrics_df['rand_entr_error'] = skmob_metrics_df['rand_entr'] - skmob_metrics_df.iloc[8]['rand_entr']\n",
    "            skmob_metrics_df['real_entr_error'] = skmob_metrics_df['real_entr'] - skmob_metrics_df.iloc[8]['real_entr']\n",
    "            skmob_metrics_df['uncorr_entr_error'] = skmob_metrics_df['uncorr_entr'] - skmob_metrics_df.iloc[8]['uncorr_entr']\n",
    "            skmob_metrics_df['max_dist_error'] = skmob_metrics_df['max_dist'] - skmob_metrics_df.iloc[8]['max_dist']\n",
    "            skmob_metrics_df['dist_straight_error'] = skmob_metrics_df['dist_straight'] - skmob_metrics_df.iloc[8]['dist_straight']\n",
    "            skmob_metrics_df['max_dist_home_error'] = skmob_metrics_df['max_dist_home'] - skmob_metrics_df.iloc[8]['max_dist_home']\n",
    "\n",
    "            # Find mean absolute error (MAE) and median absolute error for each method from the absolute differences in each metric\n",
    "            skmob_metrics_df['mae'] = (1/10) * (abs(skmob_metrics_df['no_loc_error']) + abs(skmob_metrics_df['rg_error']) + abs(skmob_metrics_df['k_rg_error']) + abs(skmob_metrics_df['spat_burst_error']) + abs(skmob_metrics_df['rand_entr_error']) + abs(skmob_metrics_df['real_entr_error']) + abs(skmob_metrics_df['uncorr_entr_error']) + abs(skmob_metrics_df['max_dist_error']) + abs(skmob_metrics_df['dist_straight_error']) + abs(skmob_metrics_df['max_dist_home_error']) )\n",
    "            skmob_metrics_df['mad'] = np.median(0 - np.median(np.array([abs(skmob_metrics_df['no_loc_error']), abs(skmob_metrics_df['rg_error']), abs(skmob_metrics_df['k_rg_error']), abs(skmob_metrics_df['spat_burst_error']), abs(skmob_metrics_df['rand_entr_error']), abs(skmob_metrics_df['real_entr_error']), abs(skmob_metrics_df['uncorr_entr_error']), abs(skmob_metrics_df['max_dist_error']), abs(skmob_metrics_df['dist_straight_error']), abs(skmob_metrics_df['max_dist_home_error']) ])))\n",
    "            # Write skmob metrics to a file\n",
    "            skmob_metrics_df.to_csv('skmob_metrics_' + str(i) + '.csv')\n",
    "\n",
    "            # Write results to a file\n",
    "            results.to_csv('results_' + str(i) + '.csv')\n",
    "\n",
    "            # Create a directory for all results if it doesn't exist\n",
    "            if not os.path.exists('/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Sept_Results/' + str(bin_len) + '/all_results'):\n",
    "                os.makedirs('/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Sept_Results/' + str(bin_len) + '/all_results')\n",
    "            # Navigate to the directory\n",
    "            os.chdir('/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Sept_Results/' + str(bin_len) + '/all_results')\n",
    "\n",
    "            # Write results there as well\n",
    "            results.to_csv('results_' + str(i) + '.csv')\n",
    "\n",
    "            # Create a directory for all skmob metrics if it doesn't exist\n",
    "            if not os.path.exists('/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Sept_Results/' + str(bin_len) + '/all_skmob_metrics'):\n",
    "                os.makedirs('/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Sept_Results/' + str(bin_len) + '/all_skmob_metrics')\n",
    "            # Navigate to the directory\n",
    "            os.chdir('/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Sept_Results/' + str(bin_len) + '/all_skmob_metrics')\n",
    "\n",
    "            # Write skmob metrics there as well\n",
    "            skmob_metrics_df.to_csv('skmob_metrics_' + str(i) + '.csv')\n",
    "\n",
    "            # Create a directory for all parameters if it doesn't exist\n",
    "            if not os.path.exists('/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Sept_Results/' + str(bin_len) + '/all_parameters'):\n",
    "                os.makedirs('/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Sept_Results/' + str(bin_len) + '/all_parameters')\n",
    "            # Navigate to the directory\n",
    "            os.chdir('/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Sept_Results/' + str(bin_len) + '/all_parameters')\n",
    "\n",
    "            # Write parameters there as well\n",
    "            params_df.to_csv('params_' + str(i) + '.csv')\n",
    "\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skmob\n",
    "import pandas as pd\n",
    "import skmob.measures.individual as ind_measure\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.kernels import RQKernel as RQ, RBFKernel as SE, \\\n",
    "PeriodicKernel as PER, ScaleKernel, LinearKernel as LIN, MaternKernel as MAT, \\\n",
    "SpectralMixtureKernel as SMK, PiecewisePolynomialKernel as PPK, CylindricalKernel as CYL\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gpytorch.constraints import Interval\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import statistics as stats\n",
    "\n",
    "# Import intra-package scripts\n",
    "import utils.helper_func as helper_func\n",
    "import utils.GP as GP\n",
    "from utils.helper_func import dec_floor\n",
    "import mobileDataToolkit.analysis as analysis\n",
    "import mobileDataToolkit.preprocessing_v2 as preprocessing\n",
    "import mobileDataToolkit.methods as methods\n",
    "import mobileDataToolkit.metrics as metrics\n",
    "\n",
    "# Import benchmarks\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingLossPlot(ls):\n",
    "    iters = range(0, len(ls))\n",
    "    fig4, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.plot(iters, ls, 'g')\n",
    "    ax.set_title('Training Loss')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    return fig4\n",
    "\n",
    "def predictionsVsActualPlot(y_test_scaled, mean):\n",
    "    plt.rcParams.update({'font.size': 9})\n",
    "            # Make the font nicer\n",
    "    plt.rcParams.update({'font.family': 'serif'})\n",
    "    fig3, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.set_title('Predictions')\n",
    "    try:\n",
    "        pd.DataFrame(mean.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='red', alpha=0.5, s=0.4, label='Predictions')\n",
    "    except AttributeError: \n",
    "        pd.DataFrame(mean).plot(x=1, y=0, kind='scatter',ax=ax, color='red', alpha=0.5, s=0.4, label='Predictions')\n",
    "    pd.DataFrame(y_test_scaled.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='blue', alpha=0.5, s=0.4, label='Actual')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    return fig3\n",
    "\n",
    "def trainTestPlot(gapped_user_data, curr_mt):\n",
    "    plt.rcParams.update({'font.size': 9})\n",
    "    plt.rcParams.update({'font.family': 'serif'})\n",
    "    fig1, ax = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n",
    "    ax[0].scatter(curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_lat'],\n",
    "                            color='blue', label='Training data', s=1)\n",
    "    ax[0].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_lat'],\n",
    "                            color='red', label='Test data', s=1)\n",
    "    ax[0].set_ylabel('Latitude')\n",
    "    ax[1].scatter(curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_long'],\n",
    "                            color='blue', label='Training data', s=1)\n",
    "    ax[1].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_long'],\n",
    "                            color='red', label='Test data', s=1)\n",
    "    ax[1].set_xlabel('Time')\n",
    "    ax[1].set_ylabel('Longitude')\n",
    "    ax[1].legend()\n",
    "    return fig1\n",
    "\n",
    "def makeSeries(y_train_scaled, y_test_scaled, unix_min_tr, unix_min_te):\n",
    "    lat = pd.Series(y_train_scaled[:,0].tolist(), unix_min_tr)\n",
    "    lat_t = pd.Series(y_test_scaled[:,0].tolist(), unix_min_te)\n",
    "            # Replace duplicates (in time) with the mean of the two values\n",
    "    lat = lat.groupby(lat.index).mean().reset_index()\n",
    "    lat = pd.Series(lat[0].tolist(), lat['index'].tolist())\n",
    "    lat_tc = lat_t.groupby(lat_t.index).mean().reset_index()\n",
    "    lat_tc = pd.Series(lat_tc[0].tolist(), lat_tc['index'].tolist())\n",
    "            # Replace zeroes with positives close to zero\n",
    "    lat.replace(0, 0.000000001, inplace=True)\n",
    "\n",
    "    lon = pd.Series(y_train_scaled[:,1].tolist(), unix_min_tr)\n",
    "    lon_t = pd.Series(y_test_scaled[:,1].tolist(),unix_min_te)\n",
    "            # Replace duplicates (in time) with the mean of the two values\n",
    "    lon = lon.groupby(lon.index).mean().reset_index()\n",
    "    lon = pd.Series(lon[0].tolist(), lon['index'].tolist())\n",
    "    lon_tc = lon_t.groupby(lon_t.index).mean().reset_index()\n",
    "    lon_tc = pd.Series(lon_tc[0].tolist(), lon_tc['index'].tolist())\n",
    "            # Replace zeroes with positives close to zero\n",
    "    lon.replace(0, 0.000000001, inplace=True)\n",
    "    return lat,lat_tc,lon,lon_tc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/ekinokos2/Library/CloudStorage/OneDrive-UW/GPR/Data/seattle_2000_all_obs_sampled.csv\"\n",
    "df = pd.read_csv(file_path, header=0)\n",
    "\n",
    "# Add month column\n",
    "df['month'] = pd.DatetimeIndex(df['datetime']).month\n",
    "\n",
    "# Group by user ID, find month with third most observations (average)\n",
    "#df_m = df.groupby('UID').apply(lambda x: x[x['month'] == x['month'].value_counts().index[2]])\n",
    "\n",
    "# Groupby user ID, keep all observations from January and February \n",
    "df_m = df.groupby('UID').apply(lambda x: x[x['month'].isin([1,2])])\n",
    "\n",
    "df_m = df_m.reset_index(drop=True)\n",
    "\n",
    "max_speed_kmh = 400 # for filtering out unrealistic speeds\n",
    "spatial_radius_km = 0.3 # for compressing similar points using Douglas-Peucker algorithm\n",
    "bin_len_ls = [10080, 1440, 360, 60, 30, 15] # Bin lengths to test: 1 week, 1 day, 6 hours, 1 hour, 30 min, 15 min\n",
    "init_period_len_1 = 60*8 # 8 hours\n",
    "init_period_len_2 = 60*24 # 24 hours\n",
    "lr = 0.3\n",
    "n_epochs=150\n",
    "\n",
    "p_values = range(0, 3)  # Example range for p\n",
    "d_values = range(0, 2)  # Example range for d\n",
    "q_values = range(0, 3)  # Example range for q\n",
    "P_values = range(0, 3)  # Example range for P\n",
    "D_values = range(0, 2)  # Example range for D\n",
    "Q_values = range(0, 3)  # Example range for Q\n",
    "m_values = range(0, 3)  # Example range for m\n",
    "\n",
    "bin_len=10080\n",
    "\n",
    "runtimes_comp = []\n",
    "bics_comp = []\n",
    "runtimes_rbf = []\n",
    "bics_rbf = []\n",
    "\n",
    "df_curr = df_m[df_m.UID == df_m.UID.unique()[3]]\n",
    "\n",
    "tdf = skmob.TrajDataFrame(df_curr, latitude='orig_lat', longitude='orig_long', datetime='datetime')\n",
    "f_tdf = skmob.preprocessing.filtering.filter(tdf, max_speed_kmh=max_speed_kmh, include_loops=False)\n",
    "# Print the difference in number of rows\n",
    "print(\"Number of rows before filtering: {}\".format(tdf.shape[0]))\n",
    "print(\"Number of rows after filtering: {}\".format(f_tdf.shape[0]))\n",
    "fc_tdf = skmob.preprocessing.compression.compress(f_tdf, spatial_radius_km=spatial_radius_km)\n",
    "# Print the difference in number of rows\n",
    "print(\"Number of rows after compression: {}\".format(fc_tdf.shape[0]))\n",
    "# Remove data points with uncertainty > 100m\n",
    "fcu_tdf = fc_tdf[fc_tdf['orig_unc'] <= 100]\n",
    "# Print the difference in number of rows\n",
    "print(\"Number of rows after uncertainty filtering: {}\".format(fcu_tdf.shape[0]))\n",
    "df_curr = fcu_tdf\n",
    "\n",
    "# Calculate sci-kit mobility metrics\n",
    "df_curr_metrics = helper_func.skmob_metric_calcs(df_curr, method='GT', lat='lat', long='lng', datetime='datetime')\n",
    "\n",
    "# Remove duplicates in the unix column\n",
    "df_curr = df_curr.drop_duplicates(subset=['unix_min'])\n",
    "\n",
    "curr_ocp = analysis.tempOcp(df_curr, 'unix_min', bin_len=bin_len)\n",
    "\n",
    "upper_bound = dec_floor(curr_ocp)\n",
    "\n",
    "# See current temporal occupancy\n",
    "print(\"Current temporal occupancy: {}\".format(curr_ocp))\n",
    "while True:\n",
    "    try:\n",
    "        if curr_ocp <= 0.1:\n",
    "            target_ocp = np.random.uniform(0, curr_ocp)\n",
    "        else:\n",
    "            # Choose random decimal between 0 and upper bound\n",
    "            target_ocp = dec_floor(np.random.uniform(0.1, upper_bound))\n",
    "        print(\"Target temporal occupancy: {}\".format(target_ocp))\n",
    "        # Simulate gaps in the user's data to match the target level\n",
    "        gapped_user_data, train_index, new_ocp = analysis.simulate_gaps(df_curr, target_ocp, unix_col='unix_min', bin_len= bin_len)\n",
    "    except:\n",
    "        continue\n",
    "    break\n",
    "\n",
    "# Change name of 'lat' and 'lon' columns to 'orig_lat' and 'orig_long'\n",
    "df_curr = df_curr.rename(columns={'lat': 'orig_lat', 'lng': 'orig_long'})\n",
    "\n",
    "# Create MultiTrip object\n",
    "curr_mt = preprocessing.dp_MultiTrip(data=df_curr)\n",
    "curr_mt.Multi_Trip_Preprocess(lat='orig_lat', long='orig_long', datetime='datetime')\n",
    "\n",
    "# Move 'unix_start_t' to before 'SaM'\n",
    "cols = list(curr_mt.data.columns)\n",
    "cols.insert(16, cols.pop(cols.index('unix_min')))\n",
    "curr_mt.data = curr_mt.data.loc[:, cols] \n",
    "# Print data columns\n",
    "# print(curr_mt.data.columns)\n",
    "\n",
    "curr_mt.Multi_Trip_TrainTestSplit(test_start_date=None, test_end_date=None, \n",
    "                        training_index = set(gapped_user_data['unix_min']), lat='orig_lat', \n",
    "                        long='orig_long', datetime='datetime', unix='unix_min', inputstart='unix_min', \n",
    "                        inputend=curr_mt.data.columns[-1])\n",
    "\n",
    "n_train = len(curr_mt.X_train[:,0])\n",
    "n_test = len(curr_mt.X_test[:,0])\n",
    "n_dims = curr_mt.X_train.shape[1]\n",
    "\n",
    "# See number of points in training and test sets\n",
    "print(\"Number of points in training set: {}\".format(n_train))\n",
    "print(\"Number of points in test set: {}\".format(n_test))\n",
    "print(\"Number of input dimensions: {}\".format(n_dims))\n",
    "\n",
    "# Visualize the training and test data in two subplots, one lat vs time and one long vs time\n",
    "plt.rcParams.update({'font.size': 9})\n",
    "plt.rcParams.update({'font.family': 'serif'})\n",
    "fig1, ax = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n",
    "ax[0].scatter(curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_lat'],\n",
    "                color='blue', label='Training data', s=1)\n",
    "ax[0].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_lat'],\n",
    "                color='red', label='Test data', s=1)\n",
    "ax[0].set_ylabel('Latitude')\n",
    "ax[1].scatter(curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_long'],\n",
    "                color='blue', label='Training data', s=1)\n",
    "ax[1].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_long'],\n",
    "                color='red', label='Test data', s=1)\n",
    "ax[1].set_xlabel('Time')\n",
    "ax[1].set_ylabel('Longitude')\n",
    "ax[1].legend()\n",
    "\n",
    "\n",
    "mean_lat = curr_mt.y_train[:,0].mean()\n",
    "mean_long = curr_mt.y_train[:,1].mean()\n",
    "std_lat = curr_mt.y_train[:,0].std()\n",
    "std_long = curr_mt.y_train[:,1].std()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "y_train_scaled = torch.tensor(np.float64(scaler.fit_transform(curr_mt.y_train)))\n",
    "y_test_scaled = torch.tensor(np.float64(scaler.transform(curr_mt.y_test)))\n",
    "# Unix time for benchmarks\n",
    "unix_min_tr = np.array(curr_mt.X_train[:,0]).astype(int)\n",
    "unix_min_te = np.array(curr_mt.X_test[:,0]).astype(int)\n",
    "\n",
    "lat, lat_tc, lon, lon_tc = makeSeries(y_train_scaled, y_test_scaled, unix_min_tr, unix_min_te)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)\n",
    "\n",
    "# Composite model with RQ * PER kernels\n",
    "model = GP.MTGPRegressor(curr_mt.X_train, y_train_scaled, \n",
    "                        ScaleKernel(ScaleKernel(RQ(ard_num_dims=n_dims)) * ScaleKernel(PER(active_dims=[0]))) + \n",
    "                        ScaleKernel(ScaleKernel(RQ(ard_num_dims=n_dims)) * ScaleKernel(PER(active_dims=[0])))\n",
    ")\n",
    "# Set initial lengthscale guess as half the average length of gap in training set\n",
    "init_lengthscale = curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'].diff().mean() / 2 \n",
    "initializations = np.ones(n_dims - 1)\n",
    "initializations = np.insert(initializations, 0, init_lengthscale)\n",
    "model.covar_module.data_covar_module.kernels[0].base_kernel.kernels[0].base_kernel.lengthscale = initializations\n",
    "model.covar_module.data_covar_module.kernels[1].base_kernel.kernels[0].base_kernel.lengthscale = initializations\n",
    "\n",
    "# Set initial period lengths\n",
    "model.covar_module.data_covar_module.kernels[0].base_kernel.kernels[1].base_kernel.period_length = init_period_len_1\n",
    "model.covar_module.data_covar_module.kernels[1].base_kernel.kernels[1].base_kernel.period_length = init_period_len_2\n",
    "\n",
    "# Train model\n",
    "start = time.time()\n",
    "ls, mll = GP.training(model, curr_mt.X_train, y_train_scaled, lr=lr, n_epochs=n_epochs)\n",
    "end = time.time()\n",
    "runtime = end - start\n",
    "runtimes_comp.append(runtime)\n",
    "\n",
    "iters = range(0, len(ls))\n",
    "fig2, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(iters, ls, 'g')\n",
    "ax.set_title('Training Loss')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    log_ll = mll(model(curr_mt.X_train), y_train_scaled) * curr_mt.X_train.shape[0]\n",
    "            \n",
    "N = curr_mt.X_train.shape[0]\n",
    "m = sum(p.numel() for p in model.hyperparameters())\n",
    "bic = -2 * log_ll + m * np.log(N)\n",
    "bics_comp.append(bic)\n",
    "\n",
    "predictions, mean = model.predict(curr_mt.X_test)\n",
    "\n",
    "# Use smaller font\n",
    "plt.rcParams.update({'font.size': 9})\n",
    "# Make the font nicer\n",
    "plt.rcParams.update({'font.family': 'serif'})\n",
    "fig3, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.set_title('Predictions')\n",
    "pd.DataFrame(mean.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='red', alpha=0.5, s=0.4, label='Predictions')\n",
    "pd.DataFrame(y_test_scaled.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='blue', alpha=0.5, s=0.4, label='Actual')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "# Model results\n",
    "mtgp_res = metrics.average_eval(pd.Series(y_test_scaled[:,0]), pd.Series(y_test_scaled[:,1]), pd.Series(mean[:,0]), pd.Series(mean[:,1]))\n",
    "\n",
    "mtgp_sim = metrics.evaluate_similarity(lat_tc, mean, y_test_scaled)\n",
    "\n",
    "# Convert mean predictions back to original scale in lat/long\n",
    "orig_preds = scaler.inverse_transform(mean.reshape(-1,2))\n",
    "\n",
    "#GP_full_preds_df = helper_func.preds_to_full_df(preds_lat=orig_preds[:,0], preds_long=orig_preds[:,1], \n",
    "#                                            test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "\n",
    "# Changelog: 09/30/2023\n",
    "# Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "GP_full_preds_df = pd.DataFrame(orig_preds, columns=['lat', 'long'])\n",
    "GP_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "\n",
    "mtgp_tdf = helper_func.skmob_metric_calcs(GP_full_preds_df, method='GP', lat='lat', long='long', datetime='datetime')\n",
    "\n",
    "mtgp_rec_acc = helper_func.matrix_acc(mtgp_tdf.recency_gp_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "mtgp_freq_rank_acc = helper_func.matrix_acc(mtgp_tdf.freq_rank_gp_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    " # Kernel benchmark with a simpler kernel\n",
    "# RBF Kernel with ARD\n",
    "model_rbf = GP.MTGPRegressor(curr_mt.X_train, y_train_scaled,\n",
    "                        SE(ard_num_dims=n_dims)\n",
    ")\n",
    "\n",
    "# Set initial lengthscale guess for unix_min as half the average length of gap in training set\n",
    "init_lengthscale = curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'].diff().mean() / 2\n",
    "# Set the rest of the lengthscale guesses to 1\n",
    "initializations = np.ones(n_dims - 1)\n",
    "initializations = np.insert(initializations, 0, init_lengthscale)\n",
    "model_rbf.covar_module.data_covar_module.lengthscale = initializations\n",
    "\n",
    "# Train model\n",
    "start = time.time()\n",
    "ls, mll = GP.training(model_rbf, curr_mt.X_train, y_train_scaled, lr=lr, n_epochs=n_epochs)\n",
    "end = time.time()\n",
    "runtime = end - start\n",
    "runtimes_rbf.append(runtime)\n",
    "\n",
    "fig4 = trainingLossPlot(ls)\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(model_rbf.likelihood, model_rbf)\n",
    "\n",
    "with torch.no_grad():\n",
    "    log_ll = mll(model_rbf(curr_mt.X_train), y_train_scaled) * curr_mt.X_train.shape[0]\n",
    "\n",
    "N = curr_mt.X_train.shape[0]\n",
    "m = sum(p.numel() for p in model_rbf.hyperparameters())\n",
    "bic = -2 * log_ll + m * np.log(N)\n",
    "bics_rbf.append(bic)\n",
    "\n",
    "predictions_rbf, mean_rbf = model_rbf.predict(curr_mt.X_test)\n",
    "\n",
    "fig5 = predictionsVsActualPlot(y_test_scaled, mean_rbf)\n",
    "\n",
    "# Model results\n",
    "mtgp_res_rbf = metrics.average_eval(pd.Series(y_test_scaled[:,0]), pd.Series(y_test_scaled[:,1]), pd.Series(mean_rbf[:,0]), pd.Series(mean_rbf[:,1]))\n",
    "\n",
    "mtgp_sim_rbf = metrics.evaluate_similarity(lat_tc, mean_rbf, y_test_scaled)\n",
    "\n",
    "# Convert mean predictions back to original scale in lat/long\n",
    "orig_preds_rbf = scaler.inverse_transform(mean_rbf.reshape(-1,2))\n",
    "\n",
    "GP_full_preds_df_rbf = helper_func.preds_to_full_df(preds_lat=orig_preds_rbf[:,0], preds_long=orig_preds_rbf[:,1],\n",
    "                                                test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "\n",
    "mtgp_tdf_rbf = helper_func.skmob_metric_calcs(GP_full_preds_df_rbf, method='GP_RBF', lat='lat', long='long', datetime='datetime')\n",
    "\n",
    "mtgp_rec_acc_rbf = helper_func.matrix_acc(mtgp_tdf_rbf.recency_gp_rbf_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "mtgp_freq_rank_acc_rbf = helper_func.matrix_acc(mtgp_tdf_rbf.freq_rank_gp_rbf_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "\n",
    "#try:\n",
    "# Linear Interpolation\n",
    "print(\"Running Linear Interpolation...\")\n",
    "LI_preds_lat, LI_preds_long = methods.LI(curr_mt.X_train[:,0], curr_mt.X_test[:,0], y_train_scaled, y_test_scaled)\n",
    "\n",
    "LI_preds_df = pd.DataFrame(LI_preds_lat, columns=['lat'])\n",
    "LI_preds_df['long'] = LI_preds_long\n",
    "\n",
    "LI_preds_origs = scaler.inverse_transform(LI_preds_df)\n",
    "\n",
    "LI_full_preds_df = helper_func.preds_to_full_df(preds_lat=LI_preds_origs[:,0], preds_long=LI_preds_origs[:,1], \n",
    "                                            test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "\n",
    "LI_preds = np.hstack(((LI_preds_lat.reshape(-1, 1), LI_preds_long.reshape(-1, 1))))\n",
    "\n",
    "LI_tdf = helper_func.skmob_metric_calcs(LI_full_preds_df, method='LI', lat='lat', long='long', datetime='datetime')\n",
    "LI_res = metrics.average_eval(np.array(y_test_scaled[:,0]), np.array(y_test_scaled[:,1]), LI_preds_lat, LI_preds_long)\n",
    "LI_sim = metrics.evaluate_similarity(lat_tc, LI_preds, y_test_scaled)\n",
    "\n",
    "LI_rec_acc = helper_func.matrix_acc(LI_tdf.recency_li_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "LI_freq_rank_acc = helper_func.matrix_acc(LI_tdf.freq_rank_li_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "#except:\n",
    "    #print(\"Error in LI\")\n",
    "    #LI_res = None\n",
    "    #LI_rec_acc = None\n",
    "    #LI_freq_rank_acc = None\n",
    "            \n",
    "\n",
    "print(\"Running ARIMA...\")\n",
    "best_order, best_aic = methods.find_best_arima_order(lat, p_values, d_values, q_values)\n",
    "\n",
    "arima_lat = ARIMA(lat, order=best_order).fit()\n",
    "pred_lat_arima = arima_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "pred_lat_comp_arima = pred_lat_arima[pred_lat_arima.index.isin(unix_min_te)]\n",
    "\n",
    "arima_lon = ARIMA(lon, order=best_order).fit()\n",
    "pred_lon_arima = arima_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "pred_lon_comp_arima = pred_lon_arima[pred_lon_arima.index.isin(unix_min_te)]\n",
    "\n",
    "arima_preds_df = pd.DataFrame(pred_lat_comp_arima, columns=['lat'])\n",
    "arima_preds_df['long'] = pred_lon_comp_arima\n",
    "\n",
    "arima_preds_origs = scaler.inverse_transform(arima_preds_df)\n",
    "\n",
    "arima_full_preds_df = helper_func.preds_to_full_df(preds_lat=arima_preds_origs[:,0], preds_long=arima_preds_origs[:,1],\n",
    "                                            test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "\n",
    "# Stack predictions into one array\n",
    "arima_preds = np.hstack(((pred_lat_comp_arima.values.reshape(-1, 1), pred_lon_comp_arima.values.reshape(-1, 1))))\n",
    "\n",
    "arima_tdf = helper_func.skmob_metric_calcs(arima_full_preds_df, method='arima', lat='lat', long='long', datetime='datetime')\n",
    "arima_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_arima, pred_lon_comp_arima)\n",
    "arima_sim = metrics.evaluate_similarity(lat_tc, arima_preds, y_test_scaled)\n",
    "\n",
    "arima_rec_acc = helper_func.matrix_acc(arima_tdf.recency_arima_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "arima_freq_rank_acc = helper_func.matrix_acc(arima_tdf.freq_rank_arima_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "# Use smaller font\n",
    "plt.rcParams.update({'font.size': 9})\n",
    "# Make the font nicer\n",
    "plt.rcParams.update({'font.family': 'serif'})\n",
    "fig3, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.set_title('Predictions')\n",
    "pd.DataFrame(arima_preds).plot(x=1, y=0, kind='scatter',ax=ax, color='red', alpha=0.5, s=0.4, label='Predictions')\n",
    "pd.DataFrame(y_test_scaled.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='blue', alpha=0.5, s=0.4, label='Actual')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "print(\"Running Holt-Winters model...\")\n",
    "holt_lat = Holt(lat, damped_trend=True, initialization_method=\"estimated\").fit()\n",
    "pred_lat_holt = holt_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "pred_lat_comp_holt = pred_lat_holt[pred_lat_holt.index.isin(unix_min_te)]\n",
    "\n",
    "#holt_smoothing_level_lon=0.1\n",
    "#holt_smoothing_slope_lon=0.0307\n",
    "\n",
    "holt_lon = Holt(lon, damped_trend=True, initialization_method=\"estimated\").fit()\n",
    "pred_lon_holt = holt_lon.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "pred_lon_comp_holt = pred_lon_holt[pred_lon_holt.index.isin(unix_min_te)]\n",
    "\n",
    "holt_preds_df = pd.DataFrame(pred_lat_comp_holt, columns=['lat'])\n",
    "holt_preds_df['long'] = pred_lon_comp_holt\n",
    "\n",
    "holt_preds_origs = scaler.inverse_transform(holt_preds_df)\n",
    "\n",
    "holt_full_preds_df = helper_func.preds_to_full_df(preds_lat=holt_preds_origs[:,0], preds_long=holt_preds_origs[:,1],\n",
    "                                            test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "\n",
    "holt_preds = np.hstack(((pred_lat_comp_holt.values.reshape(-1, 1), pred_lon_comp_holt.values.reshape(-1, 1))))\n",
    "holt_tdf = helper_func.skmob_metric_calcs(holt_full_preds_df, method='holt', lat='lat', long='long', datetime='datetime')\n",
    "\n",
    "holt_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_holt, pred_lon_comp_holt)\n",
    "holt_sim = metrics.evaluate_similarity(lat_tc, holt_preds, y_test_scaled)\n",
    "\n",
    "holt_rec_acc = helper_func.matrix_acc(holt_tdf.recency_holt_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "holt_freq_rank_acc = helper_func.matrix_acc(holt_tdf.freq_rank_holt_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "print(\"Running Simple Exponential Smoothing...\")\n",
    "# Find best parameters for SES\n",
    "ses_lat = SimpleExpSmoothing(lat, initialization_method=\"estimated\").fit()\n",
    "pred_lat_ses = ses_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "pred_lat_comp_ses = pred_lat_ses[pred_lat_ses.index.isin(unix_min_te)]\n",
    "\n",
    "ses_lon = SimpleExpSmoothing(lon, initialization_method=\"estimated\").fit()\n",
    "pred_lon_ses = ses_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "pred_lon_comp_ses = pred_lon_ses[pred_lon_ses.index.isin(unix_min_te)]\n",
    "\n",
    "ses_preds_df = pd.DataFrame(pred_lat_comp_ses, columns=['lat'])\n",
    "ses_preds_df['long'] = pred_lon_comp_ses\n",
    "\n",
    "ses_preds_origs = scaler.inverse_transform(ses_preds_df)\n",
    "\n",
    "ses_full_preds_df = helper_func.preds_to_full_df(preds_lat=ses_preds_origs[:,0], preds_long=ses_preds_origs[:,1],\n",
    "                                            test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "ses_preds = np.hstack(((pred_lat_comp_ses.values.reshape(-1, 1), pred_lon_comp_ses.values.reshape(-1, 1))))\n",
    "ses_tdf = helper_func.skmob_metric_calcs(ses_full_preds_df, method='ses', lat='lat', long='long', datetime='datetime')\n",
    "ses_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_ses, pred_lon_comp_ses)\n",
    "ses_sim = metrics.evaluate_similarity(lat_tc, ses_preds, y_test_scaled)\n",
    "\n",
    "ses_rec_acc = helper_func.matrix_acc(ses_tdf.recency_ses_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "ses_freq_rank_acc = helper_func.matrix_acc(ses_tdf.freq_rank_ses_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "print(\"Running SARIMAX...\")\n",
    "best_sarimax_order, best_seasonal_order, best_sarimax_aic = methods.find_best_SARIMAX_order(lat, p_values, d_values, q_values, P_values, D_values, Q_values, m_values)\n",
    "sarimax_lat = SARIMAX(lat, order=best_sarimax_order, seasonal_order=best_seasonal_order).fit(disp=False)\n",
    "pred_lat_sar = sarimax_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "pred_lat_comp_sar = pred_lat_sar[pred_lat_sar.index.isin(unix_min_te)]\n",
    "\n",
    "sarimax_lon = SARIMAX(lon, order=best_sarimax_order, seasonal_order=best_seasonal_order).fit(disp=False)\n",
    "pred_lon_sar = sarimax_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "pred_lon_comp_sar = pred_lon_sar[pred_lon_sar.index.isin(unix_min_te)]\n",
    "\n",
    "#sarimax_preds_df = pd.DataFrame(pred_lat_comp_sar, columns=['lat'])\n",
    "#sarimax_preds_df['long'] = pred_lon_comp_sar\n",
    "sarimax_preds = np.hstack(((pred_lat_comp_sar.values.reshape(-1, 1), pred_lon_comp_sar.values.reshape(-1, 1))))\n",
    "sarimax_preds_origs = scaler.inverse_transform(sarimax_preds)\n",
    "\n",
    "#sarimax_full_preds_df = helper_func.preds_to_full_df(preds_lat=sarimax_preds_origs[:,0], preds_long=sarimax_preds_origs[:,1],\n",
    "#                                            test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "sarimax_full_preds_df = pd.DataFrame(sarimax_preds_origs, columns=['lat', 'long'])\n",
    "sarimax_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "\n",
    "sarimax_tdf = helper_func.skmob_metric_calcs(sarimax_full_preds_df, method='sarimax', lat='lat', long='long', datetime='datetime')\n",
    "sarimax_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_sar, pred_lon_comp_sar)\n",
    "sarimax_sim = metrics.evaluate_similarity(lat_tc, sarimax_preds, y_test_scaled)\n",
    "\n",
    "sarimax_rec_acc = helper_func.matrix_acc(sarimax_tdf.recency_sarimax_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "sarimax_freq_rank_acc = helper_func.matrix_acc(sarimax_tdf.freq_rank_sarimax_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "# Exponential Smoothing\n",
    "es_seasonal_periods=24\n",
    "\n",
    "print(\"Running Exponential Smoothing...\")\n",
    "es_lat = ExponentialSmoothing(lat, seasonal_periods=es_seasonal_periods, trend='add', seasonal='add', damped_trend=True, use_boxcox=False, initialization_method='estimated').fit()\n",
    "pred_lat_es = es_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "pred_lat_comp_es = pred_lat_es[pred_lat_es.index.isin(unix_min_te)]\n",
    "\n",
    "es_lon = ExponentialSmoothing(lon, seasonal_periods=es_seasonal_periods, trend='add', seasonal='add', damped_trend=True, use_boxcox=False, initialization_method='estimated').fit()\n",
    "pred_lon_es = es_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "pred_lon_comp_es = pred_lon_es[pred_lon_es.index.isin(unix_min_te)]\n",
    "\n",
    "es_preds_df = pd.DataFrame(pred_lat_comp_es, columns=['lat'])\n",
    "es_preds_df['long'] = pred_lon_comp_es\n",
    "\n",
    "es_preds_origs = scaler.inverse_transform(es_preds_df)\n",
    "\n",
    "es_full_preds_df = helper_func.preds_to_full_df(preds_lat=es_preds_origs[:,0], preds_long=es_preds_origs[:,1],\n",
    "                                            test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "es_preds = np.hstack(((pred_lat_comp_es.values.reshape(-1, 1), pred_lon_comp_es.values.reshape(-1, 1))))\n",
    "es_tdf = helper_func.skmob_metric_calcs(es_full_preds_df, method='es', lat='lat', long='long', datetime='datetime')\n",
    "es_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_es, pred_lon_comp_es)\n",
    "es_sim = metrics.evaluate_similarity(lat_tc, es_preds, y_test_scaled)\n",
    "\n",
    "es_rec_acc = helper_func.matrix_acc(es_tdf.recency_es_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "es_freq_rank_acc = helper_func.matrix_acc(es_tdf.freq_rank_es_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
