{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import similaritymeasures as sm\n",
    "import skmob\n",
    "import pandas as pd\n",
    "import skmob.measures.individual as ind_measure\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.kernels import RQKernel as RQ, RBFKernel as SE, PeriodicKernel as PER, ScaleKernel\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "#import statistics as stats\n",
    "from itertools import product\n",
    "\n",
    "# Import intra-package scripts\n",
    "import utils.helper_func as helper_func\n",
    "import utils.GP as GP\n",
    "from utils.helper_func import dec_floor\n",
    "import mobileDataToolkit.analysis as analysis\n",
    "import mobileDataToolkit.preprocessing_v2 as preprocessing\n",
    "import mobileDataToolkit.methods as methods\n",
    "import mobileDataToolkit.metrics as metrics\n",
    "\n",
    "# Import benchmarks\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def find_best_arima_order(data, p_values, d_values, q_values):\n",
    "    best_aic = float(\"inf\")\n",
    "    best_order = None\n",
    "\n",
    "    for p, d, q in product(p_values, d_values, q_values):\n",
    "        try:\n",
    "            model = ARIMA(data, order=(p, d, q))\n",
    "            results = model.fit()\n",
    "\n",
    "            if results.aic < best_aic:\n",
    "                best_aic = results.aic\n",
    "                best_order = (p, d, q)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    return best_order, best_aic\n",
    "\n",
    "def find_best_sarimax_order(data, p_values, d_values, q_values, P_values, D_values, Q_values, m_values):\n",
    "    best_aic = float(\"inf\")\n",
    "    best_order = None\n",
    "    best_seasonal_order = None\n",
    "\n",
    "    for p, d, q, P, D, Q, m in product(p_values, d_values, q_values, P_values, D_values, Q_values, m_values):\n",
    "        try:\n",
    "            model = sm.tsa.statespace.SARIMAX(data, order=(p, d, q), seasonal_order=(P, D, Q, m))\n",
    "            results = model.fit(disp=False)\n",
    "\n",
    "            if results.aic < best_aic:\n",
    "                best_aic = results.aic\n",
    "                best_order = (p, d, q)\n",
    "                best_seasonal_order = (P, D, Q, m)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    return best_order, best_seasonal_order, best_aic\n",
    "\n",
    "\n",
    "def evaluate_similarity(lat_tc, pred_mean, y_test_scaled):\n",
    "    preds_lat = np.hstack((pd.Series(lat_tc.index).values.reshape(-1,1), pred_mean[:,0].reshape(-1,1)))\n",
    "    test_lat = np.hstack((pd.Series(lat_tc.index).values.reshape(-1,1), y_test_scaled[:,0].reshape(-1,1)))\n",
    "\n",
    "    preds_lon = np.hstack((pd.Series(lat_tc.index).values.reshape(-1,1), pred_mean[:,1].reshape(-1,1)))\n",
    "    test_lon = np.hstack((pd.Series(lat_tc.index).values.reshape(-1,1), y_test_scaled[:,1].reshape(-1,1)))\n",
    "\n",
    "    # quantify the difference between the two curves using PCM\n",
    "    pcm_lat = sm.pcm(preds_lat, test_lat)\n",
    "    pcm_lon = sm.pcm(preds_lon, test_lon)\n",
    "\n",
    "    # quantify the difference between the two curves using\n",
    "    # Discrete Frechet distance\n",
    "    df_lat = sm.frechet_dist(preds_lat, test_lat)\n",
    "    df_lon = sm.frechet_dist(preds_lon, test_lon)\n",
    "\n",
    "    # quantify the difference between the two curves using\n",
    "    # area between two curves\n",
    "    area_lat = sm.area_between_two_curves(preds_lat, test_lat)\n",
    "    area_lon = sm.area_between_two_curves(preds_lon, test_lon)\n",
    "\n",
    "    # quantify the difference between the two curves using\n",
    "    # Curve Length based similarity measure\n",
    "    cl_lat = sm.curve_length_measure(preds_lat, test_lat)\n",
    "    cl_lon = sm.curve_length_measure(preds_lon, test_lon)\n",
    "\n",
    "    # quantify the difference between the two curves using\n",
    "    # Dynamic Time Warping distance\n",
    "    dtw_lat, d_lat = sm.dtw(preds_lat, test_lat)\n",
    "    dtw_lon, d_lon = sm.dtw(preds_lon, test_lon)\n",
    "\n",
    "    # mean absolute error\n",
    "    mae_lat = sm.mae(preds_lat, test_lat)\n",
    "    mae_lon = sm.mae(preds_lon, test_lon)\n",
    "\n",
    "    # mean squared error\n",
    "    mse_lat = sm.mse(preds_lat, test_lat)\n",
    "    mse_lon = sm.mse(preds_lon, test_lon)\n",
    "\n",
    "    # Take the average of the metrics\n",
    "    return {\n",
    "        'PCM': (pcm_lat + pcm_lon) / 2,\n",
    "        'DF': (df_lat + df_lon) / 2,\n",
    "        'AREA': (area_lat + area_lon) / 2,\n",
    "        'CL': (cl_lat + cl_lon) / 2,\n",
    "        'DTW': (dtw_lat + dtw_lon) / 2,\n",
    "        'MAE': (mae_lat + mae_lon) / 2,\n",
    "        'MSE': (mse_lat + mse_lon) / 2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Data\\\\seattle_2000_all_obs_sampled.csv\"\n",
    "df = pd.read_csv(file_path, header=0)\n",
    "\n",
    "# Add month column\n",
    "df['month'] = pd.DatetimeIndex(df['datetime']).month\n",
    "\n",
    "# Group by user ID, find month with third most observations (average)\n",
    "#df_m = df.groupby('UID').apply(lambda x: x[x['month'] == x['month'].value_counts().index[2]])\n",
    "\n",
    "# Groupby user ID, keep all observations from January and February \n",
    "df_m = df.groupby('UID').apply(lambda x: x[x['month'].isin([1,2])])\n",
    "\n",
    "df_m = df_m.reset_index(drop=True)\n",
    "\n",
    "max_speed_kmh = 200 # for filtering out unrealistic speeds\n",
    "spatial_radius_km = 0.3 # for compressing similar points using Douglas-Peucker algorithm\n",
    "bin_len_ls = [10080, 5, 1] # Bin lengths to test: 1 week, 1 day, 6 hours, 1 hour, 30 min, 15 min, 1 min\n",
    "init_period_len_1 = 60*24 # 24 hours\n",
    "init_period_len_2 = 60*24*7 # 1 week\n",
    "lr = 0.3 # learning rate\n",
    "n_epochs = 150 # number of epochs\n",
    "\n",
    "# Set search range for ARIMA and SARIMAX\n",
    "p_values = range(0, 3)\n",
    "d_values = range(0, 2)  \n",
    "q_values = range(0, 3) \n",
    "P_values = range(0, 3)  \n",
    "D_values = range(0, 2)  \n",
    "Q_values = range(0, 3)  \n",
    "m_values = range(24, 25) # 24 hours \n",
    "\n",
    "runtimes_comp = []\n",
    "bics_comp = []\n",
    "runtimes_rbf = []\n",
    "bics_rbf = []\n",
    "\n",
    "def trainingLossPlot(ls):\n",
    "    iters = range(0, len(ls))\n",
    "    fig4, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.plot(iters, ls, 'g')\n",
    "    ax.set_title('Training Loss')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    return fig4\n",
    "\n",
    "def predictionsVsActualPlot(y_test_scaled, mean):\n",
    "    plt.rcParams.update({'font.size': 9})\n",
    "            # Make the font nicer\n",
    "    plt.rcParams.update({'font.family': 'serif'})\n",
    "    fig3, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.set_title('Predictions')\n",
    "    try:\n",
    "        pd.DataFrame(mean.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='red', alpha=0.5, s=0.4, label='Predictions')\n",
    "    except AttributeError: \n",
    "        pd.DataFrame(mean).plot(x=1, y=0, kind='scatter',ax=ax, color='red', alpha=0.5, s=0.4, label='Predictions')\n",
    "    pd.DataFrame(y_test_scaled.detach().numpy()).plot(x=1, y=0, kind='scatter',ax=ax, color='blue', alpha=0.5, s=0.4, label='Actual')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    return fig3\n",
    "\n",
    "def trainTestPlot(gapped_user_data, curr_mt):\n",
    "    plt.rcParams.update({'font.size': 9})\n",
    "    plt.rcParams.update({'font.family': 'serif'})\n",
    "    fig1, ax = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n",
    "    ax[0].scatter(curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_lat'],\n",
    "                            color='blue', label='Training data', s=1)\n",
    "    ax[0].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_lat'],\n",
    "                            color='red', label='Test data', s=1)\n",
    "    ax[0].set_ylabel('Latitude')\n",
    "    ax[1].scatter(curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_long'],\n",
    "                            color='blue', label='Training data', s=1)\n",
    "    ax[1].scatter(curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'],\n",
    "                            curr_mt.data[~curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['orig_long'],\n",
    "                            color='red', label='Test data', s=1)\n",
    "    ax[1].set_xlabel('Time')\n",
    "    ax[1].set_ylabel('Longitude')\n",
    "    ax[1].legend()\n",
    "    return fig1\n",
    "\n",
    "def makeSeries(y_train_scaled, y_test_scaled, unix_min_tr, unix_min_te):\n",
    "    lat = pd.Series(y_train_scaled[:,0].tolist(), unix_min_tr)\n",
    "    lat_t = pd.Series(y_test_scaled[:,0].tolist(), unix_min_te)\n",
    "            # Replace duplicates (in time) with the mean of the two values\n",
    "    lat = lat.groupby(lat.index).mean().reset_index()\n",
    "    lat = pd.Series(lat[0].tolist(), lat['index'].tolist())\n",
    "    lat_tc = lat_t.groupby(lat_t.index).mean().reset_index()\n",
    "    lat_tc = pd.Series(lat_tc[0].tolist(), lat_tc['index'].tolist())\n",
    "            # Replace zeroes with positives close to zero\n",
    "    lat.replace(0, 0.000000001, inplace=True)\n",
    "\n",
    "    lon = pd.Series(y_train_scaled[:,1].tolist(), unix_min_tr)\n",
    "    lon_t = pd.Series(y_test_scaled[:,1].tolist(),unix_min_te)\n",
    "            # Replace duplicates (in time) with the mean of the two values\n",
    "    lon = lon.groupby(lon.index).mean().reset_index()\n",
    "    lon = pd.Series(lon[0].tolist(), lon['index'].tolist())\n",
    "    lon_tc = lon_t.groupby(lon_t.index).mean().reset_index()\n",
    "    lon_tc = pd.Series(lon_tc[0].tolist(), lon_tc['index'].tolist())\n",
    "            # Replace zeroes with positives close to zero\n",
    "    lon.replace(0, 0.000000001, inplace=True)\n",
    "    return lat,lat_tc,lon,lon_tc\n",
    "\n",
    "def LI(df_curr_metrics, curr_mt, scaler, y_train_scaled, y_test_scaled, lat_tc):\n",
    "    try:\n",
    "                # Linear Interpolation\n",
    "        print(\"Running Linear Interpolation...\")\n",
    "        LI_preds_lat, LI_preds_long = methods.LI(curr_mt.X_train[:,0], curr_mt.X_test[:,0], y_train_scaled, y_test_scaled)\n",
    "\n",
    "        LI_preds = np.hstack(((LI_preds_lat.reshape(-1, 1), LI_preds_long.reshape(-1, 1))))\n",
    "\n",
    "        LI_preds_origs = scaler.inverse_transform(LI_preds)\n",
    "                \n",
    "        #LI_full_preds_df = helper_func.preds_to_full_df(preds_lat=LI_preds_origs[:,0], preds_long=LI_preds_origs[:,1], \n",
    "        #                                                    test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "        # Changelog: 09/30/2023\n",
    "        # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "        LI_full_preds_df = pd.DataFrame(LI_preds_origs, columns=['lat', 'long'])\n",
    "        LI_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "                \n",
    "        LI_tdf = helper_func.skmob_metric_calcs(LI_full_preds_df, method='LI', lat='lat', long='long', datetime='datetime')\n",
    "        LI_res = metrics.average_eval(np.array(y_test_scaled[:,0]), np.array(y_test_scaled[:,1]), LI_preds_lat, LI_preds_long)\n",
    "        LI_sim = evaluate_similarity(lat_tc, LI_preds, y_test_scaled)\n",
    "        LI_res.update(LI_sim)\n",
    "\n",
    "        LI_rec_acc = helper_func.matrix_acc(LI_tdf.recency_li_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "        LI_freq_rank_acc = helper_func.matrix_acc(LI_tdf.freq_rank_li_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "        fig6 = predictionsVsActualPlot(y_test_scaled, LI_preds)\n",
    "    except:\n",
    "        print(\"Error in LI\")\n",
    "        LI_res = None\n",
    "        LI_sim = None\n",
    "        LI_rec_acc = None\n",
    "        LI_freq_rank_acc = None\n",
    "    return LI_preds, LI_tdf, LI_res, LI_sim, LI_rec_acc, LI_freq_rank_acc, fig6\n",
    "\n",
    "def SES(df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc):\n",
    "    print(\"Running Simple Exponential Smoothing...\")\n",
    "    ses_lat = SimpleExpSmoothing(lat, initialization_method=\"estimated\").fit()\n",
    "    pred_lat_ses = ses_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "    pred_lat_comp_ses = pred_lat_ses[pred_lat_ses.index.isin(unix_min_te)]\n",
    "\n",
    "    ses_lon = SimpleExpSmoothing(lon, initialization_method=\"estimated\").fit()\n",
    "    pred_lon_ses = ses_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "    pred_lon_comp_ses = pred_lon_ses[pred_lon_ses.index.isin(unix_min_te)]\n",
    "\n",
    "    ses_preds = np.hstack(((pred_lat_comp_ses.values.reshape(-1, 1), pred_lon_comp_ses.values.reshape(-1, 1))))\n",
    "    ses_preds_origs = scaler.inverse_transform(ses_preds)\n",
    "\n",
    "    #ses_full_preds_df = helper_func.preds_to_full_df(preds_lat=ses_preds_origs[:,0], preds_long=ses_preds_origs[:,1],\n",
    "    #                                                    test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "    # Changelog: 09/30/2023\n",
    "    # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "    ses_full_preds_df = pd.DataFrame(ses_preds_origs, columns=['lat', 'long'])\n",
    "    ses_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "\n",
    "    ses_tdf = helper_func.skmob_metric_calcs(ses_full_preds_df, method='ses', lat='lat', long='long', datetime='datetime')\n",
    "    ses_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_ses, pred_lon_comp_ses)\n",
    "    ses_sim = evaluate_similarity(lat_tc, ses_preds, y_test_scaled)\n",
    "    ses_res.update(ses_sim)\n",
    "\n",
    "    ses_rec_acc = helper_func.matrix_acc(ses_tdf.recency_ses_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "    ses_freq_rank_acc = helper_func.matrix_acc(ses_tdf.freq_rank_ses_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "    fig7 = predictionsVsActualPlot(y_test_scaled, ses_preds)\n",
    "    return ses_lat, ses_lon, ses_preds, ses_tdf, ses_res, ses_sim, ses_rec_acc, ses_freq_rank_acc, fig7\n",
    "\n",
    "def Holts(df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc):\n",
    "    print(\"Running Holt-Winters model...\")\n",
    "    holt_lat = Holt(lat, damped_trend=True, initialization_method=\"estimated\").fit()\n",
    "    pred_lat_holt = holt_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "    pred_lat_comp_holt = pred_lat_holt[pred_lat_holt.index.isin(unix_min_te)]\n",
    "\n",
    "    holt_lon = Holt(lon, damped_trend=True, initialization_method=\"estimated\").fit()\n",
    "    pred_lon_holt = holt_lon.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "    pred_lon_comp_holt = pred_lon_holt[pred_lon_holt.index.isin(unix_min_te)]\n",
    "    holt_preds = np.hstack(((pred_lat_comp_holt.values.reshape(-1, 1), pred_lon_comp_holt.values.reshape(-1, 1))))\n",
    "\n",
    "    holt_preds_origs = scaler.inverse_transform(holt_preds)\n",
    "\n",
    "    #holt_full_preds_df = helper_func.preds_to_full_df(preds_lat=holt_preds_origs[:,0], preds_long=holt_preds_origs[:,1],\n",
    "    #                                                    test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "    # Changelog: 09/30/2023\n",
    "    # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "    holt_full_preds_df = pd.DataFrame(holt_preds_origs, columns=['lat', 'long'])\n",
    "    holt_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "\n",
    "    holt_tdf = helper_func.skmob_metric_calcs(holt_full_preds_df, method='holt', lat='lat', long='long', datetime='datetime')\n",
    "    holt_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_holt, pred_lon_comp_holt)\n",
    "    holt_sim = evaluate_similarity(lat_tc, holt_preds, y_test_scaled)\n",
    "    holt_res.update(holt_sim)\n",
    "\n",
    "    holt_rec_acc = helper_func.matrix_acc(holt_tdf.recency_holt_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "    holt_freq_rank_acc = helper_func.matrix_acc(holt_tdf.freq_rank_holt_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "    fig8 = predictionsVsActualPlot(y_test_scaled, holt_preds)\n",
    "    return holt_lat, holt_lon, holt_preds, holt_tdf, holt_res, holt_sim, holt_rec_acc, holt_freq_rank_acc, fig8\n",
    "\n",
    "def ES(df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc, es_seasonal_periods=24):\n",
    "    print(\"Running Exponential Smoothing...\")\n",
    "    es_lat = ExponentialSmoothing(lat, seasonal_periods=es_seasonal_periods, trend='add', seasonal='add', damped_trend=True, use_boxcox=False, initialization_method='estimated').fit()\n",
    "    pred_lat_es = es_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "    pred_lat_comp_es = pred_lat_es[pred_lat_es.index.isin(unix_min_te)]\n",
    "\n",
    "    es_lon = ExponentialSmoothing(lon, seasonal_periods=es_seasonal_periods, trend='add', seasonal='add', damped_trend=True, use_boxcox=False, initialization_method='estimated').fit()\n",
    "    pred_lon_es = es_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "    pred_lon_comp_es = pred_lon_es[pred_lon_es.index.isin(unix_min_te)]\n",
    "\n",
    "    es_preds = np.hstack(((pred_lat_comp_es.values.reshape(-1, 1), pred_lon_comp_es.values.reshape(-1, 1))))\n",
    "\n",
    "    es_preds_origs = scaler.inverse_transform(es_preds)\n",
    "\n",
    "    #es_full_preds_df = helper_func.preds_to_full_df(preds_lat=es_preds_origs[:,0], preds_long=es_preds_origs[:,1],\n",
    "    #                                                    test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "    # Changelog: 09/30/2023\n",
    "    # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "    es_full_preds_df = pd.DataFrame(es_preds_origs, columns=['lat', 'long'])\n",
    "    es_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "\n",
    "    es_tdf = helper_func.skmob_metric_calcs(es_full_preds_df, method='es', lat='lat', long='long', datetime='datetime')\n",
    "    es_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_es, pred_lon_comp_es)\n",
    "    es_sim = evaluate_similarity(lat_tc, es_preds, y_test_scaled)\n",
    "    es_res.update(es_sim)\n",
    "\n",
    "    es_rec_acc = helper_func.matrix_acc(es_tdf.recency_es_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "    es_freq_rank_acc = helper_func.matrix_acc(es_tdf.freq_rank_es_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "    fig9 = predictionsVsActualPlot(y_test_scaled, es_preds)\n",
    "    return es_seasonal_periods, es_lat, es_lon, es_tdf, es_res, es_sim, es_rec_acc, es_freq_rank_acc, fig9\n",
    "\n",
    "def arima(p_values, d_values, q_values, df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc):\n",
    "    print(\"Running ARIMA...\")\n",
    "    best_arima_order, best_arima_aic = find_best_arima_order(lat, p_values, d_values, q_values)\n",
    "\n",
    "    arima_lat = ARIMA(lat, order=best_arima_order).fit()\n",
    "    pred_lat_arima = arima_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "    pred_lat_comp_arima = pred_lat_arima[pred_lat_arima.index.isin(unix_min_te)]\n",
    "\n",
    "    arima_lon = ARIMA(lon, order=best_arima_order).fit()\n",
    "    pred_lon_arima = arima_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "    pred_lon_comp_arima = pred_lon_arima[pred_lon_arima.index.isin(unix_min_te)]\n",
    "\n",
    "    arima_preds = np.hstack(((pred_lat_comp_arima.values.reshape(-1, 1), pred_lon_comp_arima.values.reshape(-1, 1))))\n",
    "\n",
    "    arima_preds_origs = scaler.inverse_transform(arima_preds)\n",
    "            \n",
    "    #arima_full_preds_df = helper_func.preds_to_full_df(preds_lat=arima_preds_origs[:,0], preds_long=arima_preds_origs[:,1],\n",
    "    #                                                    test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "    # Changelog: 09/30/2023\n",
    "    # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "    arima_full_preds_df = pd.DataFrame(arima_preds_origs, columns=['lat', 'long'])\n",
    "    arima_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "\n",
    "    arima_tdf = helper_func.skmob_metric_calcs(arima_full_preds_df, method='arima', lat='lat', long='long', datetime='datetime')\n",
    "    arima_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_arima, pred_lon_comp_arima)\n",
    "    arima_sim = evaluate_similarity(lat_tc, arima_preds, y_test_scaled)\n",
    "    arima_res.update(arima_sim)\n",
    "\n",
    "    arima_rec_acc = helper_func.matrix_acc(arima_tdf.recency_arima_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "    arima_freq_rank_acc = helper_func.matrix_acc(arima_tdf.freq_rank_arima_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "    fig10 = predictionsVsActualPlot(y_test_scaled, arima_preds)\n",
    "    return best_arima_order, arima_tdf, arima_res, arima_sim, arima_rec_acc, arima_freq_rank_acc, fig10\n",
    "\n",
    "def sarimax(p_values, d_values, q_values, P_values, D_values, Q_values, m_values, df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc):\n",
    "    print(\"Running SARIMAX...\")\n",
    "    #sarimax_seasonal_order = (1, 1, 1, 24)\n",
    "    best_sarimax_order, best_seasonal_order, best_sarimax_aic = find_best_sarimax_order(lat, p_values, d_values, q_values, P_values, D_values, Q_values, m_values)\n",
    "    sarimax_lat = SARIMAX(lat, order=best_sarimax_order, seasonal_order=best_seasonal_order).fit(disp=False)\n",
    "    pred_lat_sar = sarimax_lat.predict(start=lat_tc.index[0], end=lat_tc.index[-1])\n",
    "    pred_lat_comp_sar = pred_lat_sar[pred_lat_sar.index.isin(unix_min_te)]\n",
    "\n",
    "    sarimax_lon = SARIMAX(lon, order=best_sarimax_order, seasonal_order=best_seasonal_order).fit(disp=False)\n",
    "    pred_lon_sar = sarimax_lon.predict(start=lon_tc.index[0], end=lon_tc.index[-1])\n",
    "    pred_lon_comp_sar = pred_lon_sar[pred_lon_sar.index.isin(unix_min_te)]\n",
    "\n",
    "    sarimax_preds = np.hstack(((pred_lat_comp_sar.values.reshape(-1, 1), pred_lon_comp_sar.values.reshape(-1, 1))))\n",
    "\n",
    "    sarimax_preds_origs = scaler.inverse_transform(sarimax_preds)\n",
    "\n",
    "    #sarimax_full_preds_df = helper_func.preds_to_full_df(preds_lat=sarimax_preds_origs[:,0], preds_long=sarimax_preds_origs[:,1],\n",
    "    #                                                    test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "    # Changelog: 09/30/2023\n",
    "    # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "    sarimax_full_preds_df = pd.DataFrame(sarimax_preds_origs, columns=['lat', 'long'])\n",
    "    sarimax_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "\n",
    "    sarimax_tdf = helper_func.skmob_metric_calcs(sarimax_full_preds_df, method='sarimax', lat='lat', long='long', datetime='datetime')\n",
    "    sarimax_res = metrics.average_eval(lat_tc, lon_tc, pred_lat_comp_sar, pred_lon_comp_sar)\n",
    "    sarimax_sim = evaluate_similarity(lat_tc, sarimax_preds, y_test_scaled)\n",
    "    sarimax_res.update(sarimax_sim)\n",
    "\n",
    "    sarimax_rec_acc = helper_func.matrix_acc(sarimax_tdf.recency_sarimax_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "    sarimax_freq_rank_acc = helper_func.matrix_acc(sarimax_tdf.freq_rank_sarimax_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "    fig11 = predictionsVsActualPlot(y_test_scaled, sarimax_preds)\n",
    "    return best_sarimax_order, best_seasonal_order, sarimax_tdf, sarimax_res, sarimax_sim, sarimax_rec_acc, sarimax_freq_rank_acc, fig11\n",
    "\n",
    "for j in bin_len_ls:\n",
    "    bin_len = j\n",
    "    # Create a directory for each bin length\n",
    "    if not os.path.exists('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len)):\n",
    "        os.makedirs('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len))\n",
    "    print(\"Starting tests on bin length = {}\".format(bin_len))\n",
    "    # Main loop that will go through each user ID, create a directory for each user, etc.\n",
    "    for i in df_m.UID.unique():\n",
    "        if not os.path.exists('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len) + '\\\\' + str(i)):\n",
    "            os.makedirs('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len) + '\\\\' + str(i))\n",
    "            print(\"Starting test on user ID = {}\".format(i))\n",
    "            try:\n",
    "                df_curr = df_m[df_m.UID == i]\n",
    "\n",
    "                tdf = skmob.TrajDataFrame(df_curr, latitude='orig_lat', longitude='orig_long', datetime='datetime')\n",
    "                f_tdf = skmob.preprocessing.filtering.filter(tdf, max_speed_kmh=max_speed_kmh, include_loops=False)\n",
    "                # Print the difference in number of rows\n",
    "                print(\"Number of rows before filtering: {}\".format(tdf.shape[0]))\n",
    "                print(\"Number of rows after filtering: {}\".format(f_tdf.shape[0]))\n",
    "                fc_tdf = skmob.preprocessing.compression.compress(f_tdf, spatial_radius_km=spatial_radius_km)\n",
    "                # Print the difference in number of rows\n",
    "                print(\"Number of rows after compression: {}\".format(fc_tdf.shape[0]))\n",
    "                # Remove data points with uncertainty > 100m\n",
    "                fcu_tdf = fc_tdf[fc_tdf['orig_unc'] <= 100]\n",
    "                # Print the difference in number of rows\n",
    "                print(\"Number of rows after uncertainty filtering: {}\".format(fcu_tdf.shape[0]))\n",
    "                df_curr = fcu_tdf\n",
    "\n",
    "                # Remove duplicates in the unix column\n",
    "                df_curr = df_curr.drop_duplicates(subset=['unix_min'])\n",
    "\n",
    "                curr_ocp = analysis.tempOcp(df_curr, 'unix_min', bin_len=bin_len)\n",
    "\n",
    "                upper_bound = dec_floor(curr_ocp)\n",
    "                \n",
    "                # See current temporal occupancy\n",
    "                print(\"Current temporal occupancy: {}\".format(curr_ocp))\n",
    "                while True:\n",
    "                    try:\n",
    "                        if curr_ocp <= 0.1:\n",
    "                            target_ocp = np.random.uniform(0, curr_ocp)\n",
    "                        else:\n",
    "                            # Choose random decimal between 0.1 and upper bound\n",
    "                            target_ocp = dec_floor(np.random.uniform(0.1, upper_bound))\n",
    "                        print(\"Target temporal occupancy: {}\".format(target_ocp))\n",
    "                        # Simulate gaps in the user's data to match the target level\n",
    "                        gapped_user_data, train_index, new_ocp = analysis.simulate_gaps(df_curr, target_ocp, unix_col='unix_min', bin_len= bin_len)\n",
    "                    except:\n",
    "                        continue\n",
    "                    break\n",
    "\n",
    "                # Change name of 'lat' and 'lon' columns to 'orig_lat' and 'orig_long'\n",
    "                df_curr = df_curr.rename(columns={'lat': 'orig_lat', 'lng': 'orig_long'})\n",
    "\n",
    "                # Create MultiTrip object\n",
    "                curr_mt = preprocessing.dp_MultiTrip(data=df_curr)\n",
    "                curr_mt.Multi_Trip_Preprocess(lat='orig_lat', long='orig_long', datetime='datetime')\n",
    "\n",
    "                # Move 'unix_start_t' to before 'SaM'\n",
    "                cols = list(curr_mt.data.columns)\n",
    "                cols.insert(16, cols.pop(cols.index('unix_min')))\n",
    "                curr_mt.data = curr_mt.data.loc[:, cols] \n",
    "                # Print data columns\n",
    "                print(curr_mt.data.columns)\n",
    "\n",
    "                curr_mt.Multi_Trip_TrainTestSplit(test_start_date=None, test_end_date=None, \n",
    "                                            training_index = set(gapped_user_data['unix_min']), lat='orig_lat', \n",
    "                                            long='orig_long', datetime='datetime', unix='unix_min', inputstart='unix_min', \n",
    "                                            inputend=curr_mt.data.columns[-1])\n",
    "\n",
    "                n_train = len(curr_mt.X_train[:,0])\n",
    "                n_test = len(curr_mt.X_test[:,0])\n",
    "                n_dims = curr_mt.X_train.shape[1]\n",
    "\n",
    "                # Calculate sci-kit mobility metrics\n",
    "                # df_curr_metrics = helper_func.skmob_metric_calcs(df_curr, method='GT', lat='lat', long='lng', datetime='datetime')\n",
    "                # CHANGELOG: 09/30/2021\n",
    "                # Calculating skmob metrics only on the test and prediction points, not the entire dataset\n",
    "                df_curr_metrics = helper_func.skmob_metric_calcs(curr_mt.test, method='GT', lat='lat', long='long', datetime='date')\n",
    "\n",
    "                # See number of points in training and test sets\n",
    "                print(\"Number of points in training set: {}\".format(n_train))\n",
    "                print(\"Number of points in test set: {}\".format(n_test))\n",
    "                print(\"Number of input dimensions: {}\".format(n_dims))\n",
    "\n",
    "                # If there are no points in the test set, skip to the next user\n",
    "                if n_test == 0:\n",
    "                    print(\"No points in test set. Skipping to next user.\")\n",
    "                    continue\n",
    "\n",
    "                # Visualize the training and test data in two subplots, one lat vs time and one long vs time\n",
    "                fig1 = trainTestPlot(gapped_user_data, curr_mt)\n",
    "\n",
    "                mean_lat = curr_mt.y_train[:,0].mean()\n",
    "                mean_long = curr_mt.y_train[:,1].mean()\n",
    "                std_lat = curr_mt.y_train[:,0].std()\n",
    "                std_long = curr_mt.y_train[:,1].std()\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                y_train_scaled = torch.tensor(np.float64(scaler.fit_transform(curr_mt.y_train)))\n",
    "                y_test_scaled = torch.tensor(np.float64(scaler.transform(curr_mt.y_test)))\n",
    "                # Unix time for benchmarks\n",
    "                unix_min_tr = np.array(curr_mt.X_train[:,0]).astype(int)\n",
    "                unix_min_te = np.array(curr_mt.X_test[:,0]).astype(int)\n",
    "\n",
    "                lat, lat_tc, lon, lon_tc = makeSeries(y_train_scaled, y_test_scaled, unix_min_tr, unix_min_te)\n",
    "\n",
    "                likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)\n",
    "\n",
    "                # Composite model with RQ * PER kernels\n",
    "                model = GP.MTGPRegressor(curr_mt.X_train, y_train_scaled, \n",
    "                                        ScaleKernel(ScaleKernel(RQ(ard_num_dims=n_dims)) * ScaleKernel(PER(active_dims=[0]))) + \n",
    "                                        ScaleKernel(ScaleKernel(RQ(ard_num_dims=n_dims)) * ScaleKernel(PER(active_dims=[0])))\n",
    "                )\n",
    "\n",
    "                # Set initial lengthscale guess for unix_min as half the average length of gap in training set\n",
    "                init_lengthscale = curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'].diff().mean() / 2 \n",
    "                # Set the rest of the lengthscale guesses to 1\n",
    "                initializations = np.ones(n_dims - 1)\n",
    "                initializations = np.insert(initializations, 0, init_lengthscale)\n",
    "                model.covar_module.data_covar_module.kernels[0].base_kernel.kernels[0].base_kernel.lengthscale = initializations\n",
    "                model.covar_module.data_covar_module.kernels[1].base_kernel.kernels[0].base_kernel.lengthscale = initializations\n",
    "\n",
    "                # Set initial period lengths\n",
    "                model.covar_module.data_covar_module.kernels[0].base_kernel.kernels[1].base_kernel.period_length = init_period_len_1\n",
    "                model.covar_module.data_covar_module.kernels[1].base_kernel.kernels[1].base_kernel.period_length = init_period_len_2\n",
    "\n",
    "                # Train model\n",
    "                start = time.time()\n",
    "                ls, mll = GP.training(model, curr_mt.X_train, y_train_scaled, lr=lr, n_epochs=n_epochs)\n",
    "                end = time.time()\n",
    "                runtime = end - start\n",
    "                runtimes_comp.append(runtime)\n",
    "\n",
    "                fig2 = trainingLossPlot(ls)\n",
    "                \n",
    "                mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    log_ll = mll(model(curr_mt.X_train), y_train_scaled) * curr_mt.X_train.shape[0]\n",
    "                            \n",
    "                N = curr_mt.X_train.shape[0]\n",
    "                m = sum(p.numel() for p in model.hyperparameters())\n",
    "                bic = -2 * log_ll + m * np.log(N)\n",
    "                bics_comp.append(bic)\n",
    "\n",
    "                predictions, mean = model.predict(curr_mt.X_test)\n",
    "\n",
    "                fig3 = predictionsVsActualPlot(y_test_scaled, mean)\n",
    "\n",
    "                # Model results\n",
    "                mtgp_res = metrics.average_eval(pd.Series(y_test_scaled[:,0]), pd.Series(y_test_scaled[:,1]), pd.Series(mean[:,0]), pd.Series(mean[:,1]))\n",
    "                \n",
    "                mtgp_sim = evaluate_similarity(lat_tc, mean, y_test_scaled)\n",
    "\n",
    "                mtgp_res.update(mtgp_sim)\n",
    "\n",
    "                # Convert mean predictions back to original scale in lat/long\n",
    "                orig_preds = scaler.inverse_transform(mean.reshape(-1,2))\n",
    "\n",
    "                #GP_full_preds_df = helper_func.preds_to_full_df(preds_lat=orig_preds[:,0], preds_long=orig_preds[:,1], \n",
    "                #                                            test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "                # Changelog: 09/30/2023\n",
    "                # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "                GP_full_preds_df = pd.DataFrame(orig_preds, columns=['lat', 'long'])\n",
    "                GP_full_preds_df['datetime'] = curr_mt.test['date'].values\n",
    "\n",
    "                mtgp_tdf = helper_func.skmob_metric_calcs(GP_full_preds_df, method='GP', lat='lat', long='long', datetime='datetime')\n",
    "\n",
    "                mtgp_rec_acc = helper_func.matrix_acc(mtgp_tdf.recency_gp_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "                mtgp_freq_rank_acc = helper_func.matrix_acc(mtgp_tdf.freq_rank_gp_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "\n",
    "                try: \n",
    "                    # Kernel benchmark with a simpler kernel\n",
    "                    # RBF Kernel with ARD\n",
    "                    model_rbf = GP.MTGPRegressor(curr_mt.X_train, y_train_scaled,\n",
    "                                            ScaleKernel(SE(ard_num_dims=n_dims))\n",
    "                    )\n",
    "\n",
    "                    # Set initial lengthscale guess for unix_min as half the average length of gap in training set\n",
    "                    init_lengthscale = curr_mt.data[curr_mt.data['unix_min'].isin(set(gapped_user_data['unix_min']))]['unix_min'].diff().mean() / 2\n",
    "                    # Set the rest of the lengthscale guesses to 1\n",
    "                    initializations = np.ones(n_dims - 1)\n",
    "                    initializations = np.insert(initializations, 0, init_lengthscale)\n",
    "                    model_rbf.covar_module.data_covar_module.base_kernel.lengthscale = initializations\n",
    "\n",
    "                    # Train model\n",
    "                    start = time.time()\n",
    "                    ls, mll = GP.training(model_rbf, curr_mt.X_train, y_train_scaled, lr=lr, n_epochs=n_epochs)\n",
    "                    end = time.time()\n",
    "                    runtime = end - start\n",
    "                    runtimes_rbf.append(runtime)\n",
    "\n",
    "                    fig4 = trainingLossPlot(ls)\n",
    "\n",
    "                    mll = gpytorch.mlls.ExactMarginalLogLikelihood(model_rbf.likelihood, model_rbf)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        log_ll = mll(model_rbf(curr_mt.X_train), y_train_scaled) * curr_mt.X_train.shape[0]\n",
    "\n",
    "                    N = curr_mt.X_train.shape[0]\n",
    "                    m = sum(p.numel() for p in model_rbf.hyperparameters())\n",
    "                    bic = -2 * log_ll + m * np.log(N)\n",
    "                    bics_rbf.append(bic)\n",
    "\n",
    "                    predictions_rbf, mean_rbf = model_rbf.predict(curr_mt.X_test)\n",
    "\n",
    "                    fig5 = predictionsVsActualPlot(y_test_scaled, mean_rbf)\n",
    "            \n",
    "                    # Model results\n",
    "                    mtgp_res_rbf = metrics.average_eval(pd.Series(y_test_scaled[:,0]), pd.Series(y_test_scaled[:,1]), pd.Series(mean_rbf[:,0]), pd.Series(mean_rbf[:,1]))\n",
    "\n",
    "                    mtgp_sim_rbf = evaluate_similarity(lat_tc, mean_rbf, y_test_scaled)\n",
    "\n",
    "                    mtgp_res_rbf.update(mtgp_sim_rbf)\n",
    "\n",
    "                    # Convert mean predictions back to original scale in lat/long\n",
    "                    orig_preds_rbf = scaler.inverse_transform(mean_rbf.reshape(-1,2))\n",
    "\n",
    "                    #GP_full_preds_df_rbf = helper_func.preds_to_full_df(preds_lat=orig_preds_rbf[:,0], preds_long=orig_preds_rbf[:,1],\n",
    "                    #                                                test_df = curr_mt.test, train_df=curr_mt.train)\n",
    "\n",
    "                    # Changelog: 09/30/2023\n",
    "                    # Making it such that skmob metrics are calculated only on the test and prediction points, not the entire dataset\n",
    "                    GP_full_preds_df_rbf = pd.DataFrame(orig_preds_rbf, columns=['lat', 'long'])\n",
    "                    GP_full_preds_df_rbf['datetime'] = curr_mt.test['date'].values\n",
    "                        \n",
    "                    mtgp_tdf_rbf = helper_func.skmob_metric_calcs(GP_full_preds_df_rbf, method='GP_RBF', lat='lat', long='long', datetime='datetime')\n",
    "\n",
    "                    mtgp_rec_acc_rbf = helper_func.matrix_acc(mtgp_tdf_rbf.recency_gp_rbf_pred, df_curr_metrics.recency_gt_pred, metric_name='recency', tolerance=1e-04)\n",
    "                    mtgp_freq_rank_acc_rbf = helper_func.matrix_acc(mtgp_tdf_rbf.freq_rank_gp_rbf_pred, df_curr_metrics.freq_rank_gt_pred, metric_name='freq_rank', tolerance=1e-01)\n",
    "                except:\n",
    "                    print(\"RBF Kernel failed.\")\n",
    "                    mtgp_res_rbf = None\n",
    "                    mtgp_sim_rbf = None\n",
    "                    mtgp_rec_acc_rbf = None\n",
    "                    mtgp_freq_rank_acc_rbf = None\n",
    "                    fig5 = None\n",
    "\n",
    "                try:\n",
    "                    # Linear Interpolation\n",
    "                    LI_preds, LI_tdf, LI_res, LI_sim, LI_rec_acc, LI_freq_rank_acc, fig6 = LI(df_curr_metrics, curr_mt, scaler, y_train_scaled, y_test_scaled, lat_tc)\n",
    "                except:\n",
    "                    print(\"Linear Interpolation failed.\")\n",
    "                    LI_res = None\n",
    "                    LI_sim = None\n",
    "                    LI_rec_acc = None\n",
    "                    LI_freq_rank_acc = None\n",
    "                    fig6 = None\n",
    "\n",
    "                # SES model\n",
    "                try:\n",
    "                    ses_lat, ses_lon, ses_preds, ses_tdf, ses_res, ses_sim, ses_rec_acc, ses_freq_rank_acc, fig7 = SES(df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc)\n",
    "                except:\n",
    "                    print(\"SES failed.\")\n",
    "                    ses_res = None\n",
    "                    ses_sim = None\n",
    "                    ses_rec_acc = None\n",
    "                    ses_freq_rank_acc = None\n",
    "                    ses_lat = None\n",
    "                    ses_lon = None\n",
    "                    fig7 = None\n",
    "\n",
    "                # Holt model\n",
    "                try:\n",
    "                    holt_lat, holt_lon, holt_preds, holt_tdf, holt_res, holt_sim, holt_rec_acc, holt_freq_rank_acc, fig8 = Holts(df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc)\n",
    "                except:\n",
    "                    print(\"Holt's failed.\")\n",
    "                    holt_res = None\n",
    "                    holt_sim = None\n",
    "                    holt_rec_acc = None\n",
    "                    holt_freq_rank_acc = None\n",
    "                    holt_lat = None\n",
    "                    holt_lon = None\n",
    "                    fig8 = None\n",
    "\n",
    "                # Exponential Smoothing\n",
    "                try:\n",
    "                    es_seasonal_periods, es_lat, es_lon, es_tdf, es_res, es_sim, es_rec_acc, es_freq_rank_acc, fig9 = ES(df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc)\n",
    "                except:\n",
    "                    print(\"Exponential Smoothing failed.\")\n",
    "                    es_res = None\n",
    "                    es_sim = None\n",
    "                    es_rec_acc = None\n",
    "                    es_freq_rank_acc = None\n",
    "                    es_seasonal_periods = None\n",
    "                    es_lat = None\n",
    "                    es_lon = None\n",
    "                    fig9 = None\n",
    "                \n",
    "                # ARIMA\n",
    "                try:\n",
    "                    best_arima_order, arima_tdf, arima_res, arima_sim, arima_rec_acc, arima_freq_rank_acc, fig10 = arima(p_values, d_values, q_values, df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc)\n",
    "                except:\n",
    "                    print(\"ARIMA failed.\")\n",
    "                    arima_res = None\n",
    "                    arima_sim = None\n",
    "                    arima_rec_acc = None\n",
    "                    arima_freq_rank_acc = None\n",
    "                    best_arima_order = None\n",
    "                    fig10 = None\n",
    "\n",
    "                # SARIMAX    \n",
    "                try:\n",
    "                    best_sarimax_order, best_seasonal_order, sarimax_tdf, sarimax_res, sarimax_sim, sarimax_rec_acc, sarimax_freq_rank_acc, fig11 = sarimax(p_values, d_values, q_values, P_values, D_values, Q_values, m_values, df_curr_metrics, curr_mt, scaler, y_test_scaled, unix_min_te, lat, lat_tc, lon, lon_tc)\n",
    "                except:\n",
    "                    print(\"SARIMAX failed.\")\n",
    "                    sarimax_res = None\n",
    "                    sarimax_sim = None\n",
    "                    sarimax_rec_acc = None\n",
    "                    sarimax_freq_rank_acc = None\n",
    "                    best_sarimax_order = None\n",
    "                    best_seasonal_order = None\n",
    "                    fig11 = None\n",
    "\n",
    "                # Create a directory for each user\n",
    "                #if not os.path.exists('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len) + '\\\\' + str(i)):\n",
    "                #    os.makedirs('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len) + '\\\\' + str(i))\n",
    "                #else:\n",
    "                #    # If directory already exists, then prediction has already been done for this user, so skip\n",
    "                #    print(\"User {} already exists\".format(i))\n",
    "                #    continue\n",
    "                # Navigate to the directory\n",
    "                os.chdir('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len) + '\\\\' + str(i))\n",
    "\n",
    "                # Save figure to file\n",
    "                try:\n",
    "                    fig1.savefig('train_test_sets_plot.png', dpi=300)\n",
    "                    fig2.savefig('training_loss_plot.png', dpi=300)\n",
    "                    fig3.savefig('mtgp_predictions_plot.png', dpi=300)\n",
    "                    fig4.savefig('training_loss_plot_rbf.png', dpi=300)\n",
    "                    fig5.savefig('mtgp_rbf_predictions_plot.png', dpi=300)\n",
    "                    fig6.savefig('li_predictions_plot.png', dpi=300)\n",
    "                    fig7.savefig('ses_predictions_plot.png', dpi=300)\n",
    "                    fig8.savefig('holt_predictions_plot.png', dpi=300)\n",
    "                    fig9.savefig('es_predictions_plot.png', dpi=300)\n",
    "                    fig10.savefig('arima_predictions_plot.png', dpi=300)\n",
    "                    fig11.savefig('sarimax_predictions_plot.png', dpi=300)\n",
    "                except:\n",
    "                        # Keep going even if figures fail to save\n",
    "                    print(\"Failed to save figures.\")\n",
    "                    pass\n",
    "                        \n",
    "                try:\n",
    "                    # Create dictionary to store parameters\n",
    "                    params = {\n",
    "                        'max_speed_kmh': max_speed_kmh,\n",
    "                        'spatial_radius_km': spatial_radius_km,\n",
    "                        'bin_len': bin_len,\n",
    "                        'tdf.shape[0]': tdf.shape[0],\n",
    "                        'f_tdf.shape[0]': f_tdf.shape[0],\n",
    "                        'fc_tdf.shape[0]': fc_tdf.shape[0],\n",
    "                        'fcu_tdf.shape[0]': fcu_tdf.shape[0],\n",
    "                        'curr_ocp': curr_ocp,\n",
    "                        'target_ocp': target_ocp,\n",
    "                        'new_ocp': new_ocp,\n",
    "                        'n_train': n_train,\n",
    "                        'n_test': n_test,\n",
    "                        'n_dims': n_dims,\n",
    "                        'mean_lat': mean_lat,\n",
    "                        'mean_long': mean_long,\n",
    "                        'std_lat': std_lat,\n",
    "                        'std_long': std_long,\n",
    "                        'init_lengthscale': init_lengthscale,\n",
    "                        'init_period_len_1': init_period_len_1,\n",
    "                        'init_period_len_2': init_period_len_2,\n",
    "                        'log_ll': log_ll,\n",
    "                        'm': m,\n",
    "                        'bic': bic,\n",
    "                        'gp_runtime': runtime,\n",
    "                        'ses_smoothing_level': ses_lat.params['smoothing_level'],\n",
    "                        'holt_smoothing_level_lat': holt_lat.params['smoothing_level'],\n",
    "                        'holt_smoothing_slope_lat': holt_lat.params['smoothing_trend'],\n",
    "                        'holt_smoothing_level_lon': holt_lon.params['smoothing_level'],\n",
    "                        'holt_smoothing_slope_lon': holt_lon.params['smoothing_trend'],\n",
    "                        'holt_damping_slope_lat': holt_lat.params['damping_trend'],\n",
    "                        'holt_damping_slope_lon': holt_lon.params['damping_trend'],\n",
    "                        'es_seasonal_periods': es_seasonal_periods,\n",
    "                        'es_smoothing_level_lat': es_lat.params['smoothing_level'],\n",
    "                        'es_smoothing_slope_lat': es_lat.params['smoothing_trend'],\n",
    "                        'es_smoothing_level_lon': es_lon.params['smoothing_level'],\n",
    "                        'es_smoothing_slope_lon': es_lon.params['smoothing_trend'],\n",
    "                        'es_damping_slope_lat': es_lat.params['damping_trend'],\n",
    "                        'es_damping_slope_lon': es_lon.params['damping_trend'],\n",
    "                        'arima_order': best_arima_order,\n",
    "                        'sarimax_order': best_sarimax_order,\n",
    "                        'sarimax_seasonal_order': best_seasonal_order\n",
    "                    }\n",
    "                     # Convert all values to float, except for tuples\n",
    "                    for k, v in params.items():\n",
    "                        try:\n",
    "                            params[k] = float(v)\n",
    "                        except TypeError:\n",
    "                            pass\n",
    "                    # Write params to a file\n",
    "                    with open('params_' + str(i) + '.json', 'w') as fp:\n",
    "                        json.dump(params, fp)\n",
    "\n",
    "                    # Create dataframe to store parameters\n",
    "                    params_df = pd.DataFrame.from_dict(params, orient='index')\n",
    "                    params_df.columns = ['value']\n",
    "                    params_df.to_csv('params_' + str(i) + '.csv')\n",
    "\n",
    "                    print(\"Saving parameters...\")\n",
    "                except:\n",
    "                    print(\"Failed to save parameters.\")\n",
    "                    pass\n",
    "\n",
    "                # See the differences in metric results\n",
    "                \n",
    "               \n",
    "                # Create dataframe to store results\n",
    "                results = pd.DataFrame.from_dict([mtgp_res, mtgp_res_rbf, ses_res, holt_res, es_res, arima_res, sarimax_res])\n",
    "                results['model'] = ['MTGP_Comp', 'MTGP_RBF', 'SES', 'Holt', 'ES', 'ARIMA', 'SARIMAX']\n",
    "                results = results.set_index('model')\n",
    "\n",
    "                # Create dataframe to store scalar scikit-mobility metrics\n",
    "                skmob_metrics_df = pd.DataFrame(columns=['no_loc', 'rg', 'k_rg',    \n",
    "                                                    'spat_burst', 'rand_entr', \n",
    "                                                    'real_entr', 'uncorr_entr',\n",
    "                                                    'max_dist', 'dist_straight', 'max_dist_home', \n",
    "                                                    'recency', 'freq_rank'])\n",
    "\n",
    "                skmob_metrics_df['methods'] = ['MTGP_Comp', 'MTGP_RBF', 'SES', 'Holt', 'ES', 'ARIMA', 'SARIMAX','LI', 'Ground Truth']\n",
    "                # Make methods the index\n",
    "                skmob_metrics_df = skmob_metrics_df.set_index('methods')\n",
    "\n",
    "                skmob_metrics_df.iloc[0] = mtgp_tdf.no_loc_gp_pred, mtgp_tdf.rg_gp_pred, mtgp_tdf.k_rg_gp_pred, mtgp_tdf.spat_burst_gp_pred, mtgp_tdf.rand_entr_gp_pred, mtgp_tdf.real_entr_gp_pred, mtgp_tdf.uncorr_entr_gp_pred, mtgp_tdf.max_dist_gp_pred, mtgp_tdf.dist_straight_gp_pred, mtgp_tdf.max_dist_home_gp_pred, mtgp_rec_acc, mtgp_freq_rank_acc\n",
    "                skmob_metrics_df.iloc[1] = mtgp_tdf_rbf.no_loc_gp_rbf_pred, mtgp_tdf_rbf.rg_gp_rbf_pred, mtgp_tdf_rbf.k_rg_gp_rbf_pred, mtgp_tdf_rbf.spat_burst_gp_rbf_pred, mtgp_tdf_rbf.rand_entr_gp_rbf_pred, mtgp_tdf_rbf.real_entr_gp_rbf_pred, mtgp_tdf_rbf.uncorr_entr_gp_rbf_pred, mtgp_tdf_rbf.max_dist_gp_rbf_pred, mtgp_tdf_rbf.dist_straight_gp_rbf_pred, mtgp_tdf_rbf.max_dist_home_gp_rbf_pred, mtgp_rec_acc_rbf, mtgp_freq_rank_acc_rbf\n",
    "                skmob_metrics_df.iloc[2] = ses_tdf.no_loc_ses_pred, ses_tdf.rg_ses_pred, ses_tdf.k_rg_ses_pred, ses_tdf.spat_burst_ses_pred, ses_tdf.rand_entr_ses_pred, ses_tdf.real_entr_ses_pred, ses_tdf.uncorr_entr_ses_pred, ses_tdf.max_dist_ses_pred, ses_tdf.dist_straight_ses_pred, ses_tdf.max_dist_home_ses_pred, ses_rec_acc, ses_freq_rank_acc\n",
    "                skmob_metrics_df.iloc[3] = holt_tdf.no_loc_holt_pred, holt_tdf.rg_holt_pred, holt_tdf.k_rg_holt_pred, holt_tdf.spat_burst_holt_pred, holt_tdf.rand_entr_holt_pred, holt_tdf.real_entr_holt_pred, holt_tdf.uncorr_entr_holt_pred, holt_tdf.max_dist_holt_pred, holt_tdf.dist_straight_holt_pred, holt_tdf.max_dist_home_holt_pred, holt_rec_acc, holt_freq_rank_acc\n",
    "                skmob_metrics_df.iloc[4] = es_tdf.no_loc_es_pred, es_tdf.rg_es_pred, es_tdf.k_rg_es_pred, es_tdf.spat_burst_es_pred, es_tdf.rand_entr_es_pred, es_tdf.real_entr_es_pred, es_tdf.uncorr_entr_es_pred, es_tdf.max_dist_es_pred, es_tdf.dist_straight_es_pred, es_tdf.max_dist_home_es_pred, es_rec_acc, es_freq_rank_acc\n",
    "                skmob_metrics_df.iloc[5] = arima_tdf.no_loc_arima_pred, arima_tdf.rg_arima_pred, arima_tdf.k_rg_arima_pred, arima_tdf.spat_burst_arima_pred, arima_tdf.rand_entr_arima_pred, arima_tdf.real_entr_arima_pred, arima_tdf.uncorr_entr_arima_pred, arima_tdf.max_dist_arima_pred, arima_tdf.dist_straight_arima_pred, arima_tdf.max_dist_home_arima_pred, arima_rec_acc, arima_freq_rank_acc\n",
    "                skmob_metrics_df.iloc[6] = sarimax_tdf.no_loc_sarimax_pred, sarimax_tdf.rg_sarimax_pred, sarimax_tdf.k_rg_sarimax_pred, sarimax_tdf.spat_burst_sarimax_pred, sarimax_tdf.rand_entr_sarimax_pred, sarimax_tdf.real_entr_sarimax_pred, sarimax_tdf.uncorr_entr_sarimax_pred, sarimax_tdf.max_dist_sarimax_pred, sarimax_tdf.dist_straight_sarimax_pred, sarimax_tdf.max_dist_home_sarimax_pred, sarimax_rec_acc, sarimax_freq_rank_acc\n",
    "                skmob_metrics_df.iloc[7] = LI_tdf.no_loc_li_pred, LI_tdf.rg_li_pred, LI_tdf.k_rg_li_pred, LI_tdf.spat_burst_li_pred, LI_tdf.rand_entr_li_pred, LI_tdf.real_entr_li_pred, LI_tdf.uncorr_entr_li_pred, LI_tdf.max_dist_li_pred, LI_tdf.dist_straight_li_pred, LI_tdf.max_dist_home_li_pred, LI_rec_acc, LI_freq_rank_acc\n",
    "                skmob_metrics_df.iloc[8] = df_curr_metrics.no_loc_gt_pred, df_curr_metrics.rg_gt_pred, df_curr_metrics.k_rg_gt_pred, df_curr_metrics.spat_burst_gt_pred, df_curr_metrics.rand_entr_gt_pred, df_curr_metrics.real_entr_gt_pred, df_curr_metrics.uncorr_entr_gt_pred, df_curr_metrics.max_dist_gt_pred, df_curr_metrics.dist_straight_gt_pred, df_curr_metrics.max_dist_home_gt_pred, -1, -1,\n",
    "                \n",
    "                # Find absolute difference between predicted and ground truth\n",
    "                skmob_metrics_df['no_loc_error'] = skmob_metrics_df['no_loc'] - skmob_metrics_df.iloc[8]['no_loc']\n",
    "                skmob_metrics_df['rg_error'] = skmob_metrics_df['rg'] - skmob_metrics_df.iloc[8]['rg']\n",
    "                skmob_metrics_df['k_rg_error'] = skmob_metrics_df['k_rg'] - skmob_metrics_df.iloc[8]['k_rg']\n",
    "                skmob_metrics_df['spat_burst_error'] = skmob_metrics_df['spat_burst'] - skmob_metrics_df.iloc[8]['spat_burst']\n",
    "                skmob_metrics_df['rand_entr_error'] = skmob_metrics_df['rand_entr'] - skmob_metrics_df.iloc[8]['rand_entr']\n",
    "                skmob_metrics_df['real_entr_error'] = skmob_metrics_df['real_entr'] - skmob_metrics_df.iloc[8]['real_entr']\n",
    "                skmob_metrics_df['uncorr_entr_error'] = skmob_metrics_df['uncorr_entr'] - skmob_metrics_df.iloc[8]['uncorr_entr']\n",
    "                skmob_metrics_df['max_dist_error'] = skmob_metrics_df['max_dist'] - skmob_metrics_df.iloc[8]['max_dist']\n",
    "                skmob_metrics_df['dist_straight_error'] = skmob_metrics_df['dist_straight'] - skmob_metrics_df.iloc[8]['dist_straight']\n",
    "                skmob_metrics_df['max_dist_home_error'] = skmob_metrics_df['max_dist_home'] - skmob_metrics_df.iloc[8]['max_dist_home']\n",
    "\n",
    "                # Find mean absolute error (MAE) and median absolute error for each method from the absolute differences in each metric\n",
    "                skmob_metrics_df['mae'] = (1/10) * (abs(skmob_metrics_df['no_loc_error']) + abs(skmob_metrics_df['rg_error']) + abs(skmob_metrics_df['k_rg_error']) + abs(skmob_metrics_df['spat_burst_error']) + abs(skmob_metrics_df['rand_entr_error']) + abs(skmob_metrics_df['real_entr_error']) + abs(skmob_metrics_df['uncorr_entr_error']) + abs(skmob_metrics_df['max_dist_error']) + abs(skmob_metrics_df['dist_straight_error']) + abs(skmob_metrics_df['max_dist_home_error']) )\n",
    "                skmob_metrics_df['mad'] = np.median(0 - np.median(np.array([abs(skmob_metrics_df['no_loc_error']), abs(skmob_metrics_df['rg_error']), abs(skmob_metrics_df['k_rg_error']), abs(skmob_metrics_df['spat_burst_error']), abs(skmob_metrics_df['rand_entr_error']), abs(skmob_metrics_df['real_entr_error']), abs(skmob_metrics_df['uncorr_entr_error']), abs(skmob_metrics_df['max_dist_error']), abs(skmob_metrics_df['dist_straight_error']), abs(skmob_metrics_df['max_dist_home_error']) ])))\n",
    "                # Write skmob metrics to a file\n",
    "                skmob_metrics_df.to_csv('skmob_metrics_' + str(i) + '.csv')\n",
    "\n",
    "                # Write results to a file\n",
    "                results.to_csv('results_' + str(i) + '.csv')\n",
    "\n",
    "                # Create a directory for all results if it doesn't exist\n",
    "                if not os.path.exists('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len) + '\\\\all_results'):\n",
    "                    os.makedirs('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len) + '\\\\all_results')\n",
    "                # Navigate to the directory\n",
    "                os.chdir('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len) + '\\\\all_results')\n",
    "\n",
    "                # Write results there as well\n",
    "                results.to_csv('results_' + str(i) + '.csv')\n",
    "\n",
    "                # Create a directory for all skmob metrics if it doesn't exist\n",
    "                if not os.path.exists('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len) + '\\\\all_skmob_metrics'):\n",
    "                    os.makedirs('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len) + '\\\\all_skmob_metrics')\n",
    "                # Navigate to the directory\n",
    "                os.chdir('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len) + '\\\\all_skmob_metrics')\n",
    "\n",
    "                # Write skmob metrics there as well\n",
    "                skmob_metrics_df.to_csv('skmob_metrics_' + str(i) + '.csv')\n",
    "\n",
    "                # Create a directory for all parameters if it doesn't exist\n",
    "                if not os.path.exists('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len) + '\\\\all_parameters'):\n",
    "                    os.makedirs('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len) + '\\\\all_parameters')\n",
    "                # Navigate to the directory\n",
    "                os.chdir('C:\\\\Users\\\\ekino\\\\OneDrive - UW\\\\GPR\\\\Sept_Results\\\\' + str(bin_len) + '\\\\all_parameters')\n",
    "\n",
    "                # Write parameters there as well\n",
    "                params_df.to_csv('params_' + str(i) + '.csv')\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            print(\"User ID = {} already exists. Skipping to next user.\".format(i))\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Using AM peak to predict PM peak behavior\n",
    "# Compared to using previous PM peaks to predict PM peak\n",
    "am_peak_data = df_curr[df_curr['AM_Peak'] == 1]\n",
    "pm_peak_data = df_curr[df_curr['PM_Peak'] == 1]\n",
    "\n",
    "\n",
    "# Experiment 2: Effect of incorporating the Holiday column\n",
    "# Choose two public holidays and set aside one for testing, other for training\n",
    "# Compare the performance of the model trained with holiday data to the model not trained with the other holiday\n",
    "\n",
    "# Experiment 3: Weekend Tests\n",
    "# Train with weekday data, test the weekend\n",
    "# Train with weekend data, test the weekend\n",
    "# Train with both, test the weekend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
